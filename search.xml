<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[使用bash脚本下载和处理基因表达数据]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F25%2F%E4%BD%BF%E7%94%A8bash%E8%84%9A%E6%9C%AC%E4%B8%8B%E8%BD%BD%E5%92%8C%E5%A4%84%E7%90%86NGS%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[使用bash脚本从ArrayExpress下载和处理基因表达数据（去接头和质控） ArrayExpress数据库简介&emsp;&emsp;ArrayExpress是EBI的微阵列实验和基因表达谱的公共数据库，它是一个一般的基因表达数据库，设计用来保存来自所有微阵列平台的数据。ArrayExpress使用MIAME(Minimum Information About a Microarray Experiment，有关微阵列实验的最小化信息)注释标准及相关的XML数据交换格式MAGE-ML(Microarray Gene Expression Markup Language，微阵列基因表达标记语言)，它被设计成以结构化的方式来存储良好注释的数据。ArrayExpress基础结构由数据库本身，以MAGE-ML格式的数据提交或通过在线的提交工具MIAMExpress，在线数据库查询接口，Expression Profiler在线分析工具组成。ArrayExpress提供三种类型的提交，阵列，实验和实验方案，它们中的每一个都分配一个登录号。数据提交和注释的帮助由监管小组提供。数据库可以用诸如作者，实验室，物种，试验或阵列类型等参数进行查询。 随着越来越多的(an increasing number of)组织采用MAGE-ML标准，提交到ArrayExpress的量在快速增长着。 具体步骤 AEArrayExpress主页搜索E-MTAB-567 https://www.ebi.ac.uk/arrayexpress/ 点击Export table in Tab-delimited format，下载E-MTAB-567.sdrf.txt 提取status,ID,link grep ‘fastq.gz’ E-MTAB-567.sdrf.txt | head -2 | awk ‘{print $39”,”$21”,”$35}’ &gt; status,ID,link.csv 输出: mainPipeline.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145#!/bin/bashset -uset -eset -o pipefail#####################################################################################################################################################################################################################使用bash脚本从ArrayExpress下载和处理基因表达数据（去接头和质控）#####################################################################软件安装##安装fastx_toolkitconda install fastx_toolkit ##安装FastQconda install fastqc#################################################################################################################################################提取相关信息（extract.sh）for LINE in $(cat status,ID,link.csv)do echo $LINE STATUS=$(echo $LINE|cut -d, -f1) ID=$(echo $LINE|cut -d, -f2) LINK=$(echo $LINE|cut -d, -f3) FILE=$(basename $LINK) STEM=$(basename $LINK .gz) NEWID=$STATUS.$ID echo $STATUS, $ID, $LINK, $STEM, $FILE, $NEWID##############################################################################################输出：#u641750@GenekServer:~$ ./extract.sh #Normal,10N_1,ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR031/ERR031017/ERR031017_1.fastq.gz#Normal, 10N_1, ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR031/ERR031017/ERR031017_1.fastq.gz, ERR031017_1.fastq, ERR031017_1.fastq.gz, Normal.10N_1#Normal,10N_2,ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR031/ERR031017/ERR031017_2.fastq.gz#Normal, 10N_2, ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR031/ERR031017/ERR031017_2.fastq.gz, ERR031017_2.fastq, ERR031017_2.fastq.gz, Normal.10N_2#例如：$STATUS：Normal, $ID:10N_1, $LINK:ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR031/ERR031017/ERR031017_1.fastq.gz , $STEM:ERR031017_1.fastq, $FILE:ERR031017_1.fastq.gz, $NEWID:Normal.10N_1################################################################################################################################################[ -d ./fastqFiles ]|| mkdir -p ./fastqFiles#下载数据 if [ ! -f ./fastqFiles/$NEWID.$STEM ];then wget $LINK gunzip $FILE mv $STEM ./fastqFiles/$NEWID.$STEM #移动并改名 fidone##############################################################################################输出：#u641750@GenekServer:~/fastqFiles$ ls -l#total 16341536#-rw-r--r-- 1 u641750 GenekVIP 8358690360 Aug 22 21:29 Normal.10N_1.ERR031017_1.fastq#-rw-r--r-- 1 u641750 GenekVIP 8358690360 Aug 25 13:20 Normal.10N_2.ERR031017_2.fastq#################################################################################################################################################质量控制(修剪)[ -d ./trimmed/fastQC ]||mkdir -p ./trimmed/fastQCfor FILE in ./fastqFiles/*.fastqdo ls -lh $FILE STEM=$(basename $FILE .fastq) if [ ! -f ./trimmed/$STEM.trimmed.fastaq ];then fastq_quality_trimmer -v -t 20 -l 20 -Q 33 -i $FILE -o ./trimmed/$STEM.trimmed.fastq fidone##############################################################################################结果#u641750@GenekServer:~/trimmed$ ls#fastQC Normal.10N_1.ERR031017_1.trimmed.fastq Normal.10N_2.ERR031017_2.trimmed.fastq#输出：#-rw-r--r-- 1 u641750 GenekVIP 7.8G Aug 22 21:29 ./fastqFiles/#Normal.10N_1.ERR031017_1.fastq#Minimum Quality Threshold: 20#Minimum Length: 20#Input: 34536162 reads.#Output: 34458851 reads.#discarded 77311 (0%) too-short reads.#-rw-r--r-- 1 u641750 GenekVIP 7.8G Aug 25 13:20 ./fastqFiles/#Normal.10N_2.ERR031017_2.fastq#Minimum Quality Threshold: 20#Minimum Length: 20#Input: 34536162 reads.#Output: 33968272 reads.#discarded 567890 (1%) too-short reads.#################################################################################################################################################质量控制（评估）[ -d ./trimmed/fastQC ]||mkdir -p ./trimmed/fastQCFASTQC=/home/u641750/miniconda3/bin/fastqcfor FILE in ./trimmed/*.trimmed.fastqdo ls -lh $FILE STEM=$(basename $FILE .fastq) if [ ! -d ./trimmed/fastQC/$STEM_fastqc ]; then $FASTQC -o ./trimmed/fastQC $FILE fidone##############################################################################################结果：#u641750@GenekServer:~/trimmed/fastQC$ ls#Normal.10N_1.ERR031017_1.trimmed_fastqc.html#Normal.10N_1.ERR031017_1.trimmed_fastqc.zip#Normal.10N_2.ERR031017_2.trimmed_fastqc.html#Normal.10N_2.ERR031017_2.trimmed_fastqc.zip#################################################################################################################################################改名for FILEONE in ./trimmed/*_1.trimmed.fastqdo STEM=$(basename $FILEONE _1.trimmed.fastq | sed '' s/_1//g') FILETWO=$(echo $FILEONE | sed 's/_1./_2./g') ls -lh $FILEONE ls -lh $FILETWO echo stem $STEMexit done##############################################################################################输出#-rw-r--r-- 1 u641750 GenekVIP 7.5G Aug 25 15:29 ./trimmed/Normal.10N_1.ERR031017_1.trimmed.fastq#-rw-r--r-- 1 u641750 GenekVIP 7.5G Aug 25 15:44 ./trimmed/Normal.10N_2.ERR031017_2.trimmed.fastq#stem Normal.10N.ERR031017################################################################################################################################################ 在后台运行脚本nohup ./mainPipeline.sh &gt; outMainPipeline.log &amp; 附录： 基本命令：basename 用途:返回一个字符串参数的基本文件名称 语法:basename String [ Suffix ] 描述:basename 命令读取 String 参数，删除以 /(斜杠) 结尾的前缀以及任何指定的 Suffix 参数，并将剩余的基本文件名称写至标准输出。 （截取文件名）例如，输入： basename Normal.10N_1.ERR031017_1.trimmed.fastq _1.trimmed.fastq 结果是：Normal.10N_1.ERR031017 例如，输入： basename /u/dee/desktop/cns.boo cns.boo 结果是：cns.boo 如果指定 Suffix（后缀名）参数，且它和字符串中所有字符都不相同，但和字符串的后缀相同，则除去指定后缀。例如，输入： basename /u/dee/desktop/cns.boo .boo 结果是：cns fastq_quality_trimmer fastq_quality_trimmer [-h] [-v] [-t N] [-l N] [-z] [-i INFILE] [-o OUTFILE] 修剪reads的末端 [-t N] = 从5’端开始，低与N的质量的碱基将被修剪掉 [-l N] = 修建之后的reads的长度允许的最短值 [-z] = 压缩输出 [-v] =详细-报告序列编号，如果使用了-o则报告会直接在STDOUT，如果没有则输入到STDERR 3.以下代码段用于脚本的分步测试 1234567891011##########################################################用于测试STATUS="do"#STATUS="done"if [ $STATUS !="done" ];then do somethingfi######################################################### 参考:（1）Bash scripting for Bioinformatics https://www.youtube.com/watch?v=3ME7gayYeUQ （2）Linux命令之basename 命令 http://blog.sina.com.cn/s/blog_5f70c7060100ukyh.html （3）高通量测序数据的质控工具—fastx_toolkit软件使用说明 http://blog.sciencenet.cn/blog-1509670-848270.html]]></content>
      <categories>
        <category>NGS</category>
      </categories>
      <tags>
        <tag>NGS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用bash脚本下载和处理基因表达数据]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F25%2F%E4%BD%BF%E7%94%A8bash%E8%84%9A%E6%9C%AC%E4%B8%8B%E8%BD%BD%E5%92%8C%E5%A4%84%E7%90%86%E5%9F%BA%E5%9B%A0%E8%A1%A8%E8%BE%BE%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[如何优雅的生成及遍历python嵌套字典]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F19%2F%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E7%9A%84%E7%94%9F%E6%88%90python%E5%B5%8C%E5%A5%97%E5%AD%97%E5%85%B8%2F</url>
    <content type="text"><![CDATA[如何优雅的生成及遍历python嵌套字典 嵌套字典生成方法一:定义类12345class Vividict(dict): def __missing__(self, key): value = self[key] = type(self)() return value 解释： 第一行：class后面紧接着是类名，即Vividict，类名通常是大写开头的单词，紧接着是(dict)，表示该类是dict类继承下来的。 我们可以使用dir(dict）查看dict的方法12In[22]: print(dir(dict))['__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values'] 同理，可以查看Vividict的方法12In[23]: print(dir(Vividict))['__class__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__iter__', '__le__', '__len__', '__lt__', '__missing__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values'] 比较两者可以发现，Vividict的方法比dict的方法多了一个missing方法，也就是我们添加的方法。所以这就是继承，继承最大的好处是子类获得了父类的全部功能，而不必重新造轮子。 第二行：python魔法方法中的自定义序列，类似于定义一个函数。missing 在字典的子类中使用，它定义了当试图访问一个字典中不存在的键时的行为（目前为止是指字典的实例，例如我有一个字典 d ， “george” 不是字典中的一个键，当试图访问 d[‘george’] 时就会调用 d.missing(“george”)，结果为{} ）。 第三行，第四行：访问字典中不存在的键(key)时，返回空字典作为其返回值（value） 例如：1234In[17]: a = dict()In[18]: type(a)()Out[18]: &#123;&#125; 注意： 特殊方法“missing”前后有两个下划线！！！ 和普通的函数相比，在类中定义的函数只有一点不同，就是第一个参数永远是实例变量self，并且，调用时，不用传递该参数。除此之外，类的方法和普通函数没有什么区别，所以，你仍然可以用默认参数、可变参数、关键字参数和命名关键字参数。 使用： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# coding=utf-8#导入模块import os, openpyxlimport pprintfrom pandas import DataFrame#pprint模块可以输出漂亮的字典结构，但是不利于后期利用R作图#DataFrame可以将字典结构转为数据框输出，方便后期利用R作图#切换工作路径os.chdir(r'F:\pycharm_project\mutation_count')#读取excel表格wb = openpyxl.load_workbook('东方肝胆数据综合.xlsx')sheet = wb.active#定义类class Vividict(dict): def __missing__(self, key): value = self[key] = type(self)() return value#实例化d = Vividict()#字典初始化，赋初值0for i in range(2,sheet.max_row+1): d[sheet.cell(row=i, column=1).value][sheet.cell(row=i, column=15).value] = 0#累加统计各个样本各种突变类型的数目for i in range(2,sheet.max_row+1): d[sheet.cell(row=i, column=1).value][sheet.cell(row=i, column=15).value] +=1pprint.pprint(d)#输出字典结构pprint.pprint(d)&#123;'PDC1279A_vs_PDC1279': &#123;'UTR3': 9, 'UTR5': 4, 'downstream': 5, 'exonic': 149, 'intergenic': 170, 'intronic': 163, 'ncRNA_exonic': 17, 'ncRNA_intronic': 23, 'splicing': 2, 'upstream;downstream': 2&#125;, 'PDC1279C_vs_PDC1279': &#123;'UTR3': 11, 'UTR5': 13, 'downstream': 1, 'exonic': 174, 'intergenic': 189, 'intronic': 172, 'ncRNA_exonic': 24, 'ncRNA_intronic': 36, 'splicing': 4, 'upstream': 2, 'upstream;downstream': 2&#125;&#125;#输出数据框结构，缺损的元素用 NaN补齐frame = DataFrame(d)print(frame) PDC1279A_vs_PDC1279 PDC1279C_vs_PDC1279 \UTR3 9.0 11.0 UTR5 4.0 13.0 downstream 5.0 1.0 exonic 149.0 174.0 exonic;splicing NaN NaN intergenic 170.0 189.0 intronic 163.0 172.0 ncRNA_exonic 17.0 24.0 ncRNA_intronic 23.0 36.0 ncRNA_splicing NaN NaN splicing 2.0 4.0 upstream NaN 2.0 upstream;downstream 2.0 2.0 方法二：使用defaultdict()两个维度字典：1234from collections import defaultdictd = defaultdict(defaultdict)d[1][2] = 3 等价于： 12345678from collections import defaultdictdef nested_dict_factory(): return defaultdict(int)def nested_dict_factory2(): return defaultdict(nested_dict_factory)db = defaultdict(nested_dict_factory2) 当然，第一种方法简洁的多！ 要获得更多维度，你可以（三维）： 1234from collections import defaultdictd = defaultdict(lambda :defaultdict(defaultdict))d[1][2][3] = 4 使用defaultdict任何未定义的key都会默认返回一个根据method_factory参数不同的默认值, 而相同情况下dict()会返回KeyError. python中lambda存在意义就是对简单函数的简洁表示 实际上 defaultdict也是通过missing方法实现的。defaultdict在dict的基础上添加了一个missing(key)方法, 在调用一个不存的key的时候, defaultdict会调用missing, 返回一个根据default_factory参数的默认值, 所以不会返回Keyerror. 12In[35]: print(dir(defaultdict))['__class__', '__contains__', '__copy__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__iter__', '__le__', '__len__', '__lt__', '__missing__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'clear', 'copy', 'default_factory', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values'] 嵌套字典的遍历方法一：一层一层的嵌套迭代,从而实现遍历1234for key,value in d.items(): for key2, val2 in value.items(): print (key2, val2) 在类中定义walk方法实现嵌套字典的遍历123456789101112class Vividict(dict): def __missing__(self, key): value = self[key] = type(self)() return value def walk(self): for key, value in self.items(): if isinstance(value, Vividict): for tup in value.walk(): yield (key,) + tup else: yield key, value 解释:第1-4行：上面已经解释过了第5-11行：定义一个walk函数，并对字典items对象的key和value进行遍历，isinstance用于判断对象类型，如果value是一个字典，那么对value调用walk（）方法继续进行遍历，一层一层将key,value存储在元祖中（）。当最里面一层，即else情况，输出key,value。整个过程即将字典数据结构扁平化为元祖 此时，我们可以这样来遍历字典（输出元祖） 12345678910111213141516#打印整个元祖for tup in d.walk(): print(tup)('PDC1279_vs_PDC1279C6', 'downstream', 3)('PDC1279_vs_PDC1279C6', 'UTR3', 11)('PDC1279_vs_PDC1279C6', 'intronic', 164)('PDC1279_vs_PDC1279C6', 'splicing', 4)**这就是扁平化的字典**#打印元祖的第3列for tup in d.walk(): print(tup[2]) 参考（1）https://ask.helplib.com/229754 （2）Python魔法方法指南（推荐阅读） http://pyzh.readthedocs.io/en/latest/python-magic-methods-guide.html]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用链接（持续更新）]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F19%2F%E5%B8%B8%E7%94%A8%E9%93%BE%E6%8E%A5%EF%BC%88%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%EF%BC%89%2F</url>
    <content type="text"><![CDATA[常用链接（持续更新） 网络生信课程 Name Description coursera 在网上学习全世界最好的课程 Bioinformatics: Introduction and Methods 北大MOOK生物信息学导论 陈魏学基因 NGS相关视频 Applied Bioinformatics Course 北大罗静初老师abc生信课程 Biomedical Data Science 统计，算法，NGS书籍 Workshops 康奈尔大学生信课件（2010-2017） CGU-GIBMS 台湾长庚大学生信课件 Bioinformatics Team (BioITeam) at the University of Texas 德州大学奥斯丁分校生信课件 CANB7640 COURSE WEBSITE 生信课件-按研究内容分 UCLA 加利福尼亚大学洛杉矶分校NGS数据处理workshop最全合集 Purdue university Discovery Park 普渡大学生物信息课程 Bioinformatics Crash Course - July 2014 &emsp; RNA-seq Analysis &emsp; Linux and HPCC 马里兰大学的生信中心课程-NGS分析，RNA-seq分析，linux和高性能计算 BTI Bioinformatics Course 2017 美国康奈尔大学博伊斯汤普森植物研究所课件 Swiss Institute of Bioinformatics 瑞士生物信息学研究所8年培训资料 BioFrontiers Education 科罗拉多大学生物信息课程 Statistics 246 Statistical Genetics Spring 2006 伯克利大学的遗传统计学课程 NGSchool 2016 Workshops NGSchool非常全面的ppt BIOINFORMATICS PLATFORM 德国柏林医学院中心生物信息学课程资料 UT HEALTH SCIENCE CENTER 圣安东尼奥-生命健康中心-jin实验室生物信息学课程ppt MSc lecture Genomics 柏林自由大学 Genomics12 Statistical Genetics &emsp;NGS &emsp;Statistical Genetics and Genomics 伯明翰阿拉巴马大学-生物信息视频教程 Babraham Institute the Bioinformatics 伯拉罕研究所生信课程 Applied Bioinformatics 2014 biostar handbook-生信分析 网络学编程及数据分析 Name Description OnePageR A Survival Guide to Data Science with R Bioconductor for Genomic Data Science 关于Bioconductor的网络课程，有视频，有材料 Linux command line exercises for NGS data processing NGS数据处理实践过程中的linux命令行知识点 Introduction to Linux for bioinformatics VIB Bioinformatics Core Wiki 分析流程 Name Description Illumina Amplicons Processing Workflow 16S pipelines bio简书 bwa+samtools+picardtools+GATK call SNP 流程 生信数据库 Name Description SNPedia SNP与人类疾病，人群频率，文献报道 GWAS Catalog GWAS研究的SNP,triat,Study等 miRBase miRNA “官方”列表。 TimeTree 进化树的时间尺度。 文献 Name Description Web of Science 大型综合性、多学科、核心期刊引文索引数据库 PubMed 免费海量英文文献 中国知网 中文期刊 SCI-HUB 收费文献免费下载 科研动力 专注EndNote, 关注科研论文写作 GCBI 与PubMed同步并课显示影响因子 pubmedplus 文献摘要自动翻译（翻译得比较好） China PubMed 英文文献摘要翻译（更新不够快） 在线生信小工具论坛 Name Description 医学论坛网 医学资讯及文献导读 seqanswer 关于测序的一个论坛 PLoB Public Library of Bioinformatics (中文博客) rna-seqblog RNA-seq英文博客 biotrainee 生信技能树 实验 Name Description Journal of Visualized Experiments ( JoVE) JoVE是一份展示可视化实验的期刊，是世界上第一个同行评议的科技视频期刊 PROTOCAL ONLINE 生物医学研究领域的实验protocols以及最新实验方面的文章 开发社区 Name Description Biostars 专注于生物信息学，计算基因组学和生物数据分析的问答社区 Segmentfault 一个专注于解决编程问题，提高开发技能的社区 Stackoverflow 编程相关的IT技术问答网站 GitHub 面向开源及私有软件项目的git托管平台 linux Name Description Linux命令大全 linux命令搜索，查看帮助 python Name Description PYMOTW Python2标准库的使用 Bioinformatics code libraries and scripts 常遇到的问题都有脚本（py/pl） 在线小工具 Name Description 在线工具 开源中国开发的，主要面向码农 smallpdf pdf转word等，在线工具 正则表达式在线测试 检查脚本中使用的正则表达式是否正确，提前发现问题 RGB颜色对照表 绘图配色必备 在线代码着色 看看就好 在线 Markdown 编译器 用着不错 ASCII码对照表 速查 在线生成二维码 比较好玩儿 搜索 Name Description PDFDRIVE 搜索下载免费的pdf电子书 软件 Name Description 冰点文库下载器 自由下载百度、豆丁、丁香、MBALib、道客巴巴、Book118等文库文档 visio 专业绘制流程图]]></content>
      <categories>
        <category>常用链接</category>
      </categories>
      <tags>
        <tag>常用链接</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bedtools使用教程详解]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F19%2Fbedtools%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;bedtools开发的目的是为了快速，灵活的比较大量的基因组特征（genomic features）。而genomic features通常使用Browser Extensible Data (BED) 或者 General Feature Format (GFF)文件表示，用UCSC Genome Browser进行可视化比较。 &emsp;&emsp;例如,bedtools可以进行取intersect（交集）, merge（并集）, count（计数）, complement（补集），以及用来对广泛使用的基因组文件格式，例如BAM, BED, GFF/GTF, VCF等进行基因组区间的转换。单个的工具设计的目的是应对简单的任务，复杂的分析能通过组合多个bedtools工具操作实现。同时，该工具允许控制输出结果的呈现形式。最初的bedtools版本支持单独的6列BED文件。但是，如今增加了对序列比对BAM文件的支持。以及GFF文件的特征，BED文件。以及VCF文件。这些工具是相当快速的，并且即使是大的数据集也可以在数秒内完成任务。 我们用bedtools都可以做些啥?&emsp;&emsp;bedtools总共有二三十个工具/命令来处理基因组数据。比较典型而且常用的功能举例如下：格式转换，bam转bed（bamToBed），bed转其他格式（bedToBam，bedToIgv）；对基因组坐标的逻辑运算，包括：交集（intersectBed，windowBed），”邻集“（closestBed），补集（complementBed），并集（mergeBed），差集（subtractBed）;计算覆盖度（coverage）（coverageBed，genomeCoverageBed）；此外，还有一些强大而实用的工具（shuffleBed，groupBy，annotateBed，……） Utility Description annotate Annotate coverage of features from multiple files. bamtobed Convert BAM alignments to BED (&amp; other) formats. bamtofastq Convert BAM records to FASTQ records. bed12tobed6 Breaks BED12 intervals into discrete BED6 intervals. bedpetobam Convert BEDPE intervals to BAM records. bedtobam Convert intervals to BAM records. closest Find the closest, potentially non-overlapping interval. cluster Cluster (but don’t merge) overlapping/nearby intervals. complement Extract intervals not represented by an interval file. coverage Compute the coverage over defined intervals. expand Replicate lines based on lists of values in columns. flank Create new intervals from the flanks of existing intervals. genomecov Compute the coverage over an entire genome. getfasta Use intervals to extract sequences from a FASTA file. groupby Group by common cols. &amp; summarize oth. cols. (~ SQL “groupBy”) igv Create an IGV snapshot batch script. intersect Find overlapping intervals in various ways. jaccard Calculate the Jaccard statistic b/w two sets of intervals. links Create a HTML page of links to UCSC locations. makewindows Make interval “windows” across a genome. map Apply a function to a column for each overlapping interval. maskfasta Use intervals to mask sequences from a FASTA file. merge Combine overlapping/nearby intervals into a single interval. multicov Counts coverage from multiple BAMs at specific intervals. multiinter Identifies common intervals among multiple interval files. nuc Profile the nucleotide content of intervals in a FASTA file. overlap Computes the amount of overlap from two intervals. pairtobed Find pairs that overlap intervals in various ways. pairtopair Find pairs that overlap other pairs in various ways. random Generate random intervals in a genome. reldist Calculate the distribution of relative distances b/w two files. shift Adjust the position of intervals. shuffle Randomly redistribute intervals in a genome. slop Adjust the size of intervals. sort Order the intervals in a file. subtract Remove intervals based on overlaps b/w two files. tag Tag BAM alignments based on overlaps with interval files. unionbedg Combines coverage intervals from multiple BEDGRAPH files. window Find overlapping intervals within a window around an interval. BEDTools suite使用详细bedtools官网： http://bedtools.readthedocs.io/en/latest/ bedtools使用说明： http://quinlanlab.org/tutorials/bedtools/bedtools.html#bedtools-merge BEDTools主要使用BED格式的前三列,即： chrom: 染色体信息 start: genome feature的起始位点，从0开始 end: genome feature的终止位点，至少为1 一般常用物种的genome file在BEDTools安装目录的/genome里面 BEDPE格式是其自定义的一种新的格式，为了简洁的描述不连续的genome features，例如结构变异和双端测序比对 注意： start1和start2起始坐标第一个碱基都为0，所以start=9, end=20表示碱基跨度是从第10位到第20位 chrom1或者chrom2用.表示unknown;start1，end1,start2,end2用-1表示unknown (1)intersect&emsp;&emsp;可以计算两个或者多个BED/BAM/VCF/GFF文件中基因组坐标位置的交集(overlap)，根据参数不同，可以得到不同的结果。 两个BED文件比较图示 一对多比较图示 语法：bedtools intersect -a &lt;bed/gff/vcf/bam&gt; -b &lt;bed/gff/vcf/bam&gt; [OPTIONS] -wa参数可以报告出原始的在A文件中的feature -wb参数可以报告出原始的在B文件中的feature -c参数可以报告出两个文件中的overlap的feature的数量 -wo 返回overlap碱基数 -v 返回非overlap区间 -s 相同链上的feature 当用bedtools intersect 处理大文件时比较耗内存，有效的方法是对A和B文件按照染色体名字(chromosome)和位置(position)排序(sort -k1,1 -k2,2n),然后用-sorted参数重新intersect 案例注意，自己生成测试bed文件，都必须用tab键分割，否则会报错！！ 案例一：包含着染色体位置的两个文件，分别记为A文件和B文件。分别来自于不同文件的染色体位置的交集是什么？ $ cat A.bed chr1 10 20 chr1 30 40 $ cat B.bed chr1 15 25 $ bedtools intersect -a A.bed -b B.bed chr1 15 20 案例二：包含着染色体位置的两个文件，分别记为A文件和B文件。求A文件中哪些染色体位置是与文件B中的染色体位置有overlap. $ cat A.bed chr1 10 20 chr1 30 40 $ cat B.bed chr1 15 25 $ bedtools intersect -a A.bed -b B.bed -wa chr1 10 20 案例三：包含着染色体位置的两个文件，分别记为A文件和B文件。求A文件中染色体位置与文件B中染色体位置的交集，以及对应的文件B中的染色体位置. $ cat A.bed chr1 10 20 chr1 30 40 $ cat B.bed chr1 15 25 $ bedtools intersect -a A.bed -b B.bed -wb chr1 15 20 chr1 15 25 案例四（经用）： 包含着染色体位置的两个文件，分别记为A文件和B文件。求对于A文件的染色体位置是否与文件B中的染色体位置有交集。如果有交集，分别输入A文件的染色体位置和B文件的染色体位置；如果没有交集，输入A文件的染色体位置并以’. -1 -1’补齐文件。 $ cat A.bed chr1 10 20 chr1 30 40 $ cat B.bed chr1 15 25 $ bedtools intersect -a A.bed -b B.bed -loj chr1 10 20 chr1 15 25 chr1 30 40 . -1 -1 案例五： 包含着染色体位置的两个文件，分别记为A文件和B文件。对于A文件中染色体位置，如果和B文件中染色体位置有overlap,则输出在A文件中染色体位置和在B文件中染色体位置，以及overlap的长度. $ cat A.bed chr1 10 20 chr1 30 40 $ cat B.bed chr1 15 20 chr1 18 25 $ bedtools intersect -a A.bed -b B.bed -wo chr1 10 20 chr1 15 20 5 chr1 10 20 chr1 18 25 2 案例六： 包含着染色体位置的两个文件，分别记为A文件和B文件。对于A文件中染色体位置，如果和B文件中染色体位置有overlap,则输出在A文件中染色体位置和在B文件中染色体位置，以及overlap的长度；如果和B文件中染色体位置都没有overlap,则用’. -1-1’补齐文件 $ cat A.bed chr1 10 20 chr1 30 40 $ cat B.bed chr1 15 20 chr1 18 25 $ bedtools intersect -a A.bed -b B.bed -wao chr1 10 20 chr1 15 20 5 chr1 10 20 chr1 18 25 2 chr1 30 40 . -1 -1 案例七： 包含着染色体位置的两个文件，分别记为A文件和B文件。对于A文件中染色体位置，输出在A文件中染色体位置和有多少B文件染色体位置与之有overlap. $ cat A.bed chr1 10 20 chr1 30 40 $ cat B.bed chr1 15 20 chr1 18 25 $ bedtools intersect -a A.bed -b B.bed -c chr1 10 20 2 chr1 30 40 0 案例八(常用)： 包含着染色体位置的两个文件，分别记为A文件和B文件。对于A文件中染色体位置，输出在A文件中染色体位置和与B文件染色体位置至少有X%的overlap的记录。 $ cat A.bed chr1 100 200 $ cat B.bed chr1 130 201 chr1 180 220 $ bedtools intersect -a A.bed -b B.bed -f 0.50 -wa -wb chr1 100 200 chr1 130 201 (2)merge&emsp;&emsp;用于合并位于同一个bed/gff/vcf 文件中有overlap或者距离在一定范围内的相邻区间，距离可由参数(-d)定义。需要注意的是，做合并之前需要先对bed文件做根据染色体排序，可以用bedtoolssort命令实现。 图示 语法bedtools merge [OPTIONS] -i &lt;bed/gff/vcf&gt; 案例$ cat A.bed chr2 10 20 chr1 30 40 chr1 15 20 chr1 18 25 排序： sort -k1,1 -k2,2n A.bed &gt; A.sort.bed $ cat A.sort.bed chr1 15 20 chr1 18 25 chr1 30 40 chr2 10 20 案例一：取并集 bedtools merge -i A.sort.bed chr1 15 25 chr1 30 40 chr2 10 20 案例二：计算重叠区间的个数,-i 指定统计的列，-o指定操作5 bedtools merge -i exons.bed -c 1 -o count chr1 15 25 2 chr1 30 40 1 chr2 10 20 1 案例三：-d 两个独立区域间距小于（等于）该值时将被合并为一个区域；-o collapse显示合并了哪些标签 $ bedtools merge -i A.sort.bed -d 5 -c 1 -o count，collapse chr1 15 40 3 chr1,chr1,chr1 chr2 10 20 1 chr2 (3)complement：返回基因组非覆盖区（用途，比如多轮设计panel)图示 语法bedtools complement -i &lt;BED/GFF/VCF&gt; -g &lt;genome files&gt; (4)genomecov：染色体和全基因组覆盖度计算要求：单个输入bed文件（-i指定）和genome files；如果输入为bam(-ibam指定)文件，则不需要genome files 图示 语法bedtools genomecov [OPTIONS] -i &lt;bed/gff/vcf&gt; -g &lt;genome&gt; 案例$ cat ranges-cov-sorted.bed chr1 4 9 chr1 1 6 chr1 8 19 chr1 25 30 chr2 0 20 $ cat cov.txt （染色体及每条染色体总碱基数） chr1 30 chr2 20 bedtools genomecov -i ranges-cov-sorted.bed -g cov.txt chr1 0 7 30 0.233333 1 chr1 1 20 30 0.666667 chr1 2 3 30 0.1 chr2 1 20 20 1 2 genome 0 7 50 0.14 3 genome 1 40 50 0.8 genome 2 3 50 0.06 #name 覆盖次数 覆盖碱基数 总碱基数 覆盖度 #同时计算单染色体和全基因组覆盖度 ranges-cov.bed文件需提前排序sort -k1,1 ranges-cov.bed &gt; ranges-cov-sorted.bed -bg参数可得到每个碱基的覆盖度。 (5)coverage 计算染色体给定区间覆盖度，输入可以是 BAM 文件$ cat A.bed chr1 0 100 chr1 100 200 chr2 0 100 $ cat B.bed chr1 10 20 chr1 20 30 chr1 30 40 chr1 100 200 $ bedtools coverage -a A.bed -b B.bed chr1 0 100 3 30 100 0.3000000 chr1 100 200 1 100 100 1.0000000 chr2 0 100 0 0 100 0.0000000 #name 覆盖次数 覆盖碱基数 总碱基数 覆盖度 (6)getfasta：提取序列提取指定位置的 DNA 序列，也是很好用的一个功能，反向互补链也可以提，不用自己写脚本提了 语法bedtools getfasta [OPTIONS] -fi &lt;fasta&gt; -bed &lt;bed/gff/vcf&gt; -fo &lt;fasta&gt; 案例要求：基因组fasta文件（-fi指定）,提取区间BED/GTF/GFF/VCF文件(-bed指定),输出文件FASTA（-fo 指定） bedtools getfasta -fi Mus_musculus.GRCm38.75.dna_rm.toplevel_chr1.fa -bed mm_GRCm38_3kb_promoters.gtf -fo mm_GRCm38_3kb_promoters.fasta 扩展： 提取序列之samtools（速度较快） #首先建立fai索引文件（第一列为染色体名字，第二列为序列碱基数） samtools faidx Mus_musculus.GRCm38.75.dna.chromosome.8.fa #序列提取，多提取区间空格隔开 samtools faidx Mus_musculus.GRCm38.75.dna.chromosome.8.fa \ 8:123407082-123410744 8:123518835-123536649 &gt;8:123407082-123410744 GAGAAAAGCTCCCTTCTTCTCCAGAGTCCCGTCTACCCTGGCTTGGCGAGGGAAAGGAAC CAGACATATATCAGAGGCAAGTAACCAAGAAGTCTGGAGGTGTTGAGTTTAGGCATGTCT [...] &gt;8:123518835-123536649 TCTCGCGAGGATTTGAGAACCAGCACGGGATCTAGTCGGAGTTGCCAGGAGACCGCGCAG CCTCCTCTGACCAGCGCCCATCCCGGATTAGTGGAAGTGCTGGACTGCTGGCACCATGGT [...] (7)nuc: 计算GC含量即各碱基数语法bedtools nuc [OPTIONS] -fi &lt;fasta&gt; -bed &lt;bed/gff/vcf&gt; Options: -fi 输入FASTA文件 -bed 提取区间BED/GTF/GFF/VCF文件(-bed指定) 案例bedtools nuc -fi hg19.fa -bed CDS.bed 输出结果解释：在原bed文件每行结尾增加以下几列 Output format:The following information will be reported after each BED entry: 1) %AT content 2) %GC content 3) Number of As observed 4) Number of Cs observed 5) Number of Gs observed 6) Number of Ts observed 7) Number of Ns observed 8) Number of other bases observed 9) The length of the explored sequence/interval. 10) The seq. extracted from the FASTA file. (opt., if -seq is used) 11) The number of times a user&apos;s pattern was observed. (opt., if -pattern is used.) 高级用法Coverage analysis for targeted DNA capture RNA-seq coverage analysis 参考：（1）王球爸的博客： http://blog.sina.com.cn/s/blog_5d5f892a0102v665.html （2）生信人 https://www.wxzhi.com/archives/871/gk4yd3ujan57e0ft/ （3）hope http://tiramisutes.github.io/2016/03/18/bedtools.html]]></content>
      <categories>
        <category>NGS软件</category>
      </categories>
      <tags>
        <tag>NGS软件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux解压缩]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F16%2Flinux%E8%A7%A3%E5%8E%8B%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[总结下linux下各种压缩文件的解压方法及批量解压 首先要弄清两个概念：打包 和 压缩： 打包 是指将一大堆文件或目录变成一个总的文件； 压缩 则是将一个大的文件通过一些压缩算法变成一个小文件。 &emsp;&emsp;Linux中很多压缩程序只能针对一个文件进行压缩，这样当你想要压缩一大堆文件时，你得先将这一大堆文件先打成一个包（tar命令），然后再用压缩程序进行压缩（gzip bzip2命令）。 单个解压 *.tar 用 tar -xvf 解压 *.gz 用 gzip -d或者gunzip 解压 .tar.gz和.tgz 用 tar -xzf 解压 *.bz2 用 bzip2 -d或者用bunzip2 解压 *.tar.bz2用tar -xjf 解压 *.Z 用 uncompress 解压 *.tar.Z 用tar -xZf 解压 *.rar 用 unrar e解压 *.zip 用 unzip 解压 *.tar.xz 用 $xz -d *.tar.xz $tar -xvf *.tar 批量解压 方法一： for i in $(ls *.tar.gz);do tar xvf $i;done 方法二: for tarfile in *.tar.gz; do tar -xvf $tarfile; done 方法三: ls *.tar.gz|xargs -n1 tar xzvf &emsp;&emsp;xargs 是一条 Unix 和类 Unix 操作系统的常用命令；它的作用是将参数列表转换成小块分段传递给其他命令，以避免参数列表过长的问题。 例如： echo “1 2 3 4”|xargs -n11234echo “1 2 3 4”|xargs -n21 23 4 所以，加n1参数，则*.tar.gz会拆成每个tar.gz文件后，一个一个传给tar xzvf命令 方法四： find -maxdepth 1 -name “*.tar.gz”|xargs -i tar xvzf {} &emsp;&emsp;这条命令可解压当前目录下的所有gz文件，maxdepth表示搜索深度，1代表只搜索当前目录 补充使用tar -xvf *.tar.gz会出错，提示“Not found in archive”？ 通配符是shell解决的问题,如 tar -xvf *.tar.gz 实际上执行tar时，tar接收到的是 tar -xvf a.tar.gz b.tar.gz c.tar.gz … 如果当前目录跟本没有tar的东西，那么tar就收到’*.tar.gz’这个参数 与win不同，linux所有字符都可以作文件名，也即目录中不存在着 *.tar.gz这个文件 扩展同样可以利用for循环遍历操作某一类文件，比如批量建索引。 for i in `ls` do samtools faidx $i done]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NGS数据格式BED/GFF/GTF之介绍，比较，转换]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F15%2FNGS%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8FBED-GFF-GTF%E4%B9%8B%E4%BB%8B%E7%BB%8D-%E6%AF%94%E8%BE%83-%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;NGS数据格式BED/GFF/GTF之介绍，比较，转换 Ensemble与UCSC各种格式说明Ensemble各种格式说明： http://www.ensembl.org/info/website/upload/bed.html UCSC各种格式说明： http://genome.ucsc.edu/FAQ/FAQformat.html#format1 基因组坐标系统&emsp;&emsp;计算机和现实世界有一个差别，即计算机的计数是从0开始的（即0-based），而现实生活中，我们的计数是从1开始的（即1-based）。在生物学领域的不同场合，这两种计数交杂出现 两套不同理念的坐标系统——base coordinate system(BCS)和interbase coordinate system（ICS） 我们先来分析一下这样的两套系统在表述序列位置时的差别 类型 1-based 0-based 单个核苷酸 4-4: T 3-4: T 一组核苷酸 4-6: TAA 3-6: TAA deletion 4-4: T/- 3-4: T/- insertion（表示AGGTCGAAGT） 4-5: -/CG 4-4: -/CG &emsp;&emsp;如果说我们不管0-based和1-based这两种计数方式对应的坐标系统，都将其视为对序列第一个核苷酸的描述（即第一个核苷酸是算0还是算1），那么0-based的系统，其区间其实是一个左闭右开的区间，而1-based的系统则是一个闭区间。 1-based，正如前述，就是为了符合人的计数习惯。那UCSC为什么要采用0-based的interbase coordinate system系统？ICS主要的好处体现在下面的几个方面： 对于splicing sites等涉及两个核苷酸之间位置的情况，ICS提供了一个更好的呈现方案； 对于site的计算更加方便，如length=end-start; 对于坐标的转换，尤其是正负链坐标间的转换更加方便。（revStart = chromSize – oneEnd，revEnd = chromeSize – zeroStart）。 NCBI等组织采用的是和人日常习惯一致的1-based计数系统，即用数字指代核苷酸位置。但是UCSC发现使用0-based计数系统（用数字指代核苷酸间的位置） &emsp;&emsp;应用范畴：对于直接给所有用户提供直观访问服务的data portal（比如说NCBI、Ensembl、UCSC Genome Browser、mirBase）、常用的生物信息学软件（如BLAST）以及某些格式的文件（如GFF、SAM、VCF），采用的是1-based的计数方法。与之相反，UCSC的Table Browser以及它在数据库中存储的数据、BED/BAM等格式的文件，以及Chado、DAS2、dbSNP这样的data portal则采用的是0-based的计数方法。 单个格式介绍BED格式 注意： chromStart起始坐标第一个碱基为0 BED行名会展示在基因组浏览器中的bed行的左侧 UCSC中展示效果： 其实，你可以通过UCSC Genome Browser展示你的注释 tracks数据,并以可视化的方式与其他注释数据进行比较！！！ UCSC官方说明—-Displaying Your Own Annotations in the Genome Browser： http://genome.ucsc.edu/goldenPath/help/customTrack.html#EXAMPLE1 具体介绍&emsp;&emsp;个人的注释数据再上传48小时后将失效，除非你登陆并创建会话。 如何登陆Genome Browser并创建会话： http://genome.ucsc.edu/goldenPath/help/hgSessionHelp.html#CTs 当然，你也可以查看别人的定制注释tracks数据： http://genome.ucsc.edu/goldenPath/customTracks/custTracks.html Genome Browser annotation tracks是基于面向行格式的文件的，文件中每行展示一个track特征，注释文件包含三种类型的行：browser lines, track lines, and data lines。空行和其他以#开头的行将被忽略。 如何构建注释文件呢？ （1）数据格式 目前支持标准的GFF格式，以及为Human Genome Project 和 UCSC Genome Browser定制的数据格式，常见的如BED,BAM,VCF,MAF等。注意GFF和GTF文件必须以tab键分割，而不是以空格分隔。参考染色体必须以chrN的形式（区分大小写）。 （2）定义 Genome Browser展示特征的区间 例如：browser position chr22:20100000-20100900 没有的话，展示的tracks在基因组的位置可能不对 （3）定义注释track的一些属性 例如track的name, description, colors, initial display mode, use score等等 如何展示呢？ 例如注释数据如下： browser position chr22:1000-10000browser hide alltrack name=”BED track” description=”BED format custom track example” visibility=2 color=0,128,0 useScore=1chr22 1000 5000 itemA 960 + 1100 4700 0 2 1567,1488, 0,2512chr22 2000 7000 itemB 200 - 2200 6950 0 4 433,100,550,1500 0,500,2000,3500 网址： http://genome.ucsc.edu/cgi-bin/hgCustom &emsp;&emsp;当然，可视化展示并不是真正的目的，主要目的是与其他注释数据进行比较，从而发现某些科学问题。最上面是你的track(Custom Tracks),下面就是按照不同数据类型划分的注释数据，默认都是隐藏的（hide），你可以勾选相应你感兴趣的数据，选择（hide,dense,squish,pack,full）不同展现形式显示其他注释数据，然后refresh，进行可视化比较。 GFF格式参考： http://www.bbioo.com/lifesciences/40-112835-1.html GFF格式（General Feature Format）是一种简单的以tab键分割的，用于描述基因组特征的格式文件。有GFF1, GFF2，GFF3和GTF2四种版本。 GFF1 注意： start第一个碱基是从1开始计数的 frame：在feature是coding exon时，frame可以取值0-2，表示读码框的第一个碱基；如果不是coding exon,则用.表示 GFF1与其他GFF格式的最大区别在于所有相同的feature的行是聚集在一起的 扩展名为.gff1而不是gff GFF2http://gmod.org/wiki/GFF2 注意： start第一个碱基是从1开始计数的 frame：在feature是coding exon时，frame可以取值0-2，表示读码框的第一个碱基；如果不是coding exon,则用.表示 attribute：用;分隔键值对（tag-value），为每个feature提供额外的信息。同时attribute包含identifiers，可以用于联系各个特征。 扩展名为.gff2而不是gff GFF2格式的不足 &emsp;&emsp;GFF2的一个问题是它只能表现一个层次的嵌套功能。 当处理具有多个交替剪接的转录物的基因时存在问题。 GFF2无法处理基因→转录本→外显子的三级体系。 大多数人给出一系列的转录本，并给出类似的名称来表明它们来自相同的基因。 第二个限制是，虽然GFF2允许您创建两级层次结构，如转录本→外显子，但它没有层次结构方向的任何概念。 所以它不知道外显子是否是转录本的subfeature，反之亦然。 这意味着你必须使用“aggregators”来整理关系。 因此，GFF2格式已被弃用，转而支持GFF3格式。 但是，目前Genome Browser是不支持GFF3的，所有的GFF track都是基于Sanger’s GFF2 specification GFF3http://gmod.org/wiki/GFF3 注意： start第一个碱基是从1开始计数的 phase：在feature是coding exon时，phase可以取值0-2，表示读码框的第一个碱基；如果不是coding exon,则用.表示 attribute：用 “=” 分隔键值对（tag-value），为每个feature提供额外的信息。同时attribute包含identifiers，可以用于联系各个特征。 扩展名为.gff3而不是gff 与其他GFF版本格式最大区别在于： 第一行表明版本：##gff-version 3 第9列attributes关联了转录本，之前的GFF版本限制于低层次的feature(exon) key=value具有特殊的含义 ID - unique idenfifier for this feature. Parent - idenfier of parent feature. Name - used as the feature label in the feature map. GTFGTF是另外一种与GFF2十分相似的数据格式，有时称之为GFF2.5。GTF的前8列与GFF2完全一样，group列（attribute列）被扩展了，每个属性包含了一个 type/value pair，属性以;结束，初次之外，属性之间还有一个空格。 属性列（第9列）： gene_id value - A globally unique identifier for the genomic source of the sequence. transcript_id value - A globally unique identifier for the predicted transcript. 属性列例子(UCSC)： gene_id &quot;Em:U62317.C22.6.mRNA&quot;; transcript_id &quot;Em:U62317.C22.6.mRNA&quot;; exon_number 1 属性列例子(Ensemble)： gene_id &quot;ENSG00000223972&quot;; transcript_id &quot;ENST00000456328&quot;; gene_name &quot;DDX11L1&quot;; gene_sourc e &quot;havana&quot;; gene_biotype &quot;transcribed_unprocessed_pseudogene&quot;; transcript_name &quot;DDX11L1-002&quot;; transcript_source &quot;havana&quot;; GTF是不被 GMOD支持的，如果使用，可以将其转换为GFF3格式。 不同格式比较BED和GFF/GTF BED文件中起始坐标为0，结束坐标至少是1,； GFF中起始坐标是1而结束坐标至少是1 两者都是基因注释文件，gtf的一行表示一个exon，多行表示一个基因。而bed的一行表示一个基因。 GTF与GFF区别 简单来说：GFF 2 -&gt; GTF -&gt; GFF 3 G的定义不同 GFF全称为general feature format，这种格式主要是用来注释基因组。 GTF全称为gene transfer format，主要是用来对基因进行注释。 GTF是在GFF（GFF2）的基础上的一个改良，GTF的前8列信息与GFF是一样的，主要区别在第九列，GTF扩充很多GFF不具备的其他的信息 数据格式转换 file conversion script GTF to GFF3 gft2gff3 GTF to BED gtf2Bed.pl BED to GTF bedToGtf.sh 汇总，将各种格式转换为GFF格式的脚本。这些脚本分散在不同的软件包中，可以根据需要下载使用。 bioPerl script description search2gff This script will turn a protein Search report (BLASTP, FASTP, SSEARCH, AXT, WABA) into a GFF File. genbank2gff3.pl Genbank to gbrowse-friendly GFF3 gff2ps This script provides GFF to postscript handling. 如果你在win下安装了perl及bioperl直接在文件夹下搜索便可以找到上述脚本，可以直接使用 gbrowse script description ucsc_genes2gff Convert UCSC Genome Browser-format gene files into GFF files suitable for loading into gbrowse blast92gff3.pl BLAST tabular output (-m 9 or 8) conversion to GFF version 3 format, DAWGPAWS script description cnv_blast2gff.pl This program will translate a blast report for a single query sequence into the GFF format. Tandy software script description gff2aplot a program to visualize the alignment of two genomic sequences together with their annotations. From GFF-format input files it produces PostScript figures for that alignment. blat2gff Converts BLAT output files to GFF formatted files BioWiki中还有一篇，总结更多GFF工具的文章，请参看下面链接： http://biowiki.org/GffTools 参考（1）http://blog.sciencenet.cn/blog-981687-726831.html （2）http://blog.sciencenet.cn/blog-1509670-847310.html （3）https://www.yaolibio.com/2016/08/15/gene-coordinate-system/]]></content>
      <categories>
        <category>NGS</category>
      </categories>
      <tags>
        <tag>NGS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux神器之grep,awk操作gtf文件]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F14%2FLinux%E7%A5%9E%E5%99%A8%E4%B9%8Bgrep%2Cawk%E6%93%8D%E4%BD%9Cgtf%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Linux神器之grep,awk操作gtf文件 Linux神器之grep,awk操作gtf文件过滤#开头注释行grep -v ^# Homo_sapiens.GRCh38.89.chr.gtf |head -5 补充： -v:反向选择 过滤空行：grep -v ‘^$’ filename 过滤空行和以#开头的行: grep -vE ‘^#|^$’ filename -E表示“或”的关系。 提取并计数有多少类featuregrep -v ^# Homo_sapiens.GRCh38.89.chr.gtf |awk &apos;{print $3}&apos;| sort | uniq -c 结果： 710166 CDS 1193694 exon 143497 five_prime_utr 58174 gene 119 Selenocysteine 83231 start_codon 74952 stop_codon 136436 three_prime_utr 199167 transcript （第一列为数目，第二列为features） 筛选出特定行第一列为染色体1-22加X,Y的行 awk &apos;$1 ~ /[0-9]/ || $1 ~ /[X|Y]/&apos; Homo_sapiens.GRCh38.89.chr.gtf | tail -5 第三列为”gene”的行 awk &apos;$3==&quot;gene&quot;&apos; Homo_sapiens.GRCh38.89.chr.gtf |head -5 查看每条染色体多少个基因 awk &apos;$1 ~ /[0-9]/ || $1 ~ /[X|Y]/&apos; Homo_sapiens.GRCh38.89.chr.gtf | awk &apos;$3==&quot;gene&quot;&apos;| awk &apos;{print $1}&apos;| sort | uniq -c 结果： 5224 1 2208 10 3248 11 2952 12 1312 13 2214 14 2155 15 2509 16 3018 17 1174 18 2951 19 3971 2 1391 20 837 21 1339 22 3019 3 2504 4 2869 5 2860 6 2884 7 2367 8 2246 9 2366 X 519 Y （第一列为数目，第二列为染色体号） 先过滤出基因，然后按照chr顺序排序然后根据基因起始位置排序awk &apos;$1 ~ /[0-9]/ || $1 ~ /[X|Y]/ &amp;&amp; $3==&quot;gene&quot;&apos;Homo_sapiens.GRCh38.89.chr.gtf|sort -t &apos;\t&apos; -k1,1n -k4,4n &gt;result_gene_sort.txt (因为根据asc码排序，所以X,Y会排在1-22之前) awk &apos;$1 ~ /[0-9]/ &amp;&amp; $3==&quot;gene&quot;&apos; Homo_sapiens.GRCh38.89.chr.gtf | sort -t $&apos;\t&apos; -k1,1n -k4,4n &gt;result_gene_sort.txt awk &apos;$1 ~ /[X|Y]/ &amp;&amp; $3==&quot;gene&quot;&apos; Homo_sapiens.GRCh38.89.chr.gtf |sort -t $&apos;\t&apos; -k1,1 -k4,4n &gt;&gt;result_gene_sort.txt sort 参数： -k 选择以哪个区间进行排序 -n 依照数值的大小排序 sort多字段的排序: -t $’\t’ 指定使用tab键分列 -k1,1n 指定以第一列按照数字（asc码）从大到小排序 -k4,4n 指定以第一列按照数字（asc码）从大到小排序 计算所有CDS的累积长度，其他类似cat Homo_sapiens.GRCh38.89.chr.gtf | awk &apos;$3 ==&quot;CDS&quot; { len=$5-$4 + 1; size += len; print &quot;Size:&quot;, size } &apos; （结果输出刷屏了！！加上个 |tail -1） 计算1号染色体cds的平均长度awk &apos;BEGIN {s = 0;line = 0 } ;$3 ==&quot;CDS&quot; &amp;&amp; $1 ==&quot;1&quot; { s += ($5 - $4);line += 1}; END {print &quot;mean=&quot; s/line}&apos; Homo_sapiens.GRCh38.89.chr.gtf 补充： awk的BEGIN和END 通常使用BEGIN来显示变量和预置（初始化）变量，使用END来输出最终结果。 总结一下awk基本结构为 : BEGIN{BEGIN操作,在输入文件之前执行} ; {文件行处理块,对来自输入文件datafile的每一行都要执行一遍} ; END{END操作,输入文件关闭后awk退出之前执行} 从gtf文件中分离提取基因名字$3 == &quot;gene&quot; { # 通过 ; 分离提取第9列 split($9, x, &quot;;&quot;) # 基因名字是第一个元素。 # 去除基因名字旁边的双引号 name = x[1] # 由于 &quot; 是一个特殊字符，我们必须写成 \&quot;;*反斜杠\表示转义符。 gsub(&quot;\&quot;&quot;, &quot;&quot;, name) # 打印特征类型、基因名字以及大小。 print name, $5 - $4 + 1} 最后，我们可以写成下边这条命令: cat result_gene_sort.txt |awk &apos;$3 == &quot;gene&quot;{split($10,x,&quot;;&quot;);name = x[1];gsub(&quot;\&quot;&quot;, &quot;&quot;, name);print name,$5-$4+1}&apos;|head -5 根据基因名列表提取gtf文件（1）shell脚本 ./sub_gtf.sh gene.txt Homo_sapiens.GRCh38.89.chr.gtf &gt;logfile 2&gt;&amp;1 #!/bin/bash set -u set -e set -o pipefail if [[ $# != 2 ]];then echo &quot;Parameter incorrect.&quot; exit 1 fi gene_file=$1 gtf_file=$2 gene=($(cat ${gene_file})) for gene in ${gene[@]}; do grep &quot;\&quot;${gene}\&quot;&quot; ${gtf_file} &gt;&gt;result.txt done （2）单行命令 gene=($(cat gene.txt));for gene in ${gene[@]}; do grep &quot;\&quot;${gene}\&quot;&quot; Homo_sapiens.GRCh38.89.chr.gtf &gt;&gt;result.txt;done 补充基本语法：&emsp;&emsp;awk擅长处理表格形式的数据。它逐行从文本中读取数据，将整行数据（record)定义为$0,然后根据指定的分隔符，将各列数据（record)分别定义为$1,$2,$3，然后使用如下结构处理数据： pattern1 {action1};pattern2 {action2};.... 注意： 如果没有定义pattern,则直接执行action； 如果没有提供action,则直接输出满足pattern的内容 参考：（1）http://www.jianshu.com/p/7af624409dcd （2）https://mp.weixin.qq.com/s/NZCt2SR3WmCnqpb2FGcbsQ]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何写shell脚本]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F13%2F%E5%A6%82%E4%BD%95%E5%86%99shell%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[shell代码风格规范及技巧 shell代码风格规范及技巧命名有标准文件名规范，以.sh结尾，方便识别 编码要统一&emsp;&emsp;在写脚本的时候尽量使用UTF-8编码，能够支持中文等一些奇奇怪怪的字符。不过虽然能写中文，但是在写注释以及打log的时候还是尽量英文，毕竟很多机器还是没有直接支持中文的，打出来可能会有乱码。 &emsp;&emsp;这里还尤其需要注意一点，就是当我们是在windows下用utf-8编码来写shell脚本的时候，一定要注意这个utf-8是否是有BOM的。默认情况下windows判断utf-8格式是通过在文件开头加上三个EF BB BF字节来判断的，但是在Linux中默认是无BOM的。因此如果我们是在windows下写脚本的时候，一定要注意将编码改成Utf-8无BOM，一般用notepad++之类的编辑器都能改。否则，在Linux下运行的时候就会识别到开头的三个字符，从而报一些无法识别命令的错。 开头有shebang&emsp;&emsp;所谓shebang其实就是在很多脚本的第一行出现的以”#!”开头的注释，他指明了当我们没有指定解释器的时候默认的解释器，一般可能是下面这样： #!/bin/bash 写出健壮Bash Shell脚本技巧set -x set -e set -u set -o pipeline （1）set -x会在执行每一行 shell 脚本时，把执行的内容输出来。它可以让你看到当前执行的情况，里面涉及的变量也会被替换成实际的值。 （2）在”set -e”之后出现的代码，一旦出现了返回值非零，整个脚本就会立即退出。 &emsp;&emsp;set -e结束程序的条件比较复杂，在man bash里面，足足用了一段话描述各种情景。大多数执行都会在出错时退出，除非 shell 命令位于以下情况： 一个 pipeline 的非结尾部分，比如 error | ok 一个组合语句的非结尾部分，比如 ok &amp;&amp; error || other 一连串语句的非结尾部分，比如 error; ok 位于判断语句内，包括test、if、while等等。 （3）set -u，当你使用未初始化的变量时，让bash自动退出 （4）set -o pipefail 设置了这个选项以后，包含管道命令的语句的返回值，会变成最后一个返回非零的管道命令的返回值。听起来比较绕，其实也很简单： 例如test.sh set -o pipefail ls ./a.txt |echo &quot;hi&quot; &gt;/dev/null echo $? 运行test.sh，因为当前目录并不存在a.txt文件，输出：ls: ./a.txt: No such file or directory1 # 设置了set -o pipefail，返回从右往左第一个非零返回值，即ls的返回值1 注释掉set -o pipefail 这一行，再次运行，输出：ls: ./a.txt: No such file or directory0 # 没有set -o pipefail，默认返回最后一个管道命令的返回值 工作路径我们会先获取当前脚本的路径，然后一这个路径为基准，去找其他的路径。 work_dir=$1 reference=${work_dir}/data/reference/TAIR10_chr_all.fas 环境变量PATH一般情况下我们会将一些重要的环境变量定义在开头，确保脚本中使用的命令能被bash搜索到。 PATH=/bin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/bin 运行脚本chmod +x ./test.sh #给脚本权限 ./test.sh #执行脚本 Shell中的变量 “=”前后不能有空格 定义时不用$,使用时需要$,且推荐给所有变量加上花括号{} 脚本的参数 先定义具体含义，后使用 gene_file=$1 ${gene_file} 代码有注释 简述某一代码段的功能 各个函数前的说明注释 太长要分行在调用某些程序的时候，参数可能会很长，这时候为了保证较好的阅读体验，我们可以用反斜杠来分行： ./configure \ –prefix=/usr \ –sbin-path=/usr/sbin/nginx \ –conf-path=/etc/nginx/nginx.conf \ 日志例如： ./sub_gtf.shell gene.txt Homo_sapiens.GRCh38.89.chr.gtf &gt;logfile 2&gt;&amp;1 1 ：表示stdout标准输出，系统默认值是1，所以”&gt;logfile”等同于”1&gt;logfile”2 ：表示stderr标准错误&amp; ：表示等同于的意思，2&gt;&amp;1，表示2的输出重定向等同于1 回显例如 if [[ $# != 2 ]];then echo &quot;Parameter incorrect.&quot; exit 1 fi 当执行： ./sub_gtf.shell gene.txt 因为参数数目不对，输出Parameter incorrect.至屏幕 当执行： ./sub_gtf.shell gene.txt &gt;logfile 2&gt;&amp;1 同样参数数目不对，但输出Parameter incorrect.至日志 函数相关巧用main函数,使得代码可读性更强#!/bin/bash func1(){ #do sth } func2(){ #do sth } main(){ func1 func2 } main &quot;$@&quot; 考虑作用域shell中默认的变量作用域都是全局的，比如下面的脚本： #!/usr/bin/env bash var=1 func(){ var=2 } func echo $var 他的输出结果就是2而不是1，这样显然不符合我们的编码习惯，很容易造成一些问题。 &emsp;&emsp;因此，相比直接使用全局变量，我们最好使用local, readonly这类的命令，其次我们可以使用declare来声明变量。这些方式都比使用全局方式定义要好。 local一般用于局部变量声明，多在在函数内部使用。（1）shell脚本中定义的变量是global的，其作用域从被定义的地方开始，到shell结束或被显示删除的地方为止。 （2）shell函数定义的变量默认是global的，其作用域从“函数被调用时执行变量定义的地方”开始，到shell结束或被显示删除处为止。函数定义的变量可以被显示定义成local的，其作用域局限于函数内。但请注意，函数的参数是local的。 （3）如果同名，Shell函数定义的local变量会屏蔽脚本定义的global变量。所以在函数内声明的变量，请务必记得加上 local 限定词 使用举例： #!/bin/bash function Hello() { local text=&quot;Hello World!!!&quot; #局部变量 echo $text } Hello 只读变量使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变。 例如： readonly myUrl myUrl=&quot;http://www.runoob.com&quot; declare -r 只读 (declare -r var1与readonly var1作用相同) declare -r var1 -i 整数 declare -i number -a 数组 declare -a indices -f 函数 declare -f 函数返回值&emsp;&emsp;在使用函数的时候一定要注意，shell中函数的返回值只能是整数，估计是因为一般情况下一个函数的返回值通常表示这个函数的运行状态，所以一般都是0或者是１就够了，因此就设计成了这样。不过，如果非得想传递字符串，也可以通过下面变通的方法: func(){ echo &quot;2333&quot; } res=$(func) echo &quot;This is from $res.&quot; 这样，通过echo或者print之类的就可以做到传一些额外参数的目的。 使用新写法&emsp;&emsp;这里的新写法不是指有多厉害，而是指我们可能更希望使用较新引入的一些语法，更多是偏向代码风格的，比如 尽量使用func(){}来定义函数，而不是func{} 尽量使用[[]]来代替[] 尽量使用$()将命令的结果赋给变量，而不是反引号 在复杂的场 下尽量使用printf代替echo进行回显 事实上，这些新写法很多功能都比旧的写法要强大，用的时候就知道了。 其他小tip 读取文件时不要使用for loop而要使用while read 简单的if尽量使用&amp;&amp; ||，写成单行。比如[[ x &gt; 2]] &amp;&amp; echo x 利用/dev/null过滤不友好或者无用的输出信息 例如 if grep ‘pattern1’ some.file &gt; /dev/null &amp;&amp; grep ‘pattern2’ some.file &gt; dev/null then echo “found ‘pattern1’ and ‘pattern2’ in some.file” fi /dev/null ：代表空设备文件 安装shellcheckShellCheck, a static analysis tool for shell scripts shellcheck 除了可以提醒语法问题以外，还能检查出 shell 脚本编写常见的 bad code。 使用方式（１）网页版： http://www.shellcheck.net github仓库： https://github.com/koalaman/shellcheck 下载安装： wget -q https://storage.googleapis.com/shellcheck/shellcheck-latest.linux.x86_64.tar.xz xz -d shellcheck-latest.linux.x86_64.tar.xz tar -xvf shellcheck-latest.linux.x86_64.tar echo ‘export PATH=/home/wangdong/softwares/shellcheck:$PATH’&gt;&gt;~/.bashrc source ~/.bashrc 使用方式（2）终端： shellcheck yourscipts Shell不能做什么 需要精密的运算的时候 需要语言效率很高的时候 需要一些网络操作的时候 总之Shell就是可以快速开发一个脚本简化开发流程，并不可以用来替代高级语言 &emsp;&emsp;解决特定的问题要用合适的工具。知道什么时候用 shell，什么时候切换到另外一门更通用的脚本语言（比如ruby/python/perl），这也是编写可靠 shell 脚本的诀窍。如果你的任务可以组合常见的命令来完成，而且只涉及简单的数据，那么 shell 脚本就是适合的锤子。如果你的任务包含较为复杂的逻辑，而且数据结构复杂，那么你需要用ruby/python之类的语言编写脚本。 参考：（1）https://blog.mythsman.com/2017/07/23/1/ （2）https://github.com/koalaman/shellcheck （3）https://segmentfault.com/a/1190000006900083]]></content>
      <categories>
        <category>shell脚本</category>
      </categories>
      <tags>
        <tag>shell脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计sanger测序引物，验证突变位点]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F12%2F%E8%AE%BE%E8%AE%A1sanger%E6%B5%8B%E5%BA%8F%E5%BC%95%E7%89%A9%EF%BC%8C%E9%AA%8C%E8%AF%81%E7%AA%81%E5%8F%98%E4%BD%8D%E7%82%B9%2F</url>
    <content type="text"><![CDATA[在进行Variation calling 分析之后的故事 &emsp;&emsp;在进行Variation calling 分析之后。首先需要进行过滤，一般基于frequency和function两方面进行突变的过滤。 frequency过滤：根据千人基因组测序项目的等位基因频率预测的0.5%作为阈值，小于这一值的认为是突变，因为千人基因组对于我们检测到的变异属于健康对照，若该变异在健康对照的频率过高，则是致病突变的可能性就比较低了。 function过滤：一般看exonic区的non-synonymous，frameshift deletion，frameshift insertion，nonframeshift deletion，nonframeshift insertion，nonsynonymous SNV，stopgain，synonymous SNV等。 &emsp;&emsp;接着需要使用IGV查看变异位点的可靠性，并没有具体的标准。 一般先看看变异位点是否在reads的两端，若是，可能是接头没去干净； 其次看看具有该变异位点的reads占总reads的占比，一般保留&gt;50%的； 最后看看变异位点两端是否”干净”; 另外，如果有家系的数据，可以对比着看看家庭其他成员在该位点是否有相同的变异，并推测可能的遗传方式。 &emsp;&emsp;当然，以上方法得到的变异只能是初步的结果，还必须通过一代测序进行突变真实性的验证，今天的正题就是如何设计sanger测序引物？ 获取指定区域DNA序列网址： http://hgsv.washington.edu/cgi-bin/hgc?hgsid=2655438_sG5Zu9tXr3MZHMAtoJMdHACABHm4&amp;o=8420409&amp;g=getDna&amp;i=mixed&amp;c=chr21&amp;l=8420409&amp;r=8420416&amp;db=hg19&amp;hgsid=2655438_sG5Zu9tXr3MZHMAtoJMdHACABHm4 贴入位点，如chr7:74009352-74009352 先在UCSC中输入位点前500，后500个碱基， 点击get DNA ,复制结果 Primer-BLAST 设计引物网址： https://www.ncbi.nlm.nih.gov/tools/primer-blast/ PCR product size 取300-500之间主要因为节省测序成本，同时，如果设计的引物没有中点在500附近的，主要可以通过调整PCR product size的上限，这样获得的产物长点就长点吧！ Get primers NCBI的Primer Blast 中这里选择的Genome(reference assembly from selected organisms)实际上是GRCh38版本。细心的可能注意到了，第一步获取序列用的是hg19版本的，但没关系，还有第三步UCSC In-Silico PCR验证呢！这一步选择的是hg19版本，这一步会进行blast!所以primer blast 和另两步的版本不同对结果不会有影响。 点击submit，之后比较慢，等等吧！（同时完成引物的设计和blast，主要是blast比较耗时！） 一般第一条结果比较好，但是如果给出的若干个引物中有一条的中心更趋近与500处（待检测位点位于中间有利于测序，因为一般测序结果两端50bp会出现杂峰），则最好选择这一条，翻查下面对应的引物的正反向序列。 UCSC In-Silico PCR验证引物网址： https://genome.ucsc.edu/cgi-bin/hgPcr?hgsid=603243225_C0DlViEzt0mvZEqK0DLtJx4pfsRN &emsp;&emsp;UCSC的PCR选项为电子PCR，输入引物（&gt;15bp），即可得到两引物间序列。UCSC是基于基因组而非转录组，如果两引物间隔很大，则先调节Max Product Size。UCSC自动预测的TM值是基于primer3的，跟我们用的DNAman算出来的值也比较接近，PCR时可直接使用其退火温度。 用法：打开UCSC中的in silicon PCR，将上下游引物分别输入，可以选择物种，基因组，产物长度等，submit即可 Submit后，如果结果只对应处一条染色体上的一个位点，且primer melting温度控制在2度左右差范围即可，如下 整理结果：]]></content>
      <categories>
        <category>NGS</category>
      </categories>
      <tags>
        <tag>NGS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基因组数据下载]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F12%2F%E5%8F%82%E8%80%83%E5%9F%BA%E5%9B%A0%E7%BB%84%E6%95%B0%E6%8D%AE%E4%B8%8B%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;常用到的基因组数据格式括:fasta,fastq,gff,GenBank format, EMBL format；常用的基因组数据库包括：（1）Ensembl基因组注释数据库；（2）UCUS基因组浏览器 &emsp;&emsp;常用到的基因组数据格式括:fasta,fastq,gff,GenBank format, EMBL format；常用的基因组数据库包括：（1）Ensembl基因组注释数据库；（2）UCUS基因组浏览器 （1）通过Ensembl基因组注释数据库下载网址：http://www.ensembl.org/info/data/ftp/index.html 下载数据前一定要仔细查看相应目录下的README文件 基因组序列下载:&emsp;&emsp;Ensembl提供的参考基因组有2种组装形式和3种重复序列处理方式, 分别是primary, toplevel和unmasked (dna)、soft-masked (dna_sm)和masked (dna_rm)。一般选择dna.primary或dna_sm.primary。 为什么选择PrimaryPrimary assembly contains all toplevel sequence regions excluding haplotypes and patches. This file is best used for performing sequence similarity searcheswhere patch and haplotype sequences would confuse analysis. 为什么不选择masked&emsp;&emsp;Masked基因组是指所有重复区和低复杂区被N代替的基因组序列，比对时就不会有reads比对到这些区域。一般不推荐用masked的基因组，因为它造成了信息的丢失，由此带来的一个问题是uniquely比对到masked基因组上的reads实际上可能不是unique的。而且masked基因组还会带来比对错误，使得在允许错配的情况下，本来来自重复区的reads比对到基因组的其它位置。 另外检测重复区和低复杂区的软件不可能是完美的，这就造成遮盖住的重复序列和低复杂区并不一定是100%准确和敏感的。 &emsp;&emsp;soft-masked基因组是指把所有重复区和低复杂区的序列用小写字母标出的基因组，由于主要的比对软件，比如BWA、bowtie2等都忽略这些soft-mask，直接把小写字母当做大写字母比对，所以使用soft-masked基因组的比对效果和使用unmasked基因组的比对效果是相同的。 (1)文件命名规则： \.\.\.\.\.fa.gz species:物种的名称 assembly:基因组的版本 sequence type（主要有三类）: ‘dna’ - unmasked genomic DNA sequences. ‘dna_rm’ - masked genomic DNA.通过RepeatMasker软件 检测弥散的重复序列和低复杂度的区域，并将重复序列使用N替代。 ‘dna_sm’ - soft-masked genomic DNA. 指Soft-masked的DNA序列，其中的重复序列和低复杂度的区域会用其相应碱基的小写字母来表示 举个栗子（1）连续的N &gt;Homo_sapiens.GRCh38.dna_rm.chromosome.15.fa NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNTTAGAGACCTTGAGA GGAATTAAACATCTCTGTGAGTATATGCTGTAGGGCTTTGCTGCACTGTCCTTGGAGGCT （2）小写字母表示碱基 &gt;Homo_sapiens.GRCh38.dna_sm.chromosome.X.fa nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnctaaccctaaccctaaccct aaccctaaccctaaccctctgaaagtggacctatcagcaggatgtgggtgggagcagatt gacaacCCCTAGAAGAGCACCTGGTTGAtaacccagttcccatctgggatttaggggacc aggggccctgggacagccctgtacatgagctcctggtctgtaacacagttcccctgtggg gatttagggACTTGGGCCTTCTGTCTTTGGGATCTACTCTCTATGGGCCACACAGATATG &emsp;&emsp;所以，使用RepeatMasker和Tandem Repeats Finder (with period of 12 or less)识别的重复在dna_rm中用大写的N表示，而在dna_sm中用小写字母表示，而非重复序列用大写字母表示。至于序列的开头和结尾，无论是dna_rm还是dna_sm，N/n表示（A,C,G,T）任意一种，也叫做Gaps，它意味着真实具体是哪一种碱基，测序平台不能确定，N/n的数目可能是不确定碱基数,都被一并maked掉了，在序列分析的时候并不会分析。 为什么要这样做呢？&emsp;&emsp;RepeatMasker是一款基于Library-based，通过相似性比对来识别重复序列，可以屏蔽序列中转座子重复序列和低复杂度序列（默认将其替换成N），几乎用于所有物种，是做基因组、非编码RNA的必备软件。在人类基因组分析当中，大约 56% 的序列会被mask； RepeatMasker在进行序列比对时可以选用常见的几种算法，包括nhmmer、cross_match、ABBlast/WUBlast、RMBlast 、Decypher（可以安装多个比对引擎，但每次只能使用其中一个）。 &emsp;&emsp;RepeatMasker应该是做已知repeat和TE的。基本原理是用已知repeats去blast。所以潜在的未知重复序列应该是无法用repeatmasker找到的，而在mRNA中的repeat序列也不应该用RepeatMasker找。repeatmasker的实际意义，就象这个软件的名字一样，是为了mask掉repeat。从而在查找基因，鉴定有功能的ncRNA，或者设计引物等提供一个精简的序列。毕竟用带有大量TE和repeat的序列做引物设计或者基因和蛋白功能分析，最后验证的时候只是得到了一批repeat 重复序列的种类： Tandem repeats 串连重复 Satellite DNA 卫星DNA Variable number tandem repeat /Minisatellite 小卫星 Short tandem repeat（STR）/Microsatellite (Trinucleotide repeat disorders)微卫星 Interspersed repeats 散落重复 Transposon (Transposable elements (TEs) )转座子 Retrotransposon 反转录转座子 SINEs – Alu sequence, MIR 短散落元件 LINEs – LINE1, LINE2 长散落元件 LTRs – HERV, MER4, retroposon 长末端重复 DNA transposon DNA转座子 MER1, MER2, Mariners TIR（Terminal Inverted Repeat） 末端方向重复 Genomic island Genomic island &emsp;&emsp;这种对RepeatMasker 和 Tandem Repeats Finder用小写表示的方式，可以用于UCSC的Genome Browser来展示重复序列。 id type: 可选值包括chromosome,nonchromosomal,seqlevel。 （1）chromosome就不用说了； （2）nonchromosal中包含了暂时未能确定染色体的序列； （3）seqlevel包括sequence scaffolds, chunks 或者clones三个层次。 ‘scaffold’：通过短的测序reads拼接组装成更大的序列—-contigs（通常来自全基因组鸟枪测序，WGS），但是还不能组成到染色体的程度。通常还需要更多的测序，来消除gaps并确定顺序（tiling path） ‘chunk’:当contigs序列能被组装成大的实体时，有时候必须认为的将其打断为更小的实体，称之为’chunks’。这是由于注释管道的限制并且受存储序列和注释信息的MySQL数据库的记录限制。 ‘clone’ ：通常这是最小的序列实体。它经常与一个BAC clone或者部分区域的序列是一样的。 id:实际序列的标示符，和\对应 fa:表示文件格式为FASTA gz:文件压缩方式为GUN Zip toplevel文件中包含了所有的在ensembl数据库schema中被定义为toplevel的序列，包括染色体，未能组装到染色体上的区域以及含有N的haplotype/patch区域。 primary_assembly文件相比于toplevel文件，减去了含有N的haplotype/patch区域。这类文件比较适合用来比对。 除基因组注释文件（GTF或者GFF）下载&emsp;&emsp;例如在RNA-seq分析流程中，参考基因组序列用于reads的比对，而GTF或者GFF用于确定比对上的reads是否落在基因内，由此来相对定量基因的表达量，鉴定差异表达基因。 (1)文件命名规则：\.\.\.gtf.gz 注释基因生物证据的比对（例如蛋白，cdna，RNA-seq）来组装基因组 对于预测的基因集：\.\.\.abinitio.gtf.gz 这里的预测是指通过GenScan和其他的从头预测工具分析产生的基因。 确定了需要下载什么后，那如何下载呢？可以通过浏览器下载，但需要再上传到服务器；也可以通过之前介绍的多线程下载工具axel下载，速度很快。当然还可以使用ensembl提供的rsync工具下载。 （2）通过UCSC下载基因组注释数据 Genome Browser’s “Table Browser”： http://genome.ucsc.edu/cgi-bin/hgTables?command=start Bulk Downloads page： http://hgdownload.cse.ucsc.edu/downloads.html &emsp;&emsp;对于NGS的序列分析来说，如果没有reference genome和genome annotation下游的分析可能根本无法进行。UCSC提供的数据分为sequence和annotation两大类，对于人、小鼠在内的71种脊椎动物，可以从UCSC同时下载sequence和annotation，其他的一般只有sequence。 基因组的版本： &emsp;&emsp;不同的生物信息学数据库对于基因组的命名方式各不相同。以人为例，NCBI/ENSEMBL用GRCh系列命名，而UCSC则使用hg系列命名。这两套命名系统背后的版本对应关系如下： UCSC NCBI hg18 GRCh36 hg19 GRCh37 hg38 GRCh38 hg系列和GRCh系列主要的差别有两处： （1）hg系列的染色体命名是”chr”+染色体号，而GRCh系列的染色体没有前缀的”chr”； （2）hg系列序列是0-based（第一个核苷酸记0），GRCh系列是1-based（第一个核苷酸记1，两种计数方法的区别参见《基因组的坐标系统》）。 （1）从FTP站点获取 获取sequence UCSC的reference genome是分染色体保存的。 对于hg38来说，单个的染色体序列可以在以下网址下载： http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/ 所有染色体打包好的文件在以下网址下载： http://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.chromFa.tar.gz 如果需要下载其他版本（19/18），可以把上述链接中的hg38换成hg19/hg18。如果需要将所有的染色体序列合并到一个文件中，可以在下载完成后依次执行下列命令（POSIX compatible）： tar –xzvf hg38.chromFa.tar.gz cd hg38 cat *.fa &amp;gt; hg38.fa 获取注释 UCSC提供了SNP、RepeatMask、refSeq、GENCODE等注释文件。 但是在UCSC的FTP站点： http://hgdownload.soe.ucsc.edu/goldenPath/hg38/database 这些数据被分成了两个文件——一个是.sql结尾的SQL语句文件，描述了数据表的结构和创建数据表的方法；另一个是.txt.gz结尾的数据文件。我们可以通过.sql文件来查看表的结构，再把.txt.gz格式的文件解压后转换成所需要的格式。下面是将.txt.gz的数据文件转换成gtf格式的三个例子： RepeatMask gunzip rmsk.txt.gz gawk &apos;OFS=&quot;\t&quot;{print $6,&quot;rmsk &quot;,$12,$7+1,$8,&quot;.&quot;,$10,&quot;.&quot;,&quot; repName \&quot;&quot;$11&quot;\&quot;; repFamily \&quot;&quot;$13&quot;\&quot;;&quot;}&apos; rmsk.txt &amp;gt; rmsk.gtf simpleRepeat gunzip simpleRepeat.txt.gz gawk &apos;OFS=&quot;\t&quot;{print $2,&quot; simpleRepeat&quot;,&quot;trf&quot;,$3+1,$4,&quot;.&quot;,&quot;+&quot;,&quot;.&quot;,&quot;name \&quot;&quot;$5&quot;\&quot;; sequence \&quot;&quot;$17&quot;\&quot;;&quot;}&apos; simpleRepeat.txt &amp;gt; simpleRepeat.gtf RefSeq &emsp;&emsp;UCSC为RefSeq和GENCODE等以genePred形式保存的注释文件提供了专门的格式转换程序——genePredToBed、genePredToGenePred、genePredToFakePsl、genePredToGtf、genePredToMafFrames 各自的使用方法参见： http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/FOOTER 若需将RefSeq转存为gtf格式，可参考下列命令： gunzip refGene.txt.gz cut -f 2- refGene.txt | genePredToGtf -utr -source=hg38 file stdin refGene.gtf （2）从Table Browser获取针对annotation，UCSC还通过Table Browser页面 http://genome.ucsc.edu/cgi-bin/hgTables 提供了一个更加友好的获取方法。Table Browser的使用基本使用方法可以参考 http://genome.ucsc.edu/goldenPath/help/hgTablesHelp.html#GettingStarted 为了避免和官方帮助文档的重叠，我仅在这里分享在做REP项目过程中发现的几种比较tricky的用法。 （1）获取GENCODE转录本ID和Gene Symbol的映射 clade设置为”Mammal”, genome设置为”Human”，assembly设置为”Dec. 2013 (GRCh38/hg38)”； 将group设置为”Genes and Gene Predictions”，track设置为”All GENCODE V24”； Table设置为”Basic (wgEncodeGencodeBasicV24)”； Output format设置为”selected fields from primary and related tables”； 点击”get output”； 在新页面中勾选”name”和”name2”前的复选框； 点击”get output”即可； （2）获取5’UTR/3’ UTR/Coding Exons/Intron的BED文件 clade设置为”Mammal”, genome设置为”Human”，assembly设置为”Dec. 2013 (GRCh38/hg38)”； 将group设置为”Genes and Gene Predictions”，track设置为”All GENCODE V24”； Table设置为”Basic (wgEncodeGencodeBasicV24)”； Output format设置为”BED – browser extensible data”； 点击”get output”； 在新页面中勾选自己需要的elements； 点击”get BED”即可。 （3）获取指定范围的序列 UCSC可以通过使用符合自己需要的注释数据，然后再获取进一步的数据。操作方法如下： 点击Table Browser的”add custom tracks”按钮； 在”Paste URLs or data”中添加数据的地址或者使用旁边的文件上传框上传文件，点击”Submit”； 在新页面中选择view in“Table Brower”，点击”go”，这时会跳回Table Browser； 这个时候将region选为“genome”，将Output format设置成”sequence”，结果可以选择“plain text”呈现或者“gzip compressed”下载，点击”get output”即可获取指定范围内的序列。 （4）Table Browser使用过程中可能会遇到的问题 由于抽取数据脚本执行超时（&gt;10min）或者下载地的网络不佳，下载下来的文件可能会不完整； 获取3’ UTR等序列时，若直接将track指定为系统自带的annotation，会有概率出现序列的start与annotation中不符的情况（0-based和1-based的杂合），建议先下载BED文件，然后通过前述的“获取指定范围的序列”来下载序列。 参考：（1）为什么序列分析要repeatmasker： http://www.dxy.cn/bbs/topic/9424163 （2）博耘生物： http://boyun.sh.cn/bio/?p=1845 （3）linux进进阶屋： http://sookk8.blog.51cto.com/455855/328076/ （4）BioDog的博客： https://www.yaolibio.com/2016/09/01/retrieve-genome-data-from-ucsc/ （5）6有才 http://www.jianshu.com/p/542c78a8ee0a （6）生信宝典 https://mp.weixin.qq.com/s/2OoXy4f1t0hE8OUqsAt1kw]]></content>
      <categories>
        <category>NGS</category>
      </categories>
      <tags>
        <tag>NGS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux多线程下载工具axel编译安装]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F06%2FLinux%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8B%E8%BD%BD%E5%B7%A5%E5%85%B7axel%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;Axel插件是基于yum下的一个多线程下载插件。axel插件也可以当独立下载工具来使用。当成独立下载工具使用时，适用于所有Linux发行版。通过打开多个HTTP/FTP连接来将一个文件进行分段下载，从而达到加速下载的目的。对于下载大文件，该工具特别有用。同时支持断点续传，速度通常情况下是Wget的几倍。可用于CentOS、RHEL、Fedora等使用yum的Linux发行版。由于没有管理员权限，只能编译安装源码。使用Axel可以在低速网络环境里提高数倍的下载速度。 Linux多线程下载工具axel编译安装&emsp;&emsp;Axel插件是基于yum下的一个多线程下载插件。axel插件也可以当独立下载工具来使用。当成独立下载工具使用时，适用于所有Linux发行版。通过打开多个HTTP/FTP连接来将一个文件进行分段下载，从而达到加速下载的目的。对于下载大文件，该工具特别有用。同时支持断点续传，速度通常情况下是Wget的几倍。可用于CentOS、RHEL、Fedora等使用yum的Linux发行版。由于没有管理员权限，只能编译安装源码。使用Axel可以在低速网络环境里提高数倍的下载速度。 官方主页:http://axel.alioth.debian.org/ 下载安装下载： wget https://sourceforge.net/projects/axel2/files/axel-2.4/axel-2.4.tar.gz tar -xzf axel-2.4.tar.gz cd axel-2.4 编译安装： ./configure --prefix=/home/u641750/axel-2.4 make &amp;&amp; make install 添加环境变量： echo &apos;export PATH=/home/u641750/axel-2.4/bin:$PATH&apos;&gt;&gt;~/.bashrc source ~/.bashrc 使用参数如下： –max-speed=x , -s x 最高速度x，指定每秒的最大比特数 –num-connections=x , -n x 指定线程数 –output=f , -o f 指定另存为目录f –search[=x] , -S [x] 搜索镜像 –header=x , -H x 添加头文件字符串x（指定 HTTP header） –user-agent=x , -U x 设置用户代理（指定 HTTP user agent –no-proxy ， -N 不使用代理服务器 –quiet ， -q 静默模式 –verbose ，-v 更多状态信息 –alternate ， -a Alternate progress indicator –help ，-h 帮助 –version ，-V 版本信息 测试例如下载Python安装包：time axel -n 10 http://mirrors.sohu.com/python/3.4.1/Python-3.4.1.tar.xz time wget http://mirrors.sohu.com/python/3.4.1/Python-3.4.1.tar.xz 如果下载过程中下载中断可以再执行下载命令即可恢复上次的下载进度 当然，linux也还有其他的多线程下载工具，比如myget。有人测试，axel、myget支持多线程，且速度较快都在2M。断点续传对比，三个工具都支持，但wget需要增加-c参数，axel、myget再次执行命令即可。 例如： wget -c http://url/iso/Centos/x86_64/CentOS-6.4-x86_64-bin-DVD1.iso 参考：（1）http://www.ha97.com/621.html （2）http://man.linuxde.net/axel （3）Dreamway的运维点滴（推荐阅读） http://dreamway.blog.51cto.com/1281816/1151886/]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SRA数据加速下载打包解决]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F06%2FSRA%E6%A0%BC%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%8A%A0%E9%80%9F%E4%B8%8B%E8%BD%BD%E6%89%93%E5%8C%85%E8%A7%A3%E5%86%B3%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;NCBI维护Short Read Archive (SRA)作为大规模平行测序（NGS）项目产生的数据仓库。这些方法在单个循环中能产生数百万碱基至千兆级碱基的数据，是标准Sanger测序仪输出的数百万倍。运用这些技术，包括新基因组的测序，捕获基因组区域测序，多个个体的完整基因组测序来寻找变异，转录组测序来研究样本可变剪切变异和表达水平，环境样本和其他宏基因组，染色质DNA结合蛋白分析等。SRA数据库可以用于搜索和展示SRA项目数据，包括SRA主页和 Entrez system。 &emsp;&emsp;NCBI维护Short Read Archive (SRA)作为大规模平行测序（NGS）项目产生的数据仓库。这些方法在单个循环中能产生数百万碱基至千兆级碱基的数据，是标准Sanger测序仪输出的数百万倍。运用这些技术，包括新基因组的测序，捕获基因组区域测序，多个个体的完整基因组测序来寻找变异，转录组测序来研究样本可变剪切变异和表达水平，环境样本和其他宏基因组，染色质DNA结合蛋白分析等。SRA数据库可以用于搜索和展示SRA项目数据，包括SRA主页和 Entrez system。 SRA下载方式：（1）Aspera（2）sratoolkit（3）FTP 比较：通过sratoolkit，可以直接下载成fastq格式，速度比ftp快，比aspera慢。 Aspera关于速铂Aspera &emsp;&emsp;速铂Aspera是一套商业的高速文件传输解决方案，随着高通量数据的大量产生，从而对于大文件快速传输的需求，开始应用到生物领域，目前NCBI、EBI的SRA库都提供这样的服务。 &emsp;&emsp;传统的FTP、HTTP等数据传输协议都是基于TCP的，TCP在远距离数据传输中存在一些先天的不足，文件越大、距离越远，其丢包、延时等问题对于传输速度的影响就越大。 Aspera使用的两种方式（1）客户端的下载与安装 &emsp;&emsp;即便Aspera是商业软件，但是作为客户应用方（相对于NCBI），我们使用其客户对进行数据的上传与下载是不用支付费用的。 &emsp;&emsp;网页下载：速度很快，不过需要把数据再上传到服务器上，多费一道工序下载网页版AsperaConnectML-3.5.2.97180.msi安装，网页上下载SRA数据时点aspera下载链接就可以。 客户端下载链接： http://downloads.asperasoft.com/connect2/ 设置下载目录及速度限制等： 至此，客户端工具准备妥当了~ （2）使用ascp下载SRA数据：ascp是Aspera Connect的命令行程序。 下载与安装（不需要root或者sudo权限）： curl -O http://download.asperasoft.com/download/sw/connect/3.6.1/aspera-connect-3.6.1.110647-linux-64.tar.gz tar zxf asper-commect-3.6.1.110647-linux.tar.gz sh aspera-connect-2.4.7.37118-linux-64.sh 添加环境变量： export PATH=&quot;/home/u641750/.aspera/connect/bin:$PATH&quot; 可以将密钥备份到/home/的家目录下方便使用（后文将用到）: $ cp ~/.aspera/connect/etc/asperaweb_id_dsa.openssh ~/ 至此，命令行工具准备妥当了~ 那么如何找到我们要下载的SRA数据呢？首先我们需要了解下NSBI的SRA数据结构的层次关系： NCBI官网说明：https://www.ncbi.nlm.nih.gov/books/NBK7522/ &emsp;&emsp;SRA数据库中的数据分为Studies, Experiments, Samples和相应的Runs四个层次。Studies有一个总体目标并可能包含数个Experiments。一个Experiments描述具体测了什么和使用的方法。它包括DNA来源信息，样本，测序平台，数据处理。每个Experiments由一个或者多个Runs组成。一个Run包含来自每个spot的reads结果。在未来，一些数据将具有相关分析。这些分析可能包括short reads组装为基因组或者转录组的contigs，现有基因组的比对，SRA数据的比对。每个水平的记录具有唯一的accession identifiers ，并且具有三个大写字母前缀： &emsp;&emsp;NCBI中SRA数据结构的层次关系： Studies: SRA Study accessions (prefixes SRP, DRP, ERP) Examples: SRP000002, DRP000617, ERP002000 BioProject accessions (prefixes PRJNA, PRJDB, PRJEB) Examples: PRJNA111397, PRJDB90, PRJEB1976 dbGaP study accessions (prefix phs) Example: phs000159 GEO Study (prefix GSE) Example: GSE12578 Samples: SRA Sample accessions (prefixes SRS, DRS, ERS) Examples: SRS000013, DRS000020, ERS000016 BioSample accessions (prefixes SAMN, SAME) Examples: SAMN00000013, SAMEA774460 GEO Sample (prefix GSM) Example: GSM769008 SRA Experiment(s) SRA Experiment accessions (prefixes SRX, DRX, ERX) Example: SRX000002,SRX000003,SRX000004 Figure 2 shows Study (SRP000095, top panel), Experiment (SRX000113, middle panel, and SRX000114), and Run (SRR000416, bottom panel) records for the 454 sequencing of James Watson’s genome by Cold Spring Harbor Laboratory. Study and Run records are displayed in the SRA browser. The corresponding Experiment records are displayed in the NCBI Entrez system as described in the next section. 在SRA浏览页面和Entrez可以搜索和查看SRA数据 Studies, Runs和它们相关的Samples可以通过SRA主页浏览和查看： www.ncbi.nlm.nih.gov/Traces/sra Experiment记录可以通过搜索 Entrez SRA数据库获得： www.ncbi.nlm.nih.gov/sites/entrez?db=sra 接下来具体介绍：搜索地址：https://www.ncbi.nlm.nih.gov/Traces/study/ （1）通过以上网址，查询得到SRA数据的SRA Experiment accessions (prefixes SRX, DRX, ERX) （2）在NCBI搜索SRA Experiment accessions，例如：SRX000004 点击Download data： 此时鼠标选中SRX实验或者任意一个SRR记录，通过Aspera client客户端下载。但这样需要等下载完再使用FTP上传到自己的服务器，前面提过，FTP速度很慢！！！ 那么如何在服务器使用ascp命令行工具下载呢？命令格式： ascp -i &lt;path-to-asperaweb_id_dsa.openssh&gt; -k1 -QTr –l200m anonftp@ftp-private.ncbi.nlm.nih.gov:/&lt;files to transfer&gt; &lt;local destination&gt; 相关的参数 –Q (for adaptive flow control) – needed for disk throttling! –T to disable encryption –k1 enable resume of failed transfers –l (maximum bandwidth of request, try 200M and go up from there) –r recursive copy –i &lt;密钥文件&gt; 表明下载存放路径，一定要有，缺少会报错！！！关键是如何获取，也就是你要下载的SRR数据的地址，并且一定要保证其存在，否则会报错！！！ 将鼠标选中上图任意一个SRR文件，例如SRR00006.sra,右键，复制链接地址：fasp://anonftp@ftp.ncbi.nlm.nih.gov:22/sra/sra-instant/reads/ByExp/sra/SRX/SRX000/SRX000004/SRR000006/SRR000006.sra?auth=no&amp;port=33001&amp;bwcap=300000&amp;targetrate=100p&amp;policy=fair&amp;enc=none&amp;lockpolicy=no&amp;locktargetrate=no&amp;lockminrate=no&amp;v=2 摘取/sra/sra-instant/reads/ByExp/sra/SRX/SRX000/SRX000004/SRR000006/SRR000006.sra部分即为 网上有其他教程说遵循如/sra/sra-instant/reads/ByRun/sra/SRR/SRR689/SRR689250/SRR689250.sra固定的格式，但实际并非如此，比如上面的例子，所以并不能图省事硬套上述格式，还是要再win下如上述方法找到文件具体的地址，摘取部分，以确保文件存在，否则会报错：“ascp: no remote host specified, exiting” 举个栗子： （1）单个文件下载： ascp -i ~/asperaweb_id_dsa.openssh -k1 -Tr -l100m anonftp@ftp-private.ncbi.nlm.nih.gov:/sra/sra-instant/reads/ByExp/sra/SRX/SRX000/SRX000004/SRR000006/SRR000006.sra ~ （2）批量下载： 观察发现，一个SRX Experiment accessions下的是有规律的，如win下的目录结构所示,只是后两个字段不同：/sra/sra-instant/reads/ByExp/sra/SRX/SRX000/SRX000004/SRR000006/SRR000006 因此可以整理为下面的格式黏贴在文本SRR_Download_List_file_list.txt 中： /sra/sra-instant/reads/ByExp/sra/SRX/SRX000/SRX000004/SRR000006/SRR000006.sra/sra/sra-instant/reads/ByExp/sra/SRX/SRX000/SRX000004/SRR000009/SRR000009.sra/sra/sra-instant/reads/ByExp/sra/SRX/SRX000/SRX000004/SRR000010/SRR000010.sra …….等 ascp -i ~/asperaweb_id_dsa.openssh --mode recv --host ftp-private.ncbi.nlm.nih.gov --user anonftp --file-list SRR_Download_List_file_list.txt ~ 如此可以实现批量下载！ 使用后会发现，从NCBI上下载SRA速度，一般的宽带的话，也可以达到100M/s，大大节约了下载的时间，非常给力 注意事项： （1）如果报错：Error: Server aborted session: Client requests stronger encryption than server allows，那么可以参考：https://support.asperasoft.com/hc/en-us/articles/216126788-Error-Client-requests-stronger-encryption-than-server-allows 对客户端和命令行两种方式都给出了解决方案。在linux命令行下也就是加个-T参数，即： ascp -T -i ~/asperaweb_id_dsa.openssh --mode recv --host ftp-private.ncbi.nlm.nih.gov --user anonftp --file-list SRR_Download_List_file_list.txt ./ （2）放入后台下载，这样不用担心关闭客户端，下载也停止了~ nohup ascp -T -i ~/asperaweb_id_dsa.openssh --mode recv --host ftp-private.ncbi.nlm.nih.gov --user anonftp --file-list SRR_Download_List_file_list.txt ./ &amp; （3）aspera默认不支持断点续传，要支持这个功能添加参数： ascp -k1 -T -i ~/asperaweb_id_dsa.openssh --mode recv --host ftp-private.ncbi.nlm.nih.gov --user anonftp --file-list SRR_Download_List_file_list.txt ./ 所以 ascp -i ~/asperaweb_id_dsa.openssh -k1 -Tr -l100m anonftp@ftp-private.ncbi.nlm.nih.gov:命令可以通用 （4）从EBI上下载也类似，给个例子： ascp -i ~/asperaweb_id_dsa.putty era-fasp@fasp.sra.ebi.ac.uk:/vol1/ERA012/ERA012008/sff/library08_GJ6U61T06.sff 提供Aspera的数据库： （1）NCBI的Sequence Read Archive (SRA), dbGaP. （2）1000genomes – EBI Aspera site, the NCBI Aspera site 1000genomes – EBI Aspera site: http://www.internationalgenome.org/aspera 1000genomes – the NCBI Aspera site https://www.ncbi.nlm.nih.gov/projects/faspftp/1000genomes/ sra数据转为fastaq&emsp;&emsp;sra是NCBI 推出的存储高通量数据的格式，而平常我们工作用得多是fastq格式。如果需要把sra 转成fastq，则下载NCBI SRA Toolkit。 下载地址：https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?cmd=show&amp;f=software&amp;m=software&amp;s=software (1)下载安装（CentOS Linux 64 bit architecture）： curl -O https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.8.2-1/sratoolkit.2.8.2-1-centos_linux64.tar.gz tar xzvf sratoolkit.2.8.2-1-centos_linux64.tar.gz cd sratoolkit.2.8.2-1-centos_linux64 程序都在bin目录下，来看看有什么： cd bin SRA Toolkit Documentation, Frequently Used Tools: fastq-dump: Convert SRA data into fastq format prefetch: Allows command-line downloading of SRA, dbGaP, and ADSP data sam-dump: Convert SRA data to sam format sra-pileup: Generate pileup statistics on aligned SRA data vdb-config: Display and modify VDB configuration information vdb-decrypt: Decrypt non-SRA dbGaP data (“phenotype data”) (2)添加环境变量 echo &apos;export PATH=/home/wangdong/softwares/sratoolkit.2.8.2-centos_linux64/bin&apos;&gt;&gt;~/.bashrc source ~/.bashrc (3)使用： 使用prefetch下载SRA数据下载文件: （1）单个下载 prefetch SRR1553610 （2）批量下载 for i in $(seq 58 79);do prefetch -v SRR8287$i ;done 这些文件区哪儿了？这些文件去哪里了？存在了你home目录下的一个默认文件夹里。 ls ~/ncbi 从NCBI下下来的数据，双端测序数据是放在一个文件里的，所以需要把它们重新拆解为两个文件。我们用程序fastq-dump来把文件拆包 fastq-dump --split-files SRR1553610 那么我怎么知道哪些数据是双端测序的呢？上文的网址关于NSBI的SRA数据结构的Study层次对实验方法有具体介绍：网址再贴一遍：https://www.ncbi.nlm.nih.gov/Traces/study/ 小细节之拆包后文件的命名： File name Description SRR030257_1.fastq Paired-end Illumina, First of pair, FASTQ format SRR030257_2.fastq Paired-end Illumina, Second of pair, FASTQ format 因为在后续分析，mapping到RefSeq上时，单端测序和双端测序命令有些不同！需要注意下。 更多的说明,请参见官方的SRA下载手册:NCBI: https://www.ncbi.nlm.nih.gov/books/NBK242625/ EBI: http://www.ebi.ac.uk/ena/about/sra_data_download 最后，附上FTP下载方式： 三大数据库的FTP地址： ensembl : ftp://ftp.ensembl.org/pubNCBI : ftp://ftp.ncbi.nih.gov/genomes/UCSC：ftp://hgdownload.soe.ucsc.edu/goldenPath 使用Xftp5 匿名登录FTP站点即可下载资源，慢点就慢点吧~有时需要下载的文件也就1-2M 参考：（1）博耘生物： http://boyun.sh.cn/bio/?p=1933 （2）Keep Learning的博客 http://blog.csdn.net/xubo245/article/details/50513201 （3）郑俊娟的博客： http://blog.sciencenet.cn/blog-1271266-775638.html （4）鳉鲈的博客： http://blog.sina.com.cn/s/blog_71df25810102w2vf.html （5）生信笔记的博客： http://www.bioinfo-scrounger.com/ （6）Rethink的博客 http://blog.leanote.com/post/hwoihann/how-to-download-series-of-sra-data-in-one-command]]></content>
      <categories>
        <category>NGS软件</category>
      </categories>
      <tags>
        <tag>NGS软件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何优雅的输出python字典]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F05%2F%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E7%9A%84%E8%BE%93%E5%87%BApython%E5%AD%97%E5%85%B8%2F</url>
    <content type="text"><![CDATA[如何优雅的输出python字典&emsp;&emsp;python中的字典是一种清晰的数据结构，生信学习过程中有很多统计的事儿。比如有10个样本，Variant calling 分析后需要根据每个样本，统计每种变异类型并绘图。此时就可以用到python的嵌套字典或者嵌套列表。 &emsp;&emsp;python中的字典是一种清晰的数据结构，生信学习过程中有很多统计的事儿。比如有10个样本，Variant calling 分析后需要根据每个样本，统计每种变异类型并绘图。此时就可以用到python的嵌套字典或者嵌套列表。 （1）一层字典1gene_symbol_ENGS=&#123;'PKHD1': 'ENSG00000170927', 'ATP6V0A4': 'ENSG00000105929', 'TSC2': 'ENSG00000103197'&#125; 123456789from pandas import Seriesframe1 = Series(gene_symbol_ENGS)print(frame1) ATP6V0A4 ENSG00000105929 PKHD1 ENSG00000170927 TSC2 ENSG00000103197 dtype: object （2）嵌套字典1sample_mutation_count = &#123;'PDC668_vs_PDC668A2': &#123;'ncRNA_intronic': 32, 'UTR5': 6, 'ncRNA_splicing': 0, 'exonic;splicing': 0, 'UTR3': 6, 'upstream;downstream': 1, 'downstream': 6, 'exonic': 155, 'upstream': 2, 'splicing': 1, 'ncRNA_exonic': 19, 'intergenic': 162, 'intronic': 162&#125;, 'PDC3748_vs_PDC3748B4': &#123;'ncRNA_intronic': 21, 'UTR5': 6, 'ncRNA_splicing': 0, 'exonic;splicing': 0, 'UTR3': 2, 'upstream;downstream': 0, 'downstream': 2, 'exonic': 123, 'upstream': 8, 'splicing': 2, 'ncRNA_exonic': 15, 'intergenic': 122, 'intronic': 130&#125;&#125; 1234567891011121314151617181920212223242526from pandas import DataFrameframe2 = DataFrame(sample_mutation_count)print(frame2) PDC3748_vs_PDC3748B4 PDC668_vs_PDC668A2 UTR3 2 6 UTR5 6 6 downstream 2 6 exonic 123 155 exonic;splicing 0 0 intergenic 122 162 intronic 130 162 ncRNA_exonic 15 19 ncRNA_intronic 21 32 ncRNA_splicing 0 0 splicing 2 1 upstream 8 2 upstream;downstream 0 1``` 那么问题来了，当每个样本的行不完全一样时如何解决呢，```pythonlen_exon = &#123;'ENSG00000008710': &#123;'ENST00000570193': 591, 'ENST00000483558': 573&#125;,'ENSG00000089597': &#123;'ENST00000526210': 529, 'ENST00000526392': 255, 'ENST00000532402': 3695&#125;&#125; 12345678910111213141516171819202122232425from pandas import Seriesfor key,value in len_exon.items(): print(key) print(Series(value)) ENSG00000008710 ENST00000483558 573 ENST00000570193 591 dtype: int64 ENSG00000089597 ENST00000526210 529 ENST00000526392 255 ENST00000532402 3695 dtype: int64``` ## 当然，也可以使用pprint输出字典结构，看着还行，但不利于后续R绘图```pythonimport pprintresultFile = open('result.py', 'w')resultFile.write(pprint.pformat(len_exon))resultFile.close() 1234&#123;'ENSG00000008710': &#123;'ENST00000483558': 573, 'ENST00000570193': 591&#125;, 'ENSG00000089597': &#123;'ENST00000526210': 529, 'ENST00000526392': 255, 'ENST00000532402': 3695&#125;&#125;]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[picard]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F03%2Fpicard%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;picard 是用java开发的用于处理高通量测序数据和格式转换（SAM/BAM/CRAM和VCF）的命令行工具集 #Picard 高通量测序数据处理及格式转换工具大合集 picard 是用java开发的用于处理高通量测序数据和格式转换（SAM/BAM/CRAM和VCF）的命令行工具集 其官网为：http://broadinstitute.github.io/picard/ 先看看picard都有哪些工具集： Available Programs:Alpha Tools: 目前无支持，需要进一步测试完善的工具集 CollectIndependentReplicateMetrics 预测bam文件中reads的独立重复率 CollectWgsMetricsWithNonZeroCoverage 收集关于全基因组（WGS）测序的覆盖度和测序质量信息 UmiAwareMarkDuplicatesWithMateCigar 利用read的位置和UMIs信息鉴定重复的reads Fasta: 操作FASTAor相关数据工具集 CreateSequenceDictionary 创建参考序列的序列字典 ExtractSequences 从参考序列中创建子区间存为新的FASTA NonNFastaSize 对fasta文件中non-N碱基计数 NormalizeFasta 规范FASTA文件中的序列行为相同长度 Fingerprinting Tools: 操作印迹图谱工具集 CheckFingerprint 计算来自提供的(SAM/BAM or VCF) 文件的指纹图谱，并与提供的基因型比较 ClusterCrosscheckMetrics 通过LOD得分对CrosscheckFingerprints的结果进行聚类 CrosscheckFingerprints 检查是否所有的指纹图谱来自相同的个体 CrosscheckReadGroupFingerprints 弃用：使用CrosscheckFingerprints.检查是否所有的指纹图谱来自相同的个体 Illumina Tools: 操作具体Illumina测序数据工具集 CheckIlluminaDirectory 维护具体的Illumina basecalling数据的有效性 CollectIlluminaBasecallingMetrics 从一次测序运行中收集Illumina Basecalling矩阵 CollectIlluminaLaneMetrics 对于每个给定的BaseCalling分析路径收集Illumina lane矩阵 ExtractIlluminaBarcodes 确定Illumina lane中每个read的barcode IlluminaBasecallsToFastq 从Illumina basecall read数据中产生ASTQ 文件 IlluminaBasecallsToSam 转换原始的Illumina测序数据为unmapped 的SAM或者BAM 文件. MarkIlluminaAdapters 读SAM或者BAM 文件并用新的接头修饰标签重写 Interval Tools: 操作Picard区间列表工具集 BedToIntervalList 转换BED 文件为Picard区间列表 IntervalListToBed 转换Picard的IntervalList文件为BED文件. IntervalListTools 操作区间列表 LiftOverIntervalList 从一个参考转为另一个时去除区间列表 ScatterIntervalsByNs 通过Ns分割参考，写入区间列表 Metrics: 各种不同数据类型报表矩阵工具集 AccumulateVariantCallingMetrics 组合多个变异Calling矩阵为单个文件 CollectAlignmentSummaryMetrics &lt;b&gt;从SAM或BAM文件产生比对矩阵总结 &lt;/b&gt; CollectBaseDistributionByCycle 对SAM或BAM文件中每个循环核苷酸分布制表 CollectGcBiasMetrics 收集关于GC bias的矩阵 CollectHiSeqXPfFailMetrics 将HiSeqX Illumina Basecalling directory下的PF-Failing reads归类为不同类别 CollectHsMetrics 从SAM或BAM文件收集杂交选择（HS）矩阵 CollectInsertSizeMetrics 从配对末端文库中收集插入片段分布矩阵 CollectJumpingLibraryMetrics 收集跳跃文库矩阵 CollectMultipleMetrics 收集多类矩阵 CollectOxoGMetrics 收集矩阵评估氧化产物 CollectQualityYieldMetrics 收集关于通过质控阈值和 Illumina-specific过滤的reads矩阵 CollectRawWgsMetrics 收集全基因组测序相关矩阵 CollectRnaSeqMetrics 从SAM或BAM文件中产生RNA比对矩阵 CollectRrbsMetrics &lt;b&gt;从简化的亚硫酸氢盐测序(Rrbs)数据收集矩阵&lt;/b&gt; CollectSequencingArtifactMetrics 收集量化单末端测序产品的矩阵 CollectTargetedPcrMetrics 从目标测序数据中收集PCR相关矩阵 CollectVariantCallingMetrics 从提供的VCF文件文件中收集每个样本和包含所有样本集合的矩阵 CollectWgsMetrics 收集关于全基因组测序（WGS）实验的覆盖度和质量矩阵 CompareMetrics 比较两个矩阵文件 ConvertSequencingArtifactToOxoG 从广义的人工矩阵提取OxoG矩阵 EstimateLibraryComplexity 预测测序文库中特异性分子数量 MeanQualityByCycle 通过循环收集均值质量 QualityScoreDistribution 为质量得分的分布绘制表格 Miscellaneous Tools: 混杂工具集 BaitDesigner 为杂交选择反应设计寡核苷酸baits FifoBuffer FIFO buffer 用来缓冲具有可定制缓冲大小的输入和输出流 SAM/BAM: 操作SAM, BAMor者相关数据的工具集 AddCommentsToBam 为headerBAM 文件的header增加评论 AddOrReplaceReadGroups 替代BAM 文件的read groups BamIndexStats 从BAM 文件产生索引统计 BamToBfq 通过maq aligner从BAM文件产生BFQ文件s from a BuildBamIndex 生成BAM索引，&quot;.bai&quot; 文件 CalculateReadGroupChecksum 基于read groups(RG)产生哈希码 CheckTerminatorBlock 维护提供的gzip 文件(e.g., BAM)最后一个区块格式正确; 否则RC 100 CleanSam 清除提供的SAM/BAM，soft-clipping beyond-end-of-reference alignments并且对于未比对上的reads设置MAPQ为0 CompareSAMs 比较两个输入的&quot;.sam&quot; or &quot;.bam&quot; 文件 DownsampleSam 对SAM或BAM文件缩小取样 FastqToSam 转换FASTQ文件为unaligned的BAM或SAM文件 FilterSamReads 从SAM或BAM文件取read数据子集 FixMateInformation 如果需要，在mates和fix之间确认mate-pair信息 GatherBamFiles 尽可能高效的连接一个或多个BAM文件 MarkDuplicates 鉴定重复的reads MarkDuplicatesWithMateCigar 鉴定重复的reads,解释mate CIGAR. MergeBamAlignment 从SAM或者BAM文件中合并alignment数据到unmapped BAM文件 MergeSamFiles 合并多个SAM和/或BAM 文件为单个文件 PositionBasedDownsampleSam 缩小SAM或者BAM文件取样来维持reads子集，基于reads在每个flowcell的每个tile的位置 ReorderSam 对SAM或者BAM文件中的reads重排序，来匹配参考文件中的顺序 ReplaceSamHeader 替代SAM或BAM文件的SAMFileHeader RevertOriginalBaseQualitiesAndAddMateCigar 转换原始碱基质量并增加mate cigar到read-group BAMs RevertSam 转换SAM或BAM 文件回到之前状态 SamFormatConverter BAM文件与SAM 文件互相转换 SamToFastq 转换SAM或者BAM文件为FASTQ文件 SetNmAndUqTags 弃用：使用SetNmMdAndUqTags代替 SetNmMdAndUqTags 修改SAM文件中NM,MD和UQ标签 SortSam 对一个SAM或BAM文件排序 SplitSamByLibrary 通过文库分割SAM或BAM文件为独立文件 ValidateSamFile 确认SAM或BAM文件 ViewSam 打印SAM或BAM文件到屏幕 Unit Testing: 测试单元 SimpleMarkDuplicatesWithMateCigar 测试提供的SAM或BAM文件中比对上的记录来定位重复分子 VCF/BCF: 操作VCF, BCFor者相关数据的工具集 FilterVcf 严格过滤VCF FindMendelianViolations 在VCF文件中寻找所有违反孟德尔法则的类型 FixVcfHeader 代替或者修改VCF header GatherVcfs 文件从一个分散的多个VCF文件产生单个VCF文件 GenotypeConcordance 评估callsets之间的基因型一致性 LiftoverVcf 从一个引用构建另一个引用时留下一个VCF文件 MakeSitesOnlyVcf 从VCF或BCF文件创建没有基因型信息的VCF文件 MergeVcfs 合并多个VCF或者BCF文件为一个VCF文件或者BCF文件 RenameSampleInVcf 对VCF或BCF中样本重命名 SortVcf 对一个活多个VCF 文件排序 SplitVcfs 分割SNPs和INDELs为独立的文件 UpdateVcfSequenceDictionary 对于VCF文件和包含有序列字典的文件，利用新的序列字典更新VCF文件 VcfFormatConverter VCF与BCF互相转换 VcfToIntervalList 转换VCF or BCF 文件为Picard区间列表 （1）接下来看看如何安装： #查看Java版本 java -version Java 1.8及以后版本即OK #从github拷贝库 git clone https://github.com/broadinstitute/picard.git #进入picard文件目录 cd picard/ #编译 ./gradlew shadowJar 此时，在build/libs/文件夹下可见picard.jar程序 #运行 java -jar build/libs/picard.jar （2）如何设置环境变量： vim ~/.bashrc i PICARD=&apos;/home/u631758/biosoftwares/picard/build/libs/picard.jar&apos; alias picard=&quot;java -jar $PICARD&quot; 如此就可以用picard命名代替官网中的java -jar picard.jar命令了！！！ （3）查看有哪些可用的工具： picard （4）查看某个工具的具体用法： 例如VcfFormatConverter工具： picard VcfFormatConverter -h 结果为： java -jar picard.jar VcfFormatConverter \ I=input.vcf \ O=output.bcf \ 所以使用方式为： picard VcfFormatConverter I=input.vcf O=output.bcf]]></content>
      <categories>
        <category>NGS软件</category>
      </categories>
      <tags>
        <tag>NGS软件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux中python2和python3环境搭建及共存]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F03%2Fpython%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;本教程将展示如何在CentOS上通过源码编辑安装最新的Python 2 和 Python 3。以下例子使用Python 2.7.13 and Python 3.6.2，但是操作流程对于其他新版python都是相同的。 linux中python2和python3环境搭建及共存&emsp;&emsp;如果你使用的是CentOS 6，你可以使用本教程安装 Python 2.7.x and Python 3.6.x。对于CentOS 7 只有Python 3.6.x的安装方法是适用的。警告！不要在CentOS 7上使用本教程安装Python 2.7.13。这样你的系统将有两个不同的python2.7二进制文件，每个具有自己的安装包路径。这将可能造成不能区分的问题！ 查看linux系统版本信息：lsb_release -a 注: 这个命令适用于所有的linux，包括Redhat、SuSE、Debian等发行版 问题所在？&emsp;&emsp;CentOS携带Python作为基础系统的一个重要部分。因为其十分重要，所以未及时更新，或者为了避免安全漏洞。缺少更新，意味着CentOS 6用户无法摆脱2010年8月发布的Python 2.6.6，CentOS 7用户无法摆脱2013年发布的Python 2.7.5。 通常/usr/bin下面的都是系统预装的可执行程序，会随着系统升级而改变 cd /usr/bin ls |grep ^p 只安装了python2.6 YUM&emsp;&emsp;YUM（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及CentOS中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，避免了手动安装的麻烦(寻找资源、下载；放到指定目录安装；处理依赖关系并下载依赖关系的包进行安装)。所以用yum安装，实质上是用RPM安装，所以RPM查询信息的指令都可用。 如果使用RPM安装了一些包，一般来说，RPM默认安装路径如下： Directory Contents of Directory /usr/bin 一些可执行文件 /usr/lib 一些程序使用的动态函数库 /usr/share/doc 一些基本的软件使用手册与帮助文档 /usr/share/man 一些man page文件所以没有root权限，是没有办法通过yum进行软件安装的 需要考虑的事项：使用 “make altinstall” 来避免麻烦 &emsp;&emsp;当安装定制版的Python时使用make altinstall时十分重要的。如果使用常规的make install的结局是安装不同的python版本，但出现在文件系统中却出现同为python的程序,这将导致很难辨别的麻烦。 &emsp;&emsp;编译安装的准备 为了编译Python之前最好先安装一系列的开发工具和一些拓展库，但不是必须的，但这样Python才能依赖这些工具和拓展库展示它强悍的功能。 (1)下载并编译安装pythonPython 2.7.13: wget http://python.org/ftp/python/2.7.13/Python-2.7.13.tar.xz tar xf Python-2.7.13.tar.xz cd Python-2.7.13 ./configure --prefix=/home/wangdong/python/python27 make &amp;&amp; make altinstall Python 3.6.2: wget http://python.org/ftp/python/3.6.2/Python-3.6.2.tar.xz tar xf Python-3.6.2.tar.xz cd Python-3.6.2 ./configure --prefix=/home/wangdong/python/python36 make &amp;&amp; make altinstall (2)设置环境变量echo &apos;export PATH=/home/wangdong/python/python36/bin:$PATH&apos;&gt;&gt;~/.bashrc echo &apos;export PATH=/home/wangdong/python/python27/bin:$PATH&apos;&gt;&gt;~/.bashrc &emsp;&emsp;注意使用python3.6, 使用python仍旧为系统预装的python版本，以后使用Python解释器时,同样需要注意使用python和python3.6调用的Python解释器是不同的！！ linux添加环境环境变量注意事项： （1）=号左右两边没有空格 （2）路径之间用：分隔 （3）$PATH 表示原先设定的路径仍然有效，注意不要漏掉 （4）需要引号,因为用echo命令输出加引号的字符串时，将字符串原样输出 (3) 安装/升级pip,setuotools和wheel 安装pip,setuotools和wheel系统中的每个Python解释器都需要自己的pip,setuotools和wheel,安装和升级这些包最简单的方式是使用get-pip.py脚本。 First get the script: wget https://bootstrap.pypa.io/get-pip.py Then execute it using Python 2.7 and/or Python 3.6: python2.7 get-pip.py python3.6 get-pip.py PYTHONPATH是Python搜索路径，默认我们import的模块都会从PYTHONPATH里面寻找 echo &apos;export PYTHONPATH=/home/wangdong/python/python36/lib/python3.6/site-packages&apos;&gt;&gt;~/.bashrc echo &apos;export PYTHONPATH=/home/wangdong/python/python27/lib/python2.7/site-packages&apos;&gt;&gt;~/.bashrc source ~/.bashrc With pip installed you can now do things like this: pip2.7 install [packagename] pip2.7 install –upgrade [packagename] pip2.7 uninstall [packagename] pip3.6 install [packagename] pip3.6 install –upgrade [packagename] pip3.6 uninstall [packagename] &emsp;&emsp;注意使用pip2.7和pip3.6安装软件不同点在于，安装文件的路径不同。pip2.7的安装路径是/home/wangdong/python/python27/lib/python2.7/site-packages，而pip3.6的安装路径是/home/wangdong/python/python36/lib/python3.6/site-packages ###（4）虚拟环境的使用 &emsp;&emsp;如果你使用Python2.7，则强烈推荐使用安装virtualenv并且学习使用它。virtualenv可以创建独立的Python环境。如果你使用Python3.3+，那么你没有必要安装virtualenv，因为其功能已经内建了。 每个独立的Python环境（又叫sandbox）能具有自己的Python版本和包。这对于多项目或者相同项目需要不同的版本的场合是十分重要的。 先看看virtualenv中文教程： http://virtualenv-chinese-docs.readthedocs.io/en/latest/#id29 Install virtualenv for Python 2.7 and create a sandbox called my27project: pip2.7 install virtualenv virtualenv my27project Use the built-in functionality in Python 3.6 to create a sandbox called my36project: python3.6 -m venv my36project (1)Check the system Python interpreter version: python --version This will show Python 2.6.6 Activate the my27project sandbox: source my27project/bin/activate (2)Check the Python version in the sandbox (it should be Python 2.7.13): python --version This will show Python 2.7.13 Deactivate the sandbox: deactivate Activate the my36project sandbox: source my36project/bin/activate (3)Check the Python version in the sandbox (it should be Python 3.6.2): python --version This will show Python 3.6.2 Deactivate the sandbox: deactivate 小结：(1) 从（1）和（2）或者（1）和（3）的对比看出：创建虚拟环境并激活后，虚拟环境的环境变量和系统的环境变量是隔离的，互不影响。 (2) 创建的虚拟环境的Python解释器版本是如何指定的呢？先看看virtualenv用法: $ virtualenv [OPTIONS] DEST_DIR其中一个选项-p PYTHON_EXE, –python=PYTHON_EXE &emsp;&emsp;指定所用的python解析器的版本，比如 –python=python2.5 就使用2.5版本的解析器创建新的隔离环境。 默认使用的是当前目录下安装(/home/wangdong/python/python36/bin/python3.6或者/home/wangdong/python/python27/bin/python2.7)的python解析器 所以可以在python27下使用-p指定python3.6解释器创建虚拟环境： 反过来，对于Python3.3+ 通过venv模块创建指定python2.7虚拟环境则不行了！ 首先看看venv模块官方文档： https://docs.python.org/3/library/venv.html 需要注意的是，在Python3.3中使用”venv”命令创建的环境不包含”pip”，你需要进行手动安装。在Python3.4中改进了这一个缺陷。 并没有相关参数！！ 所以类似的可以使用virtualenv解决： 在python36目录下： virtualenv my27project_test source my27project_test/bin/activate python This will show Python 2.7.13 在python36目录下： virtualenv -p /home/wangdong/python/python36/bin/python3.6 my36proje_test source my36project_test/bin/activate python This will show Python 3.6.2 （3）在对应虚拟环境下使用对应pip安装软件：例如： source my36project/bin/activate pip3.6 install numpy &emsp;&emsp;安装路径为： ./my36project/lib/python3.6/site-packages **所以安装包也和系统是完全隔离的，二者互不影响。因此虚拟环境不再使用时，直接删除该虚拟环境即可。 rm -rf my36project 在my27project下则使用pip2.7,其他类似。 ####（4）接下来讲讲pip使用 使用清华的pip源安装包更快： pip3.6 install -i https://pypi.tuna.tsinghua.edu.cn/simple bcbio-gff biopython cython nose numpy pandas shove sqlalchemy python-memcached pyvcf (不同安装包之间使用空格即可) 指定安装包的版本,例如： pip3.6 install pysam==0.7.5 卸载指定版本安装包，例如： pip3.6 uninstall biopython==1.70 把常用的包离线下载，然后使用pip离线安装包，例如： pip3.6 install pysam-0.7.5.tar.gz 查看当前环境pip已安装包列表： pip3.6 list 参考： https://danieleriksson.net/2017/02/08/how-to-install-latest-python-on-centos/]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux中python2和python3环境搭建及共存]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F03%2Flinux%E4%B8%ADpython2%E5%92%8Cpython3%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E5%8F%8A%E5%85%B1%E5%AD%98%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;本教程将展示如何在CentOS上通过源码编辑安装最新的Python 2 和 Python 3。以下例子使用Python 2.7.13 and Python 3.6.2，但是操作流程对于其他新版python都是相同的。 linux中python2和python3环境搭建及共存&emsp;&emsp;如果你使用的是CentOS 6，你可以使用本教程安装 Python 2.7.x and Python 3.6.x。对于CentOS 7 只有Python 3.6.x的安装方法是适用的。警告！不要在CentOS 7上使用本教程安装Python 2.7.13。这样你的系统将有两个不同的python2.7二进制文件，每个具有自己的安装包路径。这将可能造成不能区分的问题！ 查看linux系统版本信息：lsb_release -a 注: 这个命令适用于所有的linux，包括Redhat、SuSE、Debian等发行版 问题所在？&emsp;&emsp;CentOS携带Python作为基础系统的一个重要部分。因为其十分重要，所以未及时更新，或者为了避免安全漏洞。缺少更新，意味着CentOS 6用户无法摆脱2010年8月发布的Python 2.6.6，CentOS 7用户无法摆脱2013年发布的Python 2.7.5。 通常/usr/bin下面的都是系统预装的可执行程序，会随着系统升级而改变 cd /usr/bin ls |grep ^p 只安装了python2.6 YUM&emsp;&emsp;YUM（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及CentOS中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，避免了手动安装的麻烦(寻找资源、下载；放到指定目录安装；处理依赖关系并下载依赖关系的包进行安装)。所以用yum安装，实质上是用RPM安装，所以RPM查询信息的指令都可用。 如果使用RPM安装了一些包，一般来说，RPM默认安装路径如下： Directory Contents of Directory /usr/bin 一些可执行文件 /usr/lib 一些程序使用的动态函数库 /usr/share/doc 一些基本的软件使用手册与帮助文档 /usr/share/man 一些man page文件所以没有root权限，是没有办法通过yum进行软件安装的 需要考虑的事项：使用 “make altinstall” 来避免麻烦 &emsp;&emsp;当安装定制版的Python时使用make altinstall时十分重要的。如果使用常规的make install的结局是安装不同的python版本，但出现在文件系统中却出现同为python的程序,这将导致很难辨别的麻烦。 &emsp;&emsp;编译安装的准备 为了编译Python之前最好先安装一系列的开发工具和一些拓展库，但不是必须的，但这样Python才能依赖这些工具和拓展库展示它强悍的功能。 (1)下载并编译安装pythonPython 2.7.13: wget http://python.org/ftp/python/2.7.13/Python-2.7.13.tar.xz tar xf Python-2.7.13.tar.xz cd Python-2.7.13 ./configure --prefix=/home/wangdong/python/python27 make &amp;&amp; make altinstall Python 3.6.2: wget http://python.org/ftp/python/3.6.2/Python-3.6.2.tar.xz tar xf Python-3.6.2.tar.xz cd Python-3.6.2 ./configure --prefix=/home/wangdong/python/python36 make &amp;&amp; make altinstall (2)设置环境变量echo &apos;export PATH=/home/wangdong/python/python36/bin:$PATH&apos;&gt;&gt;~/.bashrc echo &apos;export PATH=/home/wangdong/python/python27/bin:$PATH&apos;&gt;&gt;~/.bashrc &emsp;&emsp;注意使用python3.6, 使用python仍旧为系统预装的python版本，以后使用Python解释器时,同样需要注意使用python和python3.6调用的Python解释器是不同的！！ linux添加环境环境变量注意事项： （1）=号左右两边没有空格 （2）路径之间用：分隔 （3）$PATH 表示原先设定的路径仍然有效，注意不要漏掉 （4）需要引号,因为用echo命令输出加引号的字符串时，将字符串原样输出 (3) 安装/升级pip,setuotools和wheel 安装pip,setuotools和wheel系统中的每个Python解释器都需要自己的pip,setuotools和wheel,安装和升级这些包最简单的方式是使用get-pip.py脚本。 First get the script: wget https://bootstrap.pypa.io/get-pip.py Then execute it using Python 2.7 and/or Python 3.6: python2.7 get-pip.py python3.6 get-pip.py PYTHONPATH是Python搜索路径，默认我们import的模块都会从PYTHONPATH里面寻找 echo &apos;export PYTHONPATH=/home/wangdong/python/python36/lib/python3.6/site-packages&apos;&gt;&gt;~/.bashrc echo &apos;export PYTHONPATH=/home/wangdong/python/python27/lib/python2.7/site-packages&apos;&gt;&gt;~/.bashrc source ~/.bashrc With pip installed you can now do things like this: pip2.7 install [packagename] pip2.7 install –upgrade [packagename] pip2.7 uninstall [packagename] pip3.6 install [packagename] pip3.6 install –upgrade [packagename] pip3.6 uninstall [packagename] &emsp;&emsp;注意使用pip2.7和pip3.6安装软件不同点在于，安装文件的路径不同。pip2.7的安装路径是/home/wangdong/python/python27/lib/python2.7/site-packages，而pip3.6的安装路径是/home/wangdong/python/python36/lib/python3.6/site-packages ###（4）虚拟环境的使用 &emsp;&emsp;如果你使用Python2.7，则强烈推荐使用安装virtualenv并且学习使用它。virtualenv可以创建独立的Python环境。如果你使用Python3.3+，那么你没有必要安装virtualenv，因为其功能已经内建了。 每个独立的Python环境（又叫sandbox）能具有自己的Python版本和包。这对于多项目或者相同项目需要不同的版本的场合是十分重要的。 先看看virtualenv中文教程： http://virtualenv-chinese-docs.readthedocs.io/en/latest/#id29 Install virtualenv for Python 2.7 and create a sandbox called my27project: pip2.7 install virtualenv virtualenv my27project Use the built-in functionality in Python 3.6 to create a sandbox called my36project: python3.6 -m venv my36project (1)Check the system Python interpreter version: python --version This will show Python 2.6.6 Activate the my27project sandbox: source my27project/bin/activate (2)Check the Python version in the sandbox (it should be Python 2.7.13): python --version This will show Python 2.7.13 Deactivate the sandbox: deactivate Activate the my36project sandbox: source my36project/bin/activate (3)Check the Python version in the sandbox (it should be Python 3.6.2): python --version This will show Python 3.6.2 Deactivate the sandbox: deactivate 小结：(1) 从（1）和（2）或者（1）和（3）的对比看出：创建虚拟环境并激活后，虚拟环境的环境变量和系统的环境变量是隔离的，互不影响。 (2) 创建的虚拟环境的Python解释器版本是如何指定的呢？先看看virtualenv用法: $ virtualenv [OPTIONS] DEST_DIR其中一个选项-p PYTHON_EXE, –python=PYTHON_EXE &emsp;&emsp;指定所用的python解析器的版本，比如 –python=python2.5 就使用2.5版本的解析器创建新的隔离环境。 默认使用的是当前目录下安装(/home/wangdong/python/python36/bin/python3.6或者/home/wangdong/python/python27/bin/python2.7)的python解析器 所以可以在python27下使用-p指定python3.6解释器创建虚拟环境： 反过来，对于Python3.3+ 通过venv模块创建指定python2.7虚拟环境则不行了！ 首先看看venv模块官方文档： https://docs.python.org/3/library/venv.html 需要注意的是，在Python3.3中使用”venv”命令创建的环境不包含”pip”，你需要进行手动安装。在Python3.4中改进了这一个缺陷。 并没有相关参数！！ 所以类似的可以使用virtualenv解决： 在python36目录下： virtualenv my27project_test source my27project_test/bin/activate python This will show Python 2.7.13 在python36目录下： virtualenv -p /home/wangdong/python/python36/bin/python3.6 my36proje_test source my36project_test/bin/activate python This will show Python 3.6.2 （3）在对应虚拟环境下使用对应pip安装软件：例如： source my36project/bin/activate pip3.6 install numpy &emsp;&emsp;安装路径为： ./my36project/lib/python3.6/site-packages **所以安装包也和系统是完全隔离的，二者互不影响。因此虚拟环境不再使用时，直接删除该虚拟环境即可。 rm -rf my36project 在my27project下则使用pip2.7,其他类似。 ####（4）接下来讲讲pip使用 使用清华的pip源安装包更快： pip3.6 install -i https://pypi.tuna.tsinghua.edu.cn/simple bcbio-gff biopython cython nose numpy pandas shove sqlalchemy python-memcached pyvcf (不同安装包之间使用空格即可) 指定安装包的版本,例如： pip3.6 install pysam==0.7.5 卸载指定版本安装包，例如： pip3.6 uninstall biopython==1.70 把常用的包离线下载，然后使用pip离线安装包，例如： pip3.6 install pysam-0.7.5.tar.gz 查看当前环境pip已安装包列表： pip3.6 list 参考： https://danieleriksson.net/2017/02/08/how-to-install-latest-python-on-centos/]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[添加PATH环境变量]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F02%2F%E6%B7%BB%E5%8A%A0PATH%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;Linux下想让某个命令可以全局使用，可以有两种方法来实现，具体请看下文！ 想让某个命令可以全局使用，可以有两种方法来实现：方法一.在命令行将路径添加到 .bashrc文件echo &apos;export PATH=~/home/biosoftwares/FastQC/bin/fastqc:$PATH&apos; &gt;&gt;~/.bashrc source ~/.bashrc 注意： （1）注意添加的路径是可执行文件的绝对路径，一般在bin下，可执行文件为绿色 （2）用echo命令输出加引号的字符串时，将字符串原样输出 （3）export可新增，修改或删除环境变量，供后续执行的程序使用。export的效力仅及于该次登陆操作 （4）&gt;&gt;以追加方式添加 （5） ~/.bashrc表示家目录下的.bashrc文件，为隐藏文件，使用ls -a可以查看 （6）使用source ~/.bashrc之前需要先回到家目录 （7）Source命令也称为“点命令”，也就是一个点符号（.）。source命令通常用于重新执行刚修改的 初始化文件，使之立即生效，而不必注销并重新登录。用法：source filename 或 . filename 也可以在用vim打开~/.bashrc文件直接添加 vim ~/.bashrc i PATH=~/home/biosoftwares/FastQC/bin/fastqc Esc :x 注意： （1）i 进入vim编辑模式 （2）Esc退出vim编辑模式 （3）:x保存修改并退出（需要在英文输入法状态下） ##方法二. 将目录添加到~/.bashrc 文件，比如~/bin，然后创建软连接（相当于win下的快捷方式） #创建~/bin目录 mkdir -p ~/bin #将~/bin目录添加到PATH echo &apos;export PATH=~/bin:$PATH&apos;&gt;&gt;~/.bashrc #使修改生效 source ~/.bashrc #在/bin下生成fastqc软连接 ln -s ~/src/fastQC/fastqc ~/bin/fastqc 注意： （1）mkdir -p 表示创建多级目录 （2）ln是link的缩写，-s选项创建软连接 创建软连接格式： ln/link SOURCE [TARGET] 创建软链接： ln/link -s SOURCE [TARGET] 软连接不占用磁盘空间，硬链接相当于拷贝，占磁盘空间]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[修改host文件-科学上网]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F02%2F%E4%BF%AE%E6%94%B9host%E6%96%87%E4%BB%B6-%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;Hosts文件是Windows系统中一个负责IP地址与域名快递解析的文件，以ASCLL格式保存。计算机在键入域名的时候，首先会去看看hosts文件汇总有没有关于此域名IP地址的记录。如果有，就直接登陆该网站；如果没有再查询DNS服务器。那么，为什么修改HOSTS文件可以实现翻墙呢？请看下文~ &emsp;&emsp;Hosts文件是Windows系统中一个负责IP地址与域名快递解析的文件，以ASCLL格式保存。计算机在键入域名的时候，首先会去看看hosts文件汇总有没有关于此域名IP地址的记录。如果有，就直接登陆该网站；如果没有再查询DNS服务器。那么，为什么修改HOSTS文件可以实现翻墙呢？ &emsp;&emsp;无论你上网打的什么网站地址, 实际上最终总归是要转换成一个IP地址才能访问的,平时这个转换工作是有网络上的DNS服务器来完成的。但是有些时候,有些网站, 处于某些原因, 网络上的DNS服务器无法给出正确的或可用IP地址(天朝特别多, 大家懂的)。 &emsp;&emsp;这个时候hosts文件就可以代劳了, 你可以直接用记事本打开这文件看看就知道了, 里面一行就是一条记录, 一个IP地址接一个空格或tab, 再后面就是一个网址。它起到的作用就是直接在你本机上就把这些网址翻译成IP地址. 从Windows 2000开始，Windows解析名称的顺序为：DNS cache –&gt; hosts 文件 –&gt; DNS Server –&gt; NetBIOS cache –&gt; WINS Server –&gt; 广播 –&gt; LMHOSTS 文件hosts 文件的优先级高于 DNS Server，因此修改hosts文件可以跳过被污染的dns服务器。 Hosts文件格式是咋样的？ &emsp;&emsp;用记事本打开hosts文件，它的作用是包含IP地址和Host name(主机名)的映射关系，是一个映射IP地址和Hostname(主机名)的规定，规定要求每段只能包括一个映射关系，IP地址要放在每段的最前面，空格后再写上映射的Host name(主机名)。对于这段的映射说明用“#”分割后用文字说明。 为啥还需要及时更新Hosts文件？ Hosts文件配置的映射是静态的，如果网络上的计算机更改了请及时更新IP地址，否则将不能访问。 该Git项目持续更新可用的Hosts文件： https://github.com/racaljk/hosts 如何修改Hosts文件实现翻墙呢？ （1）. 从上述Git项目中复制hosts文件内容至txt文件，命名为HOSTS, 并去掉扩展名！！！ Win7 系统HOSTS文件位于 C:\Windows\System32\drivers\etc\hosts，没有拓展名。 （2）. 使之生效 Windows 开始 -&gt; 运行 -&gt; 输入cmd -&gt; 在CMD窗口输入 ipconfig /flushdns （3）使用谷歌浏览器随意登陆Google、Gmail、维基百科、Twitter、Facebook等，但必须使用https加密方式打开 谷歌香港：https://www.google.com.hk 谷歌：https://www.google.com/ncr PS: www.google.com/ncr中的”/ncr”是什么意思?起什么作用的? ncr : no country redirect &emsp;&emsp;If google thinks you are from a foreign country or region,it likes to redirect you to your regional google page.For most people,this makes sense.However,if you prefer the generic,english,plain version,this would be very annoying. 参考： （1）老D博客（很多黑科技~）： https://laod.cn/hosts/2017-google-hosts.html （2）新浪博客：http://blog.sina.com.cn/s/blog_9caf88850102xnlb.html]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生命在于折腾之Github+HEXO搭建博客]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F02%2F%E7%94%9F%E5%91%BD%E5%9C%A8%E4%BA%8E%E6%8A%98%E8%85%BE%E4%B9%8BGithub-HEXO%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;生命在于折腾之Github+HEXO搭建博客 &emsp;&emsp;花了近四个小时，总算还是成功了~现在的心情是这样的： 开心之余，将我的经验和踩过的坑分享给大家~ 首先推荐下参考的博客，基本是对的，实在大赞。（1）Never_yu’s的博客： https://neveryu.github.io/2016/09/03/hexo-next-one/ （2）金石开的博客 http://www.cnblogs.com/zhcncn/p/4097881.html 建议主要根据上面两个博客的方法搭建，遇到问题看看这些填坑博客~（1）文青程序猿的博客： http://www.jianshu.com/p/31eb84182156 （2）WebEnh的博客 http://www.cnblogs.com/webenh/p/5792632.html 我的安装过程还算顺利，遇到的问题：(1)安装nvm后node -v报错，表明环境变量问题：解决： ivanyb的简书文章 http://www.jianshu.com/p/07c3456e875a 孙群的博客 http://blog.csdn.net/iispring/article/details/8023319/ （2）使用npm install -g hexo-cli命令安装Hexo，很卡，最后还报错，查了查，说因为npm被墙了。 所以首先更改了淘宝的源： nvm node_mirror https://npm.taobao.org/mirrors/node/ nvm npm_mirror https://npm.taobao.org/mirrors/npm/ 参考：https://github.com/coreybutler/nvm-windows 问题依旧！！！ 查了下报错：npm ERR! 参考：http://blog.csdn.net/weng423811758/article/details/51537594 因为我开了全局VPN,但依然没解决。 淘宝说用cnpm代替npm： 参考：https://npm.taobao.org/ 然并卵！！ 最后，查看node官网，后来看看其官网，推荐使用v6.11.2LTS，改为v6.11.2LTS之后就解决了（ps:开始是使用的node的最新的v8.2.1版本） （3）部署时报错error deployer not found:git 解决：http://www.jianshu.com/p/4d2c07a330da 我的deploy配置 deploy: type: git repository: https://github.com/AnJingwd/AnJingwd.github.io.git branch: master 然后就成功了！！！ 我的博客：https://anjingwd.github.io/ 未完待续~ 博客将继续完善]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
</search>
