<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Rmarkdown报表输出表格]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F12%2F10%2FRmarkdown%E6%8A%A5%E8%A1%A8%E8%BE%93%E5%87%BA%E8%A1%A8%E6%A0%BC%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Rmarkdown生成动态报告]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F12%2F10%2FRmarkdown%E7%94%9F%E6%88%90%E5%8A%A8%E6%80%81%E6%8A%A5%E5%91%8A%2F</url>
    <content type="text"><![CDATA[Rmarkdown生成动态报告 &emsp;&emsp;RStudio可以当做 Markdown 编辑器来用（R Markdown），根据数据处理结果快速生成报告文档，这一功能主要通过 Package Knit 及相关组件完成。 &emsp;&emsp;R Markdown 的两大特别之处，一是通过 Pandoc 将 Markdown 转化成 LaTex，再由强大的 LaTex 转换成 HTML、PDF、Word，理论上来说借助 LaTex 可以生成学术论文、期刊杂志、数据报告等规范格式的文档；另一大特点是整合了 R 语言的环境，可以在 Markdown 语法中 code block 直接执行 R 语言代码并将结果插入文档。 R Markdown from Rstudio 帮助文档 安装必要的包install.packages(&quot;devtools&quot;) # 如果以前没有安装 devtools 包 devtools::install_github(&quot;rstudio/rmarkdown&quot;) #安装github上最新版rmarkdown library(rmarkdown) 如何编辑 添加的是普通的文字: 那就按照Markdown写吧，你看到过的功能R都支持。 添加的是代码: 那就放到代码区域里面去，和Markdown的代码标记不太一样，多了 {}。 1code 然后在 {} 里告诉 R Studio 你希望显示什么: 只想显示代码 {r,echo = FALSE, eval = FALSE} 只想显示结果 {r,echo = FALSE, eval = TRUE} 想要代码+编译结果 {r,echo = TRUE，eval = FALSE} 如果是在文中添加代码，没有{} `r 2+ 2` chunk options 这个chunk放在最前面，声明以后的每一个chunk都不输出warnings和message 插入图片![Caption for the picture.](/path/to/image.png) 输出(html ,word, pdf)从Rmd格式可以导出为三种格式，用Pandoc实现:Word, HTML, PDF（额外需要LaTex支持，并且对中文的支持需要额外的设定） 软件安装Windows There is a package installer at pandoc’s download page. 选择msi installer或者zip file都可以。 For PDF output, you’ll also need to install LaTeX. We recommend MiKTeX. R Studio的全局配置(为了支持中文pdf输出) Tools – Global Options – Code—Saving， 设置Default text coding: UTF-8 coding Tools – Global Options – Sweave，将Typeset LaTeX into PDF using:设置为 XeLaTeX 需要注意的就是latex_engine:xelatex 了。即使设置了默认的编译器为xeLaTeX依旧要手动再声明一遍编译器 设置 YAML完成对R Studio的全局配置后，对每个Rmd文件（R markdown）在最开头条线之间进行设置才是更重要的。这个部分叫做YAML wiki 123456789101112---author: "wd"date: "2017.12.10"output: pdf_documenttitle: "MRstudy"outputs: pdf_document: includes: in_header: header.tex keep_tex: yes latex_engine: xelatex--- 步骤: 在YMAL里面设置 output: pdf_document，或者word_document, html_document。 设定好了之后三种导出的方法都一样，点击代码窗口上面的蓝色毛线团标记（选择输出html或者word或者pdf）。（knit英文是编织的意思，所以就是把代码织成文本吧） 注意正确的缩进和冒号 以上意思是，输出为html，word和pdf。html和word为默认设置。pdf设置包括，in_header，keep_tex和latex_engine。 如果一个设置还没到具体的内容，它紧接的是冒号，换行后需要缩进两个空格在开始。 如果已经指向明确的内容，结束没有冒号，换行之后不需要缩进。 header.tex需要和你的Rmd文档放到一个路径， 其中内容是： 12345\usepackage&#123;xeCJK&#125;\setCJKmainfont&#123;楷体&#125; % 字体可以更换\setmainfont&#123;Georgia&#125; % 設定英文字型\setromanfont&#123;Georgia&#125; % 字型\setmonofont&#123;Courier New&#125; 更多详细配置可以参考官方文档 编译前在viewer窗口预览文档使用方法： 安装 Xaringan install.packages(“devtools”) if (!requireNamespace(“xaringan”)) devtools::install_github(“yihui/xaringan”) R markdown不需额外设置。 编译的时候找到 Addin，点击Infinite Moon Reader rmarkdown转化中文字符为PDF的设置rmarkdown对于输出含中文的html和word文档，在YAML设置html_document: default或者word_document: default即可，但对于输出含有中文的pdf文件需要单独设置YAML。如下： 123456output: pdf_document: keep_tex: yes latex_engine: xelatex includes: in_header: header.tex 中文模板 详细说明安装rticles包，调用中文模板 install.packages(“rticles”) 安装成功之后，我们可以新建一个Rmarkdown文件。点击New-&gt;file-&gt;R markdown,其中有了一个From Template 使用ctrl + shift + k的快捷键，稍等片刻会出现中文PDF 参考（1）利用R-Markdown和Knitr创建动态报告（第一部分） （2）R Markdown 设置和使用 （3）R Markdown + Infinite Moon Reader + 编辑实时更新 （4）RStudio 中的 R Markdown（推荐） （5）Rmarkdown输出PDF的中文支持问题解决 （6）rmarkdown转化中文字符为PDF的设置 （7）Rmarkdown中文实现（推荐）]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[创建宏去除从pdf复制到word的回车换行]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F12%2F03%2F%E5%88%9B%E5%BB%BA%E5%AE%8F%E5%8E%BB%E9%99%A4%E4%BB%8Epdf%E5%A4%8D%E5%88%B6%E5%88%B0word%E7%9A%84%E5%9B%9E%E8%BD%A6%E6%8D%A2%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;从PDF或网页中复制一段文字再粘贴到WORD中，会出现很多多余的空格和回车，一个一个去删除费时费力费心。在word创建宏，并设置按钮即可。 创建宏去除从pdf复制到word的回车换行 如何创建宏及设置按钮Steps to Record a Macro in Word Firstly, open your Word. And make sure you have added “Developer” tab on the Ribbon first. For detailed instructions to add “Developer”, please read the article: How to Insert Background Music into Your Word Document Once “Developer” tab is available, click it. Then click “Record Macro” in “Code” group. Now a “Record Macro” dialog box will pop up. First, type the macro name, such as “copytext” here. Then in “Assign macro to” part, choose either “Button” or “Keyboard”. Here we will go “Button”. Next, click “OK”. And now you will see the “Word Options” window. Click “Customize Ribbon”. Then find a location for the macro on the right side. Here we will click “New Group” to set a new one for the macro. Noe continue to choose “Macros” from the drop-down list of “Choose commands from”. Next click the macro you just create. Then click “Add” in the middle of the window. Now the macro will be placed in the new group. Just right click the macro And choose “Rename” option. In “Rename” dialog box, choose a symbol for your macro. Then type the display name. Click “OK”. Now go back to “Word Options” window, and click “OK” there. The macro starts recording from now. 代码12345678910111213141516171819202122232425262728293031323334353637383940Sub 粘贴并替换回车() Dim Flag As Integer '粘贴并选中 startpos = Selection.Start 'Selection.Paste '普通粘贴 Selection.PasteAndFormat (wdFormatPlainText) '粘贴为文本 Selection.Start = startpos '先将多个回车并为一个 With Selection.Find .Text = "^13&#123;2,&#125;" .Replacement.Text = "^p" .MatchWildcards = True '使用通配符 .Wrap = wdFindStop '把 .Wrap = wdFindAsk 改成 .Wrap = wdFindStop对应于“是否搜索其余部分”中的“否” .Execute Replace:=wdReplaceAll End With '主要针对最后是两个回车的特殊情况这种特殊情况下，替换后少选择一位字符，影响后续操作 If Selection.Text Like "*[!。\?？\!！”：；]" &amp; Chr(13) Then Flag = 1 End If '再将前面不是句号的回车删除 With Selection.Find .Text = "([!.。\?？\!！”：；])^13" .Replacement.Text = "\1" .MatchWildcards = True '使用通配符 .Wrap = wdFindStop '把 .Wrap = wdFindAsk 改成 .Wrap = wdFindStop对应于“是否搜索其余部分”中的“否” .Execute Replace:=wdReplaceAll End With If Flag Then Selection.MoveLeft unit:=wdCharacter, Count:=-1, Extend:=wdExtend '取消选定最后一个回车符 End If '清除替换的内容 Selection.Find.Replacement.Text = ""End Sub 补充代码主要从参考1修改而来，主要修改： 源代码第一个With Selection.Find部分导致空格删除，使得中文可以，但英文全部空格被删除，所以需要注释=掉或者删除掉 第三个With Selection.Find部分需要加一个.号，主要针对英文 效果 参考（1）[源码]文本从PDF等粘贴到WORD中，智能删除多余的空格和回车 （2）How to Remove the Formatting of Pasted Texts with Macro and VBA in Your Word]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql数据库与生信]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F11%2F23%2Fmysql%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8E%E7%94%9F%E4%BF%A1%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;主要介绍win下和linux下连接重用生信数据库，操作数据库中表，以及根据自己的需要下载公共数据，创建mysql数据库。 mysql数据库与生信 mysql学习资料（1）21分钟 MySQL 入门教程 （2）MySQL 5.7 Reference Manual: 具体技巧例如 Counting Rows；Pattern Matching mysql连接生信主要公共数据库linux下格式：mysql -h[host] -u[user] -p[password] -P[port] [database-name] -h : 该命令用于指定客户端所要登录的MySQL主机名, 登录当前机器该参数可以省略; 退出：输入 exit; 或 quit; 退出登录 例如： Server User Password Port mysql-db.1000genomes.org anonymous - 4272 （1）1000 Genomes: since June 16, 2011 mysql -h mysql-db.1000genomes.org -u anonymous -P 4272 -A （2）UCSC Downloading Data using MySQL mysql -h genome-mysql.soe.ucsc.edu -u genome -P 3306 -A 或者 mysql -h genome-euro-mysql.soe.ucsc.edu -u genome -P 3306 -A 注意：必须加上端口号-P 3306，按照官网的方法会报错： ERROR 2003 (HY000): Can’t connect to MySQL server on ‘genome-mysql.soe.ucsc.edu’ (111) （3）The Ensembl public MySQL Servers mysql -h ensembldb.ensembl.org -u anonymous -P 5306 -A （4）GO mysql -h mysql-amigo.ebi.ac.uk -u go_select -P 4085 -pamigo -A The -A flag is optional but is recommended for speed. 其他数据库：参见网址 mysql直接读ucsc数据库数据mysql --user=genome --host=genome-mysql.cse.ucsc.edu -P 3306 -A -D hg19 -e &apos;select * from refGene where name=&quot;NM_000038&quot;&apos; | less 显示： bin name chrom strand txStart txEnd cdsStart cdsEnd exonCount exonStarts exonEnds score name2 1440 NM_000038 chr5 + 112073555 112181936 112090587 112179823 16 112073555,112090569,112 windows下使用Navicat，其好处在于更加方便直观 如图： 查询某一张表，查看具体内容，导出，筛选等等，很方便 mac下可以使用Sequel Pro python连接mysql数据库pip2.7 install MySQL-python 解决：libmysqlclient.so.18库在mysql安装目录的lib下，设置LD_LIBRARY_PATH环境变量即可 export LD_LIBRARY_PATH=&quot;$DYLD_LIBRARY_PATH:/home/wangdong/local/mysql/lib&quot; ps： 网上一般的做法是在mysql/lib目录下找到libmysqlclient.so.18，设置软链接到usr/lib或者usr/lib64，但这两个目录都需要管理员权限的。 应用例子利用mysql客户端查询UCSC数据库，获取geneSymbol名的bed信息 脚本geneInfo.py 根据需要创建UCSC genome MySQL database数据来源：UCSC hg19 hg38 需要两种文件： （1）*sql是一种数据库脚本语言，是一种对关系数据库中的数据进行定义和操作的句法，为大多数关系数据库管理系统所支持的工业标准。可以用文本编辑器打开并查看 （2)数据文件。txt文件 例子：.sql文件 使用shell脚本自动完成 1234567891011# create databasemysql -u root -prootab -e 'create database hg19'# obtain table schemacd /home/wangdong/local/mysql/datawget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/ensGene.sql# create tablemysql -u root -prootab hg19 &lt; ensGene.sql# obtain and import table datawget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/ensGene.txt.gzgunzip ensGene.txt.gzmysqlimport -u root -prootab --local hg19 ensGene.txt 注意：数据需要下载到/home/wangdong/local/mysql/data，位置可以在配置文件(my.cnf)中修改，具体操作查看链接：MySQL 转移 datadir 为了便于在命令提示符下显示中文, 在创建时通过 character set gbk 将数据库字符编码指定为 gbk create database samp_db character set gbk; 创建数据库表时，对于一些较长的语句在命令提示符下可能容易输错, 因此我们可以通过任何文本编辑器将语句输入好后保存为 createtable.sql 的文件中, 通过命令提示符下的文件重定向执行执行该脚本。 mysql -D samp_db -u root -p &lt; createtable.sql desc TABLE_NAME; 显示数据库表的结构（desc是describe的缩写） select * from TABLE_NAME limit 0,2;显示表的前两行内容，0表示起始行偏移量 补充LD_LIBRARY_PATH环境变量LIBRARY_PATH和LD_LIBRARY_PATH是Linux下的两个环境变量，二者的含义和作用分别如下： LIBRARY_PATH环境变量用于在程序编译期间查找动态链接库时指定查找共享库的路径， 例如，指定gcc编译需要用到的动态链接库的目录。设置方法如下（其中，LIBDIR1和LIBDIR2为两个库目录）： export LIBRARY_PATH=LIBDIR1:LIBDIR2:$LIBRARY_PATH LD_LIBRARY_PATH环境变量用于在程序加载运行期间查找动态链接库时指定除了系统默认路径之外的其他路径， 注意，LD_LIBRARY_PATH中指定的路径会在系统默认路径之前进行查找。设置方法如下（其中，LIBDIR1和LIBDIR2为两个库目录）： export LD_LIBRARY_PATH=LIBDIR1:LIBDIR2:$LD_LIBRARY_PATH 区别与使用：开发时，设置LIBRARY_PATH，以便gcc能够找到编译时需要的动态链接库。 发布时，设置LD_LIBRARY_PATH，以便程序加载运行时能够自动找到需要的动态链接库。 可视化管理工具 MySQL Workbench&emsp;&emsp;尽管我们可以在命令提示符下通过一行行的输入或者通过重定向文件来执行mysql语句, 但该方式效率较低, 由于没有执行前的语法自动检查, 输入失误造成的一些错误的可能性会大大增加, 这时不妨试试一些可视化的MySQL数据库管理工具, MySQL Workbench 就是 MySQL 官方 为 MySQL 提供的一款可视化管理工具, 你可以在里面通过可视化的方式直接管理数据库中的内容, 并且 MySQL Workbench 的 SQL 脚本编辑器支持语法高亮以及输入时的语法检查, 当然, 它的功能强大, 绝不仅限于这两点。 MySQL Workbench官方介绍: http://www.mysql.com/products/workbench/ MySQL Workbench 下载页: http://dev.mysql.com/downloads/tools/workbench/ MySQL Show命令总结MySQL中有很多的基本命令，show命令也是其中之一，在很多使用者中对show命令的使用还容易产生混淆，本文汇集了show命令的众多用法。 show tables或show tables from database_name; — 显示当前数据库中所有表的名称。 show tables like ‘my%’; — 显示当前数据库中以my开头的表。 show databases; — 显示mysql中所有数据库的名称。 show [full] columns from table_name from database_name; 或show [full] columns from database_name.table_name; — 显示表中列名称。 show grants for user_name; — 显示一个用户的权限，显示结果类似于grant 命令。 show index from table_name; — 显示表的索引。 show status;(show master status;show slave status) — 显示一些系统特定资源的信息，例如，正在运行的线程数量。 show variables; — 显示系统变量的名称和值。 show [full] processlist; — 显示系统中正在运行的所有进程，也就是当前正在执行的查询。大多数用户可以查看他们自己的进程，但是如果他们拥有process权限，就可以查看所有人的进程，包括密码。 show table status; — 显示当前使用或者指定的database中的每个表的信息。信息包括表类型和表的最新更新时间。 show privileges; — 显示服务器所支持的不同权限。 show create database database_name; — 显示create database 语句是否能够创建指定的数据库。 show create table table_name; — 显示create database 语句是否能够创建指定的数据库。 show engies; — 显示安装以后可用的存储引擎和默认引擎。 show innodb status; — 显示innoDB存储引擎的状态。 show logs; — 显示BDB存储引擎的日志。 show warnings; — 显示最后一个执行的语句所产生的错误、警告和通知。 show errors; — 只显示最后一个执行语句所产生的错误。 show [storage] engines; –显示安装后的可用存储引擎和默认引擎。 参考（1）生物信息学学者学习mysql之路 （2）Linux环境变量LD_LIBRARY_PATH （3）使用MySQL制作SNP146数据库 （4）How to: create a partial UCSC genome MySQL database （5）5分钟入门MySQL Workbench]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计及挑选捕获测序探针，并计算覆盖度]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F10%2F29%2F%E8%AE%BE%E8%AE%A1%E5%8F%8A%E6%8C%91%E9%80%89%E6%8D%95%E8%8E%B7%E6%B5%8B%E5%BA%8F%E6%8E%A2%E9%92%88%EF%BC%8C%E5%B9%B6%E8%AE%A1%E7%AE%97%E8%A6%86%E7%9B%96%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[使用bedtools进行捕获测序引物的筛选并计算覆盖度 基因CDS区域捕获测序，引物设计确定目标基因的某一转录本的CDS区域坐标，整理成bed文件最简单的方法是从gff文件中，根据转录本蛋白ID,直接grep抽取出CDS区间的chr, satrt, end wget http://ftp.ncbi.nih.gov/refseq/H_sapiens/annotation/GRCh38_latest/refseq_identifiers/GRCh38_latest_genomic.gff.gz ungz GRCh38_latest_genomic.gff.gz mv GRCh38_latest_genomic.gff GRCh38.p10_ncbi.gff #get genes positions by gene symbol list in gene.txt gene=($(cat gene.txt));for gene in ${gene[@]}; do grep -m 1 &quot;${gene}&quot; GRCh38.p10_ncbi.gff &gt;&gt;gene_result.txt;done #get cds regions positions by proteins list in proteins_id.txt protein_id=($(cat protein_id.txt));for protein_id in ${protein_id[@]}; do grep &quot;${protein_id}&quot; GRCh38.p10_ncbi.gff &gt;&gt;cds_result.txt;done #get gene chrom locations or transfor gene symbol ID to essenble ID by gene symbol list in gene.txt wget http://ftp.ncbi.nih.gov/refseq/H_sapiens/Homo_sapiens.gene_info.gz head -n 1 Homo_sapiens.gene_info &gt; chr_result.txt gene=($(cat gene.txt));for gene in ${gene[@]}; do grep -m 1 &quot;${gene}&quot; Homo_sapiens.gene_info &gt;&gt;chr_result.txt;done 整理成bed文件 安捷伦的eArray可以设计，输入只需要目标区域的bed文件即可！ Agilent Genomic Workbench Downloads Installation Guide 补充（1）NCBI 的FTP站点中GRCh37和GRCh38的最新版基因组注释数据在该路径下：http://ftp.ncbi.nih.gov/refseq/H_sapiens/annotation Last updated: April 19, 2017 assembly name: GRCh38.p10（第十版） （2）对于做湿实验的同学可能通过浏览器的方式查询CDS区域坐标，但存在一个问题（首先浏览器上的版本是GRCh38.p7（第七版））！ NCBI —&gt;选择gene数据库—&gt;输入gene symbol —-&gt;点选gene symbol—-&gt;genebank—-&gt;找到目标转录本 &emsp;&emsp;可以看到CDS由一段一段的区域join形成的，但这是相对坐标，如何转换成基因组的绝对坐标呢？或许你会用基因的起始和结束位置进行加减，但尝试比较了下，发现坐标偏差很大（0.2M）。首先是版本的问题？, 其次可能NCBI的坐标系统中CDS区域的位置，使用在基因区间的基础上加减的做法不对。将上述链接最下方的FASTA序列的第62-199区间摘取出来，在UCSC中BLAT发现其结果是与从GRCh38.p10_ncbi.gff文件摘取出来的坐标一致的，所以用基因的起始和结束位置进行加减的方法是不正确的。 引物筛选 A组引物扩增区域（Amplicon_Stop-Amplicon_Start）长度范围为126-274nt，平均值为238nt B组引物扩增区域（Amplicon_Stop-Amplicon_Start）长度范围为124-174nt,平均值为158nt A,B两组引物都是针对同一目标区域设计的捕获探针，但是参数不同，导致扩增长度不同，覆盖度不同 &emsp;&emsp;所以A组引物（primer20）偏长，B组引物（primer24）偏短，现在需要将A组引物中扩增区域大于250nt的目标区域用 B组引物来扩增，实现长度互补和综合。 方法：使用bedtools工具即可！ 将targeted.bed与primer20.bed查看覆盖情况，将没有引物覆盖的，或者覆盖的引物扩增长度大于250nt的目标区域的bed区间抽取出来整理为target_region_of_primer20_up250.bed 将target_region_of_primer20_up250.bed与primer24.bed查看覆盖情况，将覆盖上的引物挑出，此时原则是目标区域即使有某条引物能全部覆盖，但为了测序数据更加准确，一个位置保证2-3个引物覆盖为好 将A组合B组引物保留的部分引物合并，同时整理出A,B两组引物都没有覆盖的目标区域。 1234567891011#step1bedtools intersect -a target.bed -b primer20.bed -wao &gt; primer20_result.txt awk '$6-$5&gt;250 &#123;print $1,$2,$3&#125;' primer20_result.txt &gt; target_region_of_primer20_up250.bed#step2bedtools intersect -a target_region_of_primer20_up250.bed -b primer24.bed -wao &gt; primer24_result.txt#step3primer20=($(cat primer20_reserved.txt));for primer20 in $&#123;primer20[@]&#125;; do grep -m 1 "$&#123;primer20&#125;" primer20_original.txt &gt;&gt;primer20_reserved_result.txt;doneprimer24=($(cat primer24_reserved.txt));for primer24 in $&#123;primer24[@]&#125;; do grep -m 1 "$&#123;primer24&#125;" primer24_original.txt &gt;&gt;primer24_reserved_result.txt;done 根据targete.bed和primer20.bed计算覆盖度12345678910111213(1)检验bed区间是否有重叠length_sum1=`awk 'BEGIN &#123;x=0&#125;&#123;x=x+$3-$2+1&#125;&#123;print x&#125;' target.bed |tail -n 1`length_sum2=`sort -k1,1 -k2,2n target.bed |bedtools merge -i | awk 'BEGIN &#123;x=0&#125;&#123;x=x+$3-$2+1&#125;&#123;print x&#125;'|tail -n 1`#如果length_sum1 &gt; length_sum2,有重叠；如果length_sum1 = length_sum2,五重叠#所以target.bed无重叠，primer20和primer24有重叠(2)先用bedtools工具得到primer20的merge后的bed文件，然后与target.bed查看覆盖碱基数，并求和sort -k1,1 -k2,2n primer20.bed|bedtools merge -i &gt; primer20_merge.bedlength_sum3=`bedtools intersect -a target.bed -b primer20_merge.bed -wao | awk 'BEGIN &#123;x=0&#125;&#123;x=x+$7&#125;&#123;print x&#125;' |tail -n 1` coverage = length_sum3/length_sum1*100 bedtools merge功能图示（需要先排序，否则报错） 参考（1）染色体位置overlap的计算–bedtools应用 （2）Tool: Converting Genome Coordinates From One Genome Version To Another (Ucsc Liftover, Ncbi Remap, Ensembl Api)]]></content>
      <categories>
        <category>NGS实验</category>
      </categories>
      <tags>
        <tag>NGS实验</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据库安装及入门]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F10%2F21%2FMySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%89%E8%A3%85%E5%8F%8A%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[无root权限，从源码编译安装MySQL数据库，及其简单使用 科德十二定律科德十二定律（Codd’s 12 rules）是由数据库的关系模型的先驱埃德加·科德（EdgarF. Codd）提出的，使数据库管理系统关系化需满足的十三条（从0至12）准则。又称为“黄金十二定律”。 准则0:&emsp;一个关系形的关系数据库系统必须能完全通过它的关系能力来管理数据库。 准则1 信息准则: &emsp;关系数据库系统的所有信息都应该在逻辑一级上用表中的值这一种方法显式的表示。 准则2 保证访问准则: &emsp; 依靠表名、主码和列名的组合，保证能以逻辑方式访问关系数据库中的每个数据项。 准则3 空值的系统化处理: &emsp;全关系的关系数据库系统支持空值的概念，并用系统化的方法处理空值。 准则4 基于关系模型的动态的联机数据字典: &emsp;数据库的描述在逻辑级上和普通数据采用同样的表述方式。 准则5 统一的数据子语言: &emsp;一个关系数据库系统可以具有几种语言和多种终端访问方式，但必须有一种语言，它的语句可以表示为严格语法规定的字符串，并能全面的支持各种规则。 准则6 视图更新准则: &emsp;所有理论上可更新的视图也应该允许由系统更新。 准则7 高级的插入、修改和删除操作: &emsp;系统应该对各种操作进行查询优化。 准则8 数据的物理独立性: &emsp;无论数据库的数据在存储表示或存取方法上作任何变化，应用程序和终端活动都保持逻辑上的不变性。 准则9 数据逻辑独立性: &emsp;当对基本关系进行理论上信息不受损害的任何改变时，应用程序和终端活动都保持逻辑上的不变性。 准则10 数据完整的独立性: &emsp;关系数据库的完整性约束条件必须是用数据库语言定义并存储在数据字典中的。 准则11 分布独立性: &emsp;关系数据库系统在引入分布数据或数据重新分布时保持逻辑不变。 准则12 无破坏准则: &emsp;如果一个关系数据库系统具有一个低级语言，那么这个低级语言不能违背或绕过完整性准则。 Relational Database Management Systems (RDBMS) Commercial products Open-source offerings Oracle MySQL (Oracle) DB2 (IBM) PostgreSQL MS SQL Server (Microsoft) SQLite 下载MySQL下载MySQL源代码 下载选项： Source Code -&gt; 源代码 Generic Linux (Architecture Independent) -&gt; 通用的Linux（独立结构） Compressed TAR Archive，Includes Boost Headers -&gt; 选择带有Boost头的压缩包（MySQL需要Boost C++库构建） wget https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-boost-5.7.20.tar.gz 从源安装MySQL，需要安装CMakecmake，一个跨平台的编译自动配置工具（作用生成makefile 文件） 123456789#!bin/shHOME=/home/wangdonginstall_path=$HOME/local/cmakemkdir -p $install_path/.srccd $install_path/.srcwget http://www.cmake.org/files/v2.8/cmake-2.8.12.2.tar.gztar zxvf cmake-2.8.12.2.tar.gzcd cmake-2.8.12.2./configure --prefix=$install_path 输出 CMake has bootstrapped. Now run gmake. #确保cmake没有错误发生 echo $? 0 #下面运行gmake命令 gmake gmake install 编译安装MySQL12345678910111213141516171819202122232425262728293031#（1）下载MySQL-5.6及创建相应目录结构HOME=/home/wangdonginstall_path=$HOME/local/mysqlmkdir -p /home/wangdong/local/mysqlwget https://dev.mysql.com/get/Downloads/MySQL-5.6/mysql-5.6.35.tar.gztar zxvf mysql-5.6.35.tar.gzcd mysql-5.6.35mkdir $install_path/data#（2）编译(系统已经安装cmake,此处徐使用上述自己安装的cmake)/home/wangdong/local/cmake/.src/cmake-2.8.12.2/bin/cmake -DCMAKE_INSTALL_PREFIX=$install_path \-DMYSQL_DATADIR=$install_path/data \-DMYSQL_UNIX_ADDR=$install_path/mysql.sock \-DDEFAULT_CHARSET=utf8 \-DDEFAULT_COLLATION=utf8_general_ci \-DMYSQL_TCP_PORT=8701 \ -DWITH_INNOBASE_STORAGE_ENGINE=1 \ -DENABLED_LOCAL_INFILE=1#（3)安装make &amp;&amp; make install#(4)配置（找到support-files/my-default.cnf，该文件是一个配置模板，在此基础上自己修改配置）cp support-files/my-default.cnf my.cnf#(5)使用启动my.cnf配置文件中的设置启动mysqld_safe服务和启动mysqlbin/mysqld_safe --defaults-file=my.cnf &amp; #&amp;表示后台运行support-files/mysql.server start|stop|restart #开始|停止|重启 如果执行成功，会提示如下内容： 同时，生成mysql.pid和mysql.sock文件，正常！ 以及查看13306端口，正常！ 关于配置的注意事项（1）最基本的需要修改以下7项（关于my.cnf配置文件的具体解释可以看这里（MySQL之——MySQL5.6 my.cnf 参数说明(附上自己服务器上真实配置文件)）） 在[mysqld]里修改port=13306 在[mysqld]里修改sock文件位置（socket） 在[mysqld]里添加MySQL基础目录（basedir） 在[mysqld]里添加MySQL数据存储目录（datadir） 在[mysqld]里添加MySQL日志文件（log-error） 在[mysqld]里添加MySQL进程文件（pid-file） 在[mysqld]里添加指定MySQL数据库所属用户（user -这里填写你所用的linux用户） （2）检查MySQL默认端口3306是否被占用，执行命令：netstat -apn | grep 3306如下图表明3306已经被占用，那么换一个端口，建议端口号在10000以上，比如改成13306，所以可以设置port=13306 如下图为my-default.cnf原始文件（模板） 如下图为修改配置后的my.cnf文件 注意：修改好my.cnf文件后，记得创建/home/wangdong/local/mysql/data和/home/wangdong/local/mysql/logs文件目录，否则在第（5）步会报错 启动MySQL每次启动MySQL都需要bin/mysqld_safe --defaults-file=my.cnf &amp; #&amp;表示后台运行 support-files/mysql.server start #开始 support-files/mysql.server stop #关闭 可以看到MySQL正常启动了，但是写入日志时不具有/var/log/mysqld.log的权限，问题是我们之前不是设置了log-error = /home/wangdong/local/mysql/logs/mysql_error.log吗？原因在于在linux系统中mysql配置文件的读取顺序为:/etc/my.cnf;/etc/mysql/my.cnf;/usr/local/mysql/etc/my.cnf ;~/.my.cnf 我们可以使用locate查找系统已有的my.cnf配置文件，如下图， 网上搜了一下解决办法，删除系统上/etc目录下的my.cnf配置文件，但无root权限做不到的，但还好，写入日志文件无权限并不影响MySQL的正常启动和使用！ MySQL数据库使用大致步骤包括： 第一次访问MySQL数据库的时候要以root进去（及-u root），且没有密码。 然后使用mysql数据库，修改root密码。 创建新的用户，以及授权 正常使用、登录mysql,root进去，且需要密码。 第一次访问MySQL数据库执行命令格式：bin/mysql –socket=mysql.sock -u root 一定要加上–socket=mysql.sock（mysql安装目录中的mysql.sock，否则提示ERROR 2002 (HY000): Can’t connect to local MySQL server through socket ‘/var/lib/mysql/mysql.sock’ (2)） 查看已有数据库，删除内置的test数据库执行命令（注意命令末尾的分号不可少）： show databases; drop database test; 查看内置MySQL用户，删除匿名用户执行命令： use mysql; #选择mysql数据库 select host, user, password from user; #从mysql数据库中的user表格中选择host, user, password三个字段 delete from user where user = &apos;&apos;; 修改密码执行命令： update user set password = password(&apos;rootab&apos;); 修改密码后需要先关闭mysql服务，然后重新启动my.cnf配置文件中的设置启动mysqld_safe服务和启动mysql，使新密码生效！！ 执行命令： support-files/mysql.server stop bin/mysqld_safe --defaults-file=my.cnf &amp; support-files/mysql.server start 正常使用，使用root登陆，并且需要密码登陆mysql -u 用户名 -h host_name -P 端口号 -p密码 (‘-P 端口号’的P大写 ‘-p密码’的p小写，且密码(password)和p之间没有空格) 如bin/mysql –socket=mysql.sock -u root -h localhost -P 13306 -prootab 也可以这样写： bin/mysql --socket=mysql.sock -u root -h localhost -P 13306 -p 然后在窗口弹出Enter password:时输入密码即可 （查看mysql命令的帮助: bin/mysql -I） 将bin/mysql加入环境变量echo &apos;export PATH=/home/wangdong/local/mysql/bin:$PATH&apos;&gt;&gt;~/.bashrc source ~/.bashrc 以后登陆直接： WORKDIR=/home/wangdong/local/mysql/ mysqld_safe --defaults-file=$WORKDIR/my.cnf &amp; $WORKDIR/support-files/mysql.server start mysql --socket=$WORKDIR/mysql.sock -u root -h localhost -P 13306 -p Do somethings exit; $WORKDIR/support-files/mysql.server stop (使用版本：Server version: 5.6.35 Source distribution) 参考（1）CentOS 6.4编译安装mysql 5.7 （2）从源编译MySQL （3）Linux MySQL免安装及配置 （4） linux中无root权限，装mysql 于用户目录步骤 （5）非root权限安装mysql]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R一页多图]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F10%2F06%2FR%E4%B8%80%E9%A1%B5%E5%A4%9A%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[发表文章一般都要求将多个相关的图组合成单个图，并标上A,B,C等标签。所以总结一下R语言绘图的一页多图，以及多图的拼接，子母图等的各种实现方法，！ cowplot：基于ggplot的可视化系统简介&emsp;&emsp;cowplot是一个ggplot2包的简单补充，意味着其可以为ggplot2提供出版物级的主题等。更重要的是，这个包可以组合多个”ggplot2”绘制的图为一个图，并且为每个图加上例如A,B,C等标签， 这在具体的出版物上通常是要求的。 代码123456789101112131415161718192021222324library(ggplot2)require(cowplot) plot.iris &lt;- ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_grid(. ~ Species) + stat_smooth(method = "lm") + background_grid(major = 'y', minor = "none") + panel_border() + labs(title = "dot-line plot")plot.mpg &lt;- ggplot(mpg, aes(x = cty, y = hwy, colour = factor(cyl))) + geom_point(size=2.5) + labs(title = "dot plot")plot.diamonds &lt;- ggplot(diamonds, aes(clarity, fill = cut)) + geom_bar() + theme(axis.text.x = element_text(angle=70, vjust=0.5)) + labs(title = "bar plot")ggdraw() + draw_plot(plot.iris, 0, .5, 1, .5) + draw_plot(plot.mpg, 0, 0, .5, .5) + draw_plot(plot.diamonds, .5, 0, .5, .5) + draw_plot_label(c("A", "B", "C"), c(0, 0, 0.5), c(1, 0.5, 0.5), size = 15) 结果 注意：其中draw_plot(plot, x = 0, y = 0, width = 1, height = 1, scale = 1)，坐标参数范围为0-1，相当于每幅图占画布长宽的百分比 rmisc包中的multiplot函数实现上述相同效果简单高效，一句话搞定！可以实现上述同样的结果。 代码12345library("Rmisc")library("lattice")library("plyr")multiplot(plot.iris,plot.mpg,plot.diamonds, layout = matrix(c(1,1,2,3), nrow=2, byrow=TRUE)) 下面看看源码中multiplot函数，本质是grid包！！ 源代码123456789101112131415161718192021222324252627282930313233343536multiplot &lt;- function(..., plotlist=NULL, file, cols=1, layout=NULL) &#123; library(grid) # Make a list from the ... arguments and plotlist plots &lt;- c(list(...), plotlist) numPlots = length(plots) # If layout is NULL, then use 'cols' to determine layout if (is.null(layout)) &#123; # Make the panel # ncol: Number of columns of plots # nrow: Number of rows needed, calculated from # of cols layout &lt;- matrix(seq(1, cols * ceiling(numPlots/cols)), ncol = cols, nrow = ceiling(numPlots/cols)) &#125; if (numPlots==1) &#123; print(plots[[1]]) &#125; else &#123; ###新建图表版面 grid.newpage() ####将版面分成nrow(layout)*ncol(layout)的矩阵 pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout)))) # Make each plot, in the correct location for (i in 1:numPlots) &#123; # Get the i,j matrix positions of the regions that contain this subplot matchidx &lt;- as.data.frame(which(layout == i, arr.ind = TRUE)) print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row, layout.pos.col = matchidx$col)) &#125; &#125;&#125; grid包实现上述相同效果代码12345678grid.newpage() ###新建图表版面grid.text("title of this panel", vp = viewport(layout.pos.row = 1, layout.pos.col = 1:2))pushViewport(viewport(layout = grid.layout(2,2))) ####将版面分成2*2矩阵vplayout &lt;- function(x,y)&#123;viewport(layout.pos.row = x, layout.pos.col = y)&#125;print(plot.iris, vp = vplayout(1,1:2)) ###将（1,1)和(1,2)的位置画图plot.irisprint(plot.mpg, vp = vplayout(2,1)) ###将(2,1)的位置画图plot.mpg print(plot.diamonds , vp = vplayout(2,2)) ###将（2,2)的位置画图plot.diamonds 使用gridExtra包实现上述相同效果123grid.arrange( arrangeGrob(plot.iris,left="A"), arrangeGrob(plot.mpg, left="B"), arrangeGrob(plot.diamonds, left="C"), layout_matrix = matrix(c(1,1,2,3), ncol=2, byrow=TRUE), top = "Title",left = "This is my global Y-axis title") 结果 单个图的标签（A,B,C）位置和属性不是很方便调整 使用gridExtra包实现多图的轴向组合代码12345678910111213141516hist_top &lt;- ggplot()+geom_histogram(aes(mtcars$mpg)) #绘制上方条形图empty &lt;- ggplot()+geom_point(aes(1,1), colour="white")+ theme(axis.ticks=element_blank(), panel.background=element_blank(), axis.line =element_blank(), axis.text.x=element_blank(), axis.text.y=element_blank(), axis.title.x=element_blank(), axis.title.y=element_blank())scatter &lt;- ggplot()+geom_point(aes(mtcars$mpg, mtcars$qsec)) #绘制主图散点图hist_right &lt;- ggplot()+geom_histogram(aes(mtcars$qsec))+coord_flip() #绘制右侧条形图#最终组合，由四个图拼图而成，只是右上角的图已经将各种标注去除了grid.arrange(hist_top, empty, scatter, hist_right, ncol=2, nrow=2, widths=c(4, 1), heights=c(1, 4)) 结果 其实绘制这种组合图已经有相应的R包了–ggExtra代码12345library(ggplot2)df &lt;- data.frame(x = mtcars$mpg, y = mtcars$qsec)p &lt;- ggplot(df, aes(x, y)) + geom_point() + theme_classic()ggExtra::ggMarginal(p, type = "histogram") ggExtra包中最主要的函数ggMarginal - Add marginal histograms/boxplots/density plots to ggplot2 scatterplots 子母图适用场景&emsp;&emsp;当做分组条形图时，有时碰到一组数据特北大，其他组数据特别小，这时候就不太美观了。可能你想到的第一个办法是截断，但其实还可以用字母图，形成局部放大的效果，既可以从整体上对比，又兼顾特别小的数据组可以轻松查看，而没必要单独做两张图。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# 载入需要的包library(plyr)library(Rmisc)library(ggplot2)library(grid)#创建数据A &lt;- rnorm(10,75,10)B &lt;- rnorm(10,12,5)C &lt;- rnorm(10,4,3)Group &lt;- c(rep('A',10),rep('B',10),rep('C',10))Value &lt;- c(A,B,C)D &lt;- data.frame(Group,Value)# 求标准差，后面画误差要用DSe &lt;- summarySE(D,measurevar = 'Value',groupvars = 'Group')# 把组别转化成因子D$Group &lt;- factor(D$Group)# 挑三种颜色C &lt;- c('#55a0fb','#ff8080','#c5944e')#画大图p1 &lt;- ggplot(DSe,aes(Group,Value)) + # 定义环境 geom_bar(aes(Group,Value,fill = Group),stat = 'identity',width = 0.5,show.legend = T) + # 画条形图 geom_errorbar(aes(ymin = Value-se, ymax = Value+se),width = .2) + #加误差线 scale_fill_manual(values = C) + # 配色 labs(x='修改X轴为Group',y='修改Y轴为Value',title = '添加标题')+ theme(axis.title = element_text(face = 'bold'),# 定义主題，如坐标轴，标题样式、背景等 axis.text = element_text(face = 'bold',colour = 'black'), plot.title = element_text(face = 'bold',colour = 'black',hjust = 0.5,size = 20), legend.background = element_rect(I(0),linetype = 1))p1 # 看下效果# 选择小数据集、小颜色集DSe2 &lt;- DSe[2:3,]C2 &lt;- C[2:3]# 画小图，基本和大图一樣，后面要背景透明、显示坐标轴p2 &lt;- ggplot(DSe2,aes(Group,Value)) + geom_bar(aes(Group,Value,fill = Group), stat = 'identity',width = 0.5) + geom_errorbar(aes(ymin = Value-se, ymax = Value+se),width = .2)+ scale_fill_manual(values = C2)+ guides(fill = F)+ theme(axis.text = element_text(face = 'bold',colour = 'black'), axis.title.y = element_blank(), axis.line = element_line(linetype = 1,colour = 'black'), # 显示坐标轴 plot.background = element_rect(I(0),linetype = 0), # 背景透明 panel.background = element_rect(I(0)), panel.grid.major = element_line(colour = NA), panel.grid.minor = element_line(colour = NA))p2 # 看下效果# 嵌套sub &lt;- viewport(width = 0.5,height = 0.4,x = 0.64,y = 0.6) # 配置环境。前兩個是子图的大小比例，后两个是位置，可自行调整到好看位置。p1 # 上大图print(p2,vp = sub) # 加小图 结果 关键代码就是利用了grid包的viewport函数，viewport简单说就是图形中一块矩形区域（类似于图层），是在这个区域中进一步绘图的基础。所以我们可以利用viewport函数在任意指定位置将两个或者多个图进行拼接~ ggplot2之分组和分面（利用facet_grid）&emsp;&emsp;以上是不同数据来源的图，以不同展现形式组合成一个图，另外ggplot2对一个数据集合进行分组，通过分面绘图，也算是实现一页多图的效果吧~ 代码12345678910111213setwd("F:/R working directory/GO enrichment")data&lt;-read.csv("GO.csv",header=TRUE,check.names = FALSE)data_new&lt;-subset(data,data$FDR&lt;0.5)library("ggplot2")ggplot(data=data_new)+ theme(plot.margin=unit(c(1,1,1,3),"cm"))+ #调整作图边距，上，右，下，左 geom_bar(aes(x=Term,y=Count,fill=-log10(Pvalue)),stat="identity")+ #指定横纵变量，stat指定统计变换 scale_fill_gradient(low='red',high='blue')+xlab('')+ylab('')+ #指定注释条带 #指定横纵标签的大小，角度，距离等细节 theme(axis.text.x = element_text(color='black',size = 5,angle=90, hjust=1), axis.text.y = element_text(color='black',size=10))+ facet_grid(.~Type) #根据Type划分作图区块 结果 可以看到结果并不是很好，横轴标签每个分组（BP,CC,MF）均含有13个通路，这是三个分组富集到的GO通路的交集，空值显示出来并不太美观 解决1234567891011setwd("F:/R working directory/GO enrichment")data&lt;-read.csv("GO.csv",header=TRUE,check.names = FALSE)data_new&lt;-subset(data,data$FDR&lt;0.5) #过滤数据Order &lt;-1:dim(data_new)[1] #生成横坐标输出顺序data_order&lt;-cbind(data_new,Order) #数据框添加一列library("ggplot2")ggplot(data=data_order,aes(x=reorder(Term,data_order$Order),y=-log10(Pvalue),fill=Type))+ coord_flip()+ #将图横向 xlab('')+ #覆盖横坐标 theme(plot.margin=unit(c(1,1,1,1),"cm"))+ #调整作图边距，上，右，下，左 geom_bar(stat="identity") 结果 主要通过给数据框加一列来对每一组进行排序输出 参考（1）【R:ggplot2】同时绘制多个图 （2）cowplot：基于ggplot的可视化系统 （3）R语言可视化——图表排版之一页多图 （4）Add labels to a plot made by grid.arrange from multiple plots （5）R语言grid包使用笔记——viewport （6）利用ggplot将多个图形组合在一起（推荐-置信椭圆） （7）Scatterplot with marginal histograms in ggplot2（推荐） （8）你終於會做截斷圖了，可審稿人說不讓做，怎麼破？ （9）ggplot2之分组和分面]]></content>
      <categories>
        <category>R绘图</category>
      </categories>
      <tags>
        <tag>R绘图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文本挖掘与展示之文献关键词]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F10%2F01%2F%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E4%B8%8E%E5%B1%95%E7%A4%BA%E4%B9%8B%E6%96%87%E7%8C%AE%E5%85%B3%E9%94%AE%E8%AF%8D%2F</url>
    <content type="text"><![CDATA[文本挖掘与展示之文献关键词 &emsp;&emsp;想看看Biotechnology and bioengineering杂志上最新的研究方向及研究热点，通过对文献的keywords进行统计分析，然后用词云进行可视化展示，似乎可行。 文献检索 可以看到我们需要的keywords在\标签下的\段落内，可以通过class = “keywords”进行定位。 火狐浏览器另存页面为PubMed-NCBI.html 利用python的bs4模块解析html文件123456789101112131415161718192021# coding=utf-8import osfrom bs4 import BeautifulSoupos.chdir(r'F:\pycharm_project\bs4')# 读取html文件并生成Soup对象File = open('PubMed-NCBI.html',encoding='UTF-8')Soup = BeautifulSoup(File,"lxml")# 使用finfAll方法定位class属性为keywords的div块，存入列表divs = Soup.findAll('div', &#123;'class':'keywords'&#125;)resultFile = open(r'.\keywords_result.txt', 'w',encoding='UTF-8')# 遍历列表divs，将结果keywords写入文件for div in divs: resultFile.write(div.p.string + '\n')resultFile.close() 利用R绘制词云进行展示123456789101112131415161718192021222324#第一步：读取setwd("F:/R working directory/wordcloud")text&lt;-scan("keywords_result.txt",what='')#第二步：分词library(jiebaR) ##加载包engine_s&lt;-worker() ##初始化分词器seg&lt;-segment(text,engine_s) ##分词head(seg)f1&lt;-freq(seg) ##统计词频head(f1)f2&lt;-f1[order((f1)[2],decreasing=TRUE),]##根据词频降序排列head(f2) # 第三步：绘图library(wordcloud2)#加载wordcloud2包wordcloud2(f2, size = 0.8 ,shape='star')#绘制成五角形状词云letterCloud(f2, size=1, word="生信学习") #绘制成字符 参考（1）python网络数据采集.pdf （2）https://mp.weixin.qq.com/s/Zhq7ysZ5wAvWNhPEauhRsQ]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas使用入门]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F10%2F01%2Fpandas%E4%BD%BF%E7%94%A8%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[pandas使用入门 pandas从文件读写数据读取数据 读取txt数据（read_table默认以tab键分割数据） Exp_matrix = pd.read_table(r’.\data\game_of_thrones_geneExp.txt’) 读取csv数据（read_csv默认以逗号分割数据） Exp_matrix = pd.read_csv(r’.\data\game_of_thrones_geneExp.csv’) 读取数量不定的空白符分割的txt文件 Exp_matrix = pd.read_table(r’.\data\game_of_thrones_geneExp.txt’,sep=’\s+’) 读取Excel文件数据 xls_file = pd.ExcelFile(‘data.xls’)&emsp;&emsp; #创建ExcelFile实例 table = xls_file.parse( ‘Sheet1’) &emsp;&emsp; #通过parse读取到DataFrame （对于使用复杂分隔符或多字符串分隔符的文件，csv模块就无能为力了。这种情况下，就只能使用字符串的split方法或者正则表达式的re.split进行拆分和整理了） 读数据时关于行列名选项 header=None &emsp;&emsp; #指定列名从0开始增长的数 names=[‘a’,’b’,’c’,’d’,’e’] &emsp;&emsp; #用names参数指定列名 index_col=’e’ &emsp;&emsp; #用指定的列作为行名（index） 逐块读取文本文件 nrows = 5 &emsp;&emsp; #只读取文件前5行 其他参数 使用skiprows跳过文件的某些行 skiprows = [0,2,3] 写数据至文件 写入csv文件 result.to_csv(‘D:\data\out1.csv’) 写数据时关于行列名选项 index=False &emsp;&emsp; #舍弃行名 header=False &emsp;&emsp; #舍弃列名 其他参数 sep = ‘|’ &emsp;&emsp; #指定分隔符 na_rep = ‘NULL’ &emsp;&emsp; #指定缺失值表示符号 数据规整化：清理，转换，合并，重塑pandas访问元素 取前8行 Exp_matrix_class_3 = Exp_matrix[0:8] 取前8列 Exp_matrix_class_3 = Exp_matrix[Exp_matrix.columns[0:8]] 取前8行，及1-3列 Exp_matrix_class_3 = Exp_matrix.iloc[0:8,1:4] 根据条件筛选行 Exp_matrix[Exp_matrix[‘Class’] == 3] 数据框转置 Exp_matrix_class_3_T = Exp_matrix_class_3.T 注意： (1)行列最小从0开始 (2)取值区间左闭右开 Dataframe 排序 逆序 Exp_matrix_T.sort_values([‘abs_dif_3_5’], ascending=False) 轴向连接 行合并 result = pd.concat([Exp_matrix_T[1:2],result_gene]) 增加一列 Exp_matrix_T[‘abs_dif_3_5’] = abs_dif_3_5 计算描述统计：统计函数 方法 说明 count 非NA值的数量 describe 针对Series或各DataFrame列计算汇总统计 min,max 计算最小值和最大值 argmin,argmax 计算能够获取到最小值和最大值的索引位置（整数) idxmin,idxmax 计算能够获取到最小值和最大值的索引值 quantile 计算样本的分位数（0到 1） sum 值的总和 mean 值的平均数， a.mean() 默认对每一列的数据求平均值；若加上参数a.mean(1)则对每一行求平均值 media 值的算术中位数（50%分位数) mad 根据平均值计算平均绝对离差 var 样本值的方差 std 样本值的标准差 skew 样本值的偏度（三阶矩） kurt 样本值的峰度（四阶矩） cumsum 样本值的累计和 cummin,cummax 样本值的累计最大值和累计最小 cumprod 样本值的累计积 diff 计算一阶差分（对时间序列很有用) pct_change 计算百分数变化 参考（1）PANDAS 数据合并与重塑（concat篇） （2）pandas数据的导入与导出 （3）使用Pandas对数据进行筛选和排序 （4）pandas小记：pandas计算工具-汇总统计]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux的CPU拓扑结构（Topology）]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F09%2F17%2Flinux%E7%9A%84CPU%E6%8B%93%E6%89%91%E7%BB%93%E6%9E%84%EF%BC%88Topology%EF%BC%89%2F</url>
    <content type="text"><![CDATA[由一次教训驱动的学习 &emsp;&emsp;“用服务器的同学注意下，以后请尽量不要在登录节点（frontend）提交任务，实在需要提交任务请注意任务消耗的资源情况，以免内存耗尽导致服务器死机！因为登录节点死机的话其他节点就都无法使用了！” 登陆节点frontend： [wangdong@frontend ~]$ 查看某一节点cpu使用情况例如第6节点： ssh node6 &apos;top -n 1 -b&apos;| head 批量查看节点cpu使用情况for((i=2;i&lt;=8;i++));do ssh node${i} &apos;top -n 1 -b&apos;| head ;done 更换节点例如更换至6节点： ssh node6 [wangdong@node6 ~]$ 具体含义：参考 不切换当前节点，在其他节点运行命令直接把命令写在ssh nodeX后面，然后是我习惯于加引号 ssh nodeX &apos;command&apos; 注意： 登录节点主要用于日常操作，如提交作业、查看作业运行情况、编辑、编译、压缩/解压缩等 不要在登录节点不通过作业调度系统直接运行作业，以免影响其余用户的正常使用 &emsp;&emsp;一般跑小程序不用考虑这些，但大程序消耗内存很大，所以尽量不要在登录节点（frontend）提交任务，实在需要提交任务请注意任务消耗的资源情况，以免内存耗尽导致服务器死机！因为登录节点死机的话其他节点就都无法使用了。 补充命令top查看当前运行的任务情况&emsp;&emsp;linux下用top命令查看cpu利用率超过100%。这里显示的所有的cpu加起来的使用率，说明你的CPU是多核，你运行top后按大键盘1看看，可以显示每个cpu的使用率，top里显示的是把所有使用率加起来。 注意： 按下1后可以看到我的机器的CPU是16核的。%Cpu0,,,,,,%Cpu15。 若是 0 代表目前该核心空闲， 可以运算作业。 100代表 cpu核心满负荷运算。若某个节点 16 个 CPU 负荷已很高， 换至其余空闲节点运算作业。 建议一个计算节点核心数最多使用 15 核。 查看 CPU Topology（拓扑结构）：cat /proc/cpuinfo 具体含义：参考 逻辑CPU个数： cat /proc/cpuinfo | grep ‘processor’ | wc -l 物理CPU个数： cat /proc/cpuinfo | grep ‘physical id’ | sort | uniq | wc -l 每个物理CPU中Core的个数： cat /proc/cpuinfo | grep ‘cpu cores’ | wc -l 查看内存的总量及消耗free -g 具体含义 参考 补充知识NUNA与SMP&emsp;&emsp;NUMA(Non-Uniform Memory Access，非一致性内存访问)和SMP(Symmetric Multi-Processor，对称多处理器系统)是两种不同的CPU硬件体系架构。 &emsp;&emsp;SMP的主要特征是共享，所有的CPU共享使用全部资源，例如内存、总线和I/O，多个CPU对称工作，彼此之间没有主次之分，平等地访问共享的资源，这样势必引入资源的竞争问题，从而导致它的扩展内力非常有限。 &emsp;&emsp;NUMA技术将CPU划分成不同的组（Node)，每个Node由多个CPU组成，并且有独立的本地内存、I/O等资源。Node之间通过互联模块连接和沟通，因此除了本地内存外，每个CPU仍可以访问远端Node的内存，只不过效率会比访问本地内存差一些，我们用Node之间的距离（Distance，抽象的概念）来定义各个Node之间互访资源的开销。 &emsp;&emsp;如果你只知道CPU这么一个概念，那么是无法理解CPU的拓扑的。事实上，在NUMA架构下，CPU的概念从大到小依次是：Node、Socket、Core、Processor（Node-&gt;Socket-&gt;Core-&gt;Processor） &emsp;&emsp;随着多核技术的发展，我们将多个CPU封装在一起，这个封装一般被称为Socket（插槽的意思，也有人称之为Packet，不知到哪个更加准确？），而Socket中的每个核心被称为Core。为了进一步提升CPU的处理能力，Intel又引入了HT（Hyper-Threading，超线程)的技术，一个Core打开HT之后，在OS看来就是两个核，当然这个核是逻辑上的概念，所以也被称为Logical Processor，本文简称为Processor。 &emsp;&emsp;综上所述，一个NUMA Node可以有一个或者多个Socket，一个多核Socket显然包含多个Core，一个Core如果打开HT则变成两个Logical Processor。 Logical processor只是OS内部看到的，实际上两个Processor还是位于同一个Core上，所以频繁的调度仍可能导致资源竞争，影响性能。 查看Numa Nodenumactl是设定进程NUMA策略的命令行工具，也可以用来查看当前的Nuwa node: numactl --hardware 从上面可以看出本机有两个Numa node，如果要进一步知道一个Node包含哪几个CPU，该怎么办？ 一种方法是通过查看ls /sys/devices/system/node/目录下的信息，例如： cpu0 cpu1 cpu10 cpu11 cpu2 cpu3 cpu8 cpu9 cpumap distance meminfo numastat 可见, node0包含4/5/6/7/12/13/14/15八个Processor（刚好是一个Socket）。 查看Socket&emsp;&emsp;一个Socket对应主板上的一个插槽，在本文中是指一个CPU封装。在/proc/cpuinfo中的physical id就是Socket的ID，可以从中找到本机到底有多少个Socket，并且每个Socket有那几个Processor。 1) 查看有几个Socket $ grep &apos;physical id&apos; /proc/cpuinfo | awk -F: &apos;{print $2 | &quot;sort -un&quot;}&apos; 0 1 $ grep &apos;physical id&apos; /proc/cpuinfo | awk -F: &apos;{print $2 | &quot;sort -un&quot;}&apos; | wc -l 2 2) 查看每个Socket有几个Processor $ grep &apos;physical id&apos; /proc/cpuinfo | awk -F: &apos;{print $2}&apos; | sort | uniq -c 8 0 8 1 3) 查看Socket对应那几个Processor 123456789101112131415awk -F: '&#123; if ($1 ~ /processor/) &#123; gsub(/ /,"",$2); p_id=$2; &#125; else if ($1 ~ /physical id/)&#123; gsub(/ /,"",$2); s_id=$2; arr[s_id]=arr[s_id] " " p_id &#125;&#125; END&#123; for (i in arr) print arr[i];&#125;' /proc/cpuinfo | cut -c2- 0 1 2 3 8 9 10 114 5 6 7 12 13 14 15 查看Core/proc/cpuinfo文件中的cpu cores表明一个socket中有几个cores，例如： cat /proc/cpuinfo | grep &apos;core&apos; | sort -u 上面的结果说明一个socket有4个cores，它们的id分别为0/1/9/10，根据之前查到的我们的机器有2个socket，所以总共有8个core。 查看Processor&emsp;&emsp;查看Processors的个数就比较简单了，从上面的统计结果中我们已经可以知道有16个Logical processor，不过也可以直接从/proc/cpuinfo文件中获取： $ grep &apos;processor&apos; /proc/cpuinfo | wc -l 16 参考（1）http://www.bio-info-trainee.com/1028.html （2）rabbit gao’s blog （3）http://blog.csdn.net/huangshanchun/article/details/44397581 （4）团子的小窝 （推荐） （5）http://tieba.baidu.com/p/1285719732?red_tag=s0690960116]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R语言之批量导入csv文件，并以文件名作为变量名]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F09%2F04%2FR%E8%AF%AD%E8%A8%80%E4%B9%8B%E6%89%B9%E9%87%8F%E5%AF%BC%E5%85%A5csv%E6%96%87%E4%BB%B6%EF%BC%8C%E5%B9%B6%E4%BB%A5%E6%96%87%E4%BB%B6%E5%90%8D%E4%BD%9C%E4%B8%BA%E5%8F%98%E9%87%8F%E5%90%8D%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;当要处理一批csv文件时，一个一个读入就太麻烦而且太low了。那么如何一次将工作路径的csv文件读入当前环境空间（Environment），并且使用文件名作为变量名，使用时直接调用即可呢?? 批量读入csv文件12345#将列表中文件读入环境空间，并以文件名作为变量名mycsvfile = list.files(pattern="*.csv") #生成.csv文件的列表list2env( lapply(setNames(mycsvfile, make.names(gsub("*.csv$", "", mycsvfile))), read.csv,header=TRUE,check.names=FALSE), envir = .GlobalEnv) 当然，上述语句也可以用于批量读入txt文件1234mytxtfile = list.files(pattern="*.txt")list2env( lapply(setNames(mytxtfile, make.names(gsub("*.txt$", "", mytxtfile))), read.table,header=TRUE,sep="\t"), envir = .GlobalEnv) 批量读入自定义函数对于自定义的函数，可以通过source函数导入环境空间，然后才能调用，当然也可以写个函数，调用此函数批量导入罗！！！ 12345678sourceDir &lt;- function(path, trace = TRUE, ...) &#123; for (nm in list.files(path, pattern = "[.][RrSsQq]$")) &#123; if(trace) cat(nm,":") source(file.path(path, nm), ...) if(trace) cat("\n") &#125;&#125;sourceDir("自定义函数所在文件夹的完整路径") 补充：涉及到的语法，包括R语言的环境空间，正则表达式。 1.R语言的环境空间 在R语言中，不管是变量，对象，或者函数，都存在于R的环境空间中，R程序在运行时都有自己的运行空间。 1234567# 当前环境空间&gt; .GlobalEnv# 列出当前环境中的变量 &gt; ls()# 查看环境空间加载的包&gt; search() 2.正则表达式 12pattern="*.csv" #list.files函数中用正则表达式匹配文件名的一个选项gsub("*.txt$", "", mytxtfile) #匹配以.txt结尾的文件名，将其替换为空，也就是去掉.txt，只留文件名作为变量名 3.lapply函数用于对mytxtfile列表中的文件，都应用read.csv函数，从而实现批量读取文件]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python实现excel二维表格格式化]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F09%2F02%2Fpython%E5%AE%9E%E7%8E%B0excel%E4%BA%8C%E7%BB%B4%E8%A1%A8%E6%A0%BC%E6%A0%BC%E5%BC%8F%E5%8C%96%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;Excel 是 Windows 环境下流行的、强大的电子表格应用。openpyxl 模块让 Python 程序能读取和修改 Excel 电子表格文件。例如，可能有一个无聊的任务，需要从一个电子表格拷贝一些数据，粘贴到另一个电子表格中。或者可能需要从几千行中挑选几行，根据某种条件稍作修改。或者需要查看几百份部门预算电子表格，寻找其中的赤字。正是这种无聊无脑的电子表格任务，可以通过 Python 来完成。 &emsp;&emsp;同时，发现之前写了很多小脚本，但代码的复用率极低。因此，今后的代码得更加注意用函数进行代码的封装。EXCEL中行的排序及筛选比较容易，但对于列比较麻烦。在此，通过一个对EXCEL列排序及筛选（取子列）的例子进行演示。 sort_column.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# -*- coding:utf-8 -*-'''EXCEL文件列根据自定义规则排序'''__file__ = 'sort_column.py'__date__ = '2017-9-2'__version__ = '0.1'__author__ = 'anjing'__blog__ = 'https://anjingwd.github.io/'__license__ = 'GPL v3 License'def sort_column(file, row1, column1_letter ,column2_letter): import openpyxl from openpyxl.cell import column_index_from_string #列字母和数字之间的转换 column1_num = column_index_from_string(str(column1_letter)) column2_num = column_index_from_string(str(column2_letter)) #从工作簿中取得工作表 wb = openpyxl.load_workbook(file) sheet1 = wb.get_sheet_by_name('Sheet1') sheet2 = wb.get_sheet_by_name('Sheet2') sheet3 = wb.create_sheet('result_sorted') #按顺序记录下待排序列的列号 order = [] #注意range范围是左闭区间，右开区间 for i in range(1,sheet2.max_row+1): for j in range(column1_num, column2_num+1): if sheet2.cell(row=i, column=1).value == sheet1.cell(row=row1, column=j).value: order.append(j) #将排序结果写入新的sheet3 count = column1_num for j in order: for i in range(row1, sheet1.max_row+1): sheet3.cell(row=i, column=count).value = sheet1.cell(row=i, column=j).value count += 1 #将起始列之前的列写入，一般是行名等 for i in range(row1, sheet1.max_row+1): for j in range(1, column1_num): sheet3.cell(row=i, column=j).value = sheet1.cell(row=i, column=j).value # 写入之后需要保存 wb.save(file) print("Everything has been done")######################################################################################################if __name__ == "__main__": print('对excel文件指定列区间排序，sheet1为数据，sheet2为自定义顺序顺序。') print('四个参数：输入excel绝对路径，需排序列的标题，列的起始列，终止列') print(r'用法例如：python sort_column.py F:\pycharm_project\excel_format\new1.xlsx 1 H AU') #Sys.argv是一个元组，里边的项为用户从程序外部输入的参数 import sys file = sys.argv[1] row1 = int(sys.argv[2]) column1_letter = sys.argv[3] column2_letter = sys.argv[4] sort_column(file, row1, column1_letter, column2_letter) 测试数据 new1.xlsxSheet1 Sheet2 使用在cmd下进入脚本sort_column.py和测试数据new1.xlsx所在目录，运行： python sort_column.py F:\pycharm_project\excel_format\new1.xlsx 1 B S 结果result_sorted 参考（1）书籍-Python编程快速上手让繁琐工作自动化–第12章处理 Excel 电子表格]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决ggplot 作图x轴和y轴不重合问题]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F09%2F01%2F%E8%A7%A3%E5%86%B3ggplot-%E4%BD%9C%E5%9B%BEx%E8%BD%B4%E5%92%8Cy%E8%BD%B4%E4%B8%8D%E9%87%8D%E5%90%88%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[解决ggplot 作图x轴和y轴不重合问题 只需要在作图时加上加上一下代码，即可即可让x轴和y轴在坐标原点重合： scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) 现在用测试代码看看前后变化： library(ggplot2) df &lt;- data.frame(x = 1:5, y = 1:5) p1 &lt;- ggplot(df, aes(x, y)) + geom_point() p1 &lt;- p1 + expand_limits(x = 0, y = 0) p1 library(ggplot2) df &lt;- data.frame(x = 1:5, y = 1:5) p2 &lt;- ggplot(df, aes(x, y)) + geom_point() p2 &lt;- p2 + expand_limits(x = 0, y = 0)+ scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) p2 参考： （1）http://stackoverflow.com/questions/36669095/y-axis-wont-start-at-0-in-ggplot （2）http://stackoverflow.com/questions/13701347/force-the-origin-to-start-at-0-in-ggplot2-r （3）http://stackoverflow.com/questions/29955618/starting-y-axis-at-0-using-ggplot-and-facet-wrap]]></content>
      <categories>
        <category>R-绘图</category>
      </categories>
      <tags>
        <tag>R-绘图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[批量fastqc]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F30%2F%E6%89%B9%E9%87%8Ffastqc%2F</url>
    <content type="text"><![CDATA[分别介绍使用循环和MultiQC软件两种方式批量进行fastqc 方法一：循环与命令并行问： 我当前文件夹下面有100个fastq测序文件，我要批量对他们运行fastqc软件（软件安装目录：~/biosoft/fastqc ）来处理，所以写了脚本 ls *fastq | while read id ; do ~/biosoft/fastqc $id;done 但是这样有一个弊端，就是这100个fastq测序文件是一个个被运行，这样耗时太长！所以我稍作修改： ls *fastq | while read id ; do nohup ~/biosoft/fastqc $id &amp; ;done 这样又一下子把100个fastqc任务提交了，我的服务器根本受不了。 所以需要修改修改脚本，让它一下子并行提交10个任务，因为我服务器限制。 该如何最简单的完成呢？ 答：加个参数-p ls *fastq | xargs -n 1 -p 10 -i FQ ~/biosoft/fastqc FQ 每次最多提交10个文件，加-p就可以并行计算啦 方法二：MultiQChomepage: http://multiqc.info 功能：把多个测序结果的qc结果整合成一个报告。支持fastqc、trimmomatic、bowtie、STAR等多种软件结果的整合。 Installation 安装conda install -c bioconda multiqc Run MultiQC 运行安装好后，进入你要分析的测序文件所在的文件夹，直接输入multiqc加要扫描的目录即可运行，如果要扫描当前文件夹，直接输入”multiqc .”即可 multiqc . multiqc /data/mydir/ multiqc /data/*fastqc.zip multiqc /data/sanple_1* 相关参数使用“–ignore”参数忽略某些文件multiqc . --ingore *_R2* multiqc . --ignore run_two/ multiqc . --ignore */run_three/*/fastqc/*.zip 使用文本指定要分析的文件的路径multiqc --file-list_my_file_list.txt 重命名输出结果分析结果默认命名为“multiqc_report.html”，相关的以tab风格的data file保存在“multiqc_data”文件夹下。可以用“-n”参数改变结果文件的名字，用“-o”改变输出文件的位置。 覆盖已存在的结果添加参数“-f”，输出结果时会自动覆盖同名文件。 更换报告模板添加参数“-t”或者“–template”可以选择不同风格的报告模板，具体内容请查看帮助文档“multiqc –help”。同时，MultiQC也支持自行创作结果文件的模板。 输出图片Exporting plot除了直接输出html文件外，Multiqc还可以直接保存图片文件。用以下参数进行保存： multiqc -p/--export 默认设置下，图片会保存在“multiqc_plots”文件夹中，以.png/.svg或者pdf格式保存。 同时，也可以直接在html文件中使用“toolbox”中的Export 保存图片。 报告正文报告页面分为三部分，左边是导航页面，中间是报告正文，右边是toolbox General Statistics （Configure Conlumns）可选择需要展示的列名 点击列名可进行排序 (plot)可选择任意两列进行plot Toolbox 工具栏主要有Hightlight Samples、Rename Samples、Show/Hide Samples、Export Plots、Saving Setting以及帮助等功能。 参考（1）https://mp.weixin.qq.com/s/tuGWPqCAG4TvIxpGsYWIDA（2）http://www.cnblogs.com/leezx/p/7360668.html]]></content>
      <categories>
        <category>NGS</category>
      </categories>
      <tags>
        <tag>NGS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[修复Chrome浏览器崩溃]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F28%2F%E4%BF%AE%E5%A4%8DChrome%E6%B5%8F%E8%A7%88%E5%99%A8%E5%B4%A9%E6%BA%83%2F</url>
    <content type="text"><![CDATA[Chrome浏览器一进入就全面崩溃，包括设置页面！ 解决办法： 原因就是 C:\Windows\System32\drivers\bd0001.sys 这个文件 首先可以把这个文件利用360杀毒工具强力删除。 因为书签路径是 C:\Users\Administrator\AppData\Local\Google\Chrome\User Data,所以然后将原Google删除（C:\Program Files (x86)\Google）并不会删除书签。 重新安装Chrome稳定版ChromeStandalone_60.0.3112.101_Setup.exe 重启电脑即可！ &emsp;&emsp;win7不能删除文件提示(您需要来自administrators的权限才能对此文件夹进行更改）该如何解决呢？ 可以使用360杀毒–功能大全-文件粉碎机]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用bash脚本下载和处理基因表达数据]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F25%2F%E4%BD%BF%E7%94%A8bash%E8%84%9A%E6%9C%AC%E4%B8%8B%E8%BD%BD%E5%92%8C%E5%A4%84%E7%90%86%E5%9F%BA%E5%9B%A0%E8%A1%A8%E8%BE%BE%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[使用bash脚本从ArrayExpress下载和处理基因表达数据（去接头和质控），也就是练习写生信分析pipline ArrayExpress数据库简介&emsp;&emsp;ArrayExpress是EBI的微阵列实验和基因表达谱的公共数据库，它是一个一般的基因表达数据库，设计用来保存来自所有微阵列平台的数据。ArrayExpress使用MIAME(Minimum Information About a Microarray Experiment，有关微阵列实验的最小化信息)注释标准及相关的XML数据交换格式MAGE-ML(Microarray Gene Expression Markup Language，微阵列基因表达标记语言)，它被设计成以结构化的方式来存储良好注释的数据。ArrayExpress基础结构由数据库本身，以MAGE-ML格式的数据提交或通过在线的提交工具MIAMExpress，在线数据库查询接口，Expression Profiler在线分析工具组成。ArrayExpress提供三种类型的提交，阵列，实验和实验方案，它们中的每一个都分配一个登录号。数据提交和注释的帮助由监管小组提供。数据库可以用诸如作者，实验室，物种，试验或阵列类型等参数进行查询。 随着越来越多的(an increasing number of)组织采用MAGE-ML标准，提交到ArrayExpress的量在快速增长着。 具体步骤1.AEArrayExpress主页搜索E-MTAB-567 https://www.ebi.ac.uk/arrayexpress/ 点击Export table in Tab-delimited format，下载E-MTAB-567.sdrf.txt 2.提取status,ID,link grep &apos;fastq.gz&apos; E-MTAB-567.sdrf.txt | head -2 | awk &apos;{print $39&quot;,&quot;$21&quot;,&quot;$35}&apos; &gt; status,ID,link.csv 输出: 3.mainPipeline.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145#!/bin/bashset -uset -eset -o pipefail#####################################################################################################################################################################################################################使用bash脚本从ArrayExpress下载和处理基因表达数据（去接头和质控）#####################################################################软件安装##安装fastx_toolkitconda install fastx_toolkit ##安装FastQconda install fastqc#################################################################################################################################################提取相关信息（extract.sh）for LINE in $(cat status,ID,link.csv)do echo $LINE STATUS=$(echo $LINE|cut -d, -f1) ID=$(echo $LINE|cut -d, -f2) LINK=$(echo $LINE|cut -d, -f3) FILE=$(basename $LINK) STEM=$(basename $LINK .gz) NEWID=$STATUS.$ID echo $STATUS, $ID, $LINK, $STEM, $FILE, $NEWID##############################################################################################输出：#u641750@GenekServer:~$ ./extract.sh #Normal,10N_1,ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR031/ERR031017/ERR031017_1.fastq.gz#Normal, 10N_1, ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR031/ERR031017/ERR031017_1.fastq.gz, ERR031017_1.fastq, ERR031017_1.fastq.gz, Normal.10N_1#Normal,10N_2,ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR031/ERR031017/ERR031017_2.fastq.gz#Normal, 10N_2, ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR031/ERR031017/ERR031017_2.fastq.gz, ERR031017_2.fastq, ERR031017_2.fastq.gz, Normal.10N_2#例如：$STATUS：Normal, $ID:10N_1, $LINK:ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR031/ERR031017/ERR031017_1.fastq.gz , $STEM:ERR031017_1.fastq, $FILE:ERR031017_1.fastq.gz, $NEWID:Normal.10N_1################################################################################################################################################[ -d ./fastqFiles ]|| mkdir -p ./fastqFiles#下载数据 if [ ! -f ./fastqFiles/$NEWID.$STEM ];then wget $LINK gunzip $FILE mv $STEM ./fastqFiles/$NEWID.$STEM #移动并改名 fidone##############################################################################################输出：#u641750@GenekServer:~/fastqFiles$ ls -l#total 16341536#-rw-r--r-- 1 u641750 GenekVIP 8358690360 Aug 22 21:29 Normal.10N_1.ERR031017_1.fastq#-rw-r--r-- 1 u641750 GenekVIP 8358690360 Aug 25 13:20 Normal.10N_2.ERR031017_2.fastq#################################################################################################################################################质量控制(修剪)[ -d ./trimmed/fastQC ]||mkdir -p ./trimmed/fastQCfor FILE in ./fastqFiles/*.fastqdo ls -lh $FILE STEM=$(basename $FILE .fastq) if [ ! -f ./trimmed/$STEM.trimmed.fastaq ];then fastq_quality_trimmer -v -t 20 -l 20 -Q 33 -i $FILE -o ./trimmed/$STEM.trimmed.fastq fidone##############################################################################################结果#u641750@GenekServer:~/trimmed$ ls#fastQC Normal.10N_1.ERR031017_1.trimmed.fastq Normal.10N_2.ERR031017_2.trimmed.fastq#输出：#-rw-r--r-- 1 u641750 GenekVIP 7.8G Aug 22 21:29 ./fastqFiles/#Normal.10N_1.ERR031017_1.fastq#Minimum Quality Threshold: 20#Minimum Length: 20#Input: 34536162 reads.#Output: 34458851 reads.#discarded 77311 (0%) too-short reads.#-rw-r--r-- 1 u641750 GenekVIP 7.8G Aug 25 13:20 ./fastqFiles/#Normal.10N_2.ERR031017_2.fastq#Minimum Quality Threshold: 20#Minimum Length: 20#Input: 34536162 reads.#Output: 33968272 reads.#discarded 567890 (1%) too-short reads.#################################################################################################################################################质量控制（评估）[ -d ./trimmed/fastQC ]||mkdir -p ./trimmed/fastQCFASTQC=/home/u641750/miniconda3/bin/fastqcfor FILE in ./trimmed/*.trimmed.fastqdo ls -lh $FILE STEM=$(basename $FILE .fastq) if [ ! -d ./trimmed/fastQC/$STEM_fastqc ]; then $FASTQC -o ./trimmed/fastQC $FILE fidone##############################################################################################结果：#u641750@GenekServer:~/trimmed/fastQC$ ls#Normal.10N_1.ERR031017_1.trimmed_fastqc.html#Normal.10N_1.ERR031017_1.trimmed_fastqc.zip#Normal.10N_2.ERR031017_2.trimmed_fastqc.html#Normal.10N_2.ERR031017_2.trimmed_fastqc.zip#################################################################################################################################################改名for FILEONE in ./trimmed/*_1.trimmed.fastqdo STEM=$(basename $FILEONE _1.trimmed.fastq | sed '' s/_1//g') FILETWO=$(echo $FILEONE | sed 's/_1./_2./g') ls -lh $FILEONE ls -lh $FILETWO echo stem $STEMexit done##############################################################################################输出#-rw-r--r-- 1 u641750 GenekVIP 7.5G Aug 25 15:29 ./trimmed/Normal.10N_1.ERR031017_1.trimmed.fastq#-rw-r--r-- 1 u641750 GenekVIP 7.5G Aug 25 15:44 ./trimmed/Normal.10N_2.ERR031017_2.trimmed.fastq#stem Normal.10N.ERR031017################################################################################################################################################ 在后台运行脚本nohup ./mainPipeline.sh &gt; outMainPipeline.log &amp; 附录：1.基本命令：basename 用途:返回一个字符串参数的基本文件名称 语法:basename String [ Suffix ] 描述:basename 命令读取 String 参数，删除以 /(斜杠) 结尾的前缀以及任何指定的 Suffix 参数，并将剩余的基本文件名称写至标准输出。 （截取文件名）例如，输入： basename Normal.10N_1.ERR031017_1.trimmed.fastq _1.trimmed.fastq 结果是：Normal.10N_1.ERR031017 例如，输入： basename /u/dee/desktop/cns.boo cns.boo 结果是：cns.boo 如果指定 Suffix（后缀名）参数，且它和字符串中所有字符都不相同，但和字符串的后缀相同，则除去指定后缀。例如，输入： basename /u/dee/desktop/cns.boo .boo 结果是：cns 2.fastq_quality_trimmer fastq_quality_trimmer [-h] [-v] [-t N] [-l N] [-z] [-i INFILE] [-o OUTFILE] 修剪reads的末端 [-t N] = 从5’端开始，低与N的质量的碱基将被修剪掉 [-l N] = 修建之后的reads的长度允许的最短值 [-z] = 压缩输出 [-v] =详细-报告序列编号，如果使用了-o则报告会直接在STDOUT，如果没有则输入到STDERR 3.以下代码段用于脚本的分步测试 1234567891011##########################################################用于测试STATUS="do"#STATUS="done"if [ $STATUS !="done" ];then do somethingfi######################################################### 参考:（1）Bash scripting for Bioinformatics https://www.youtube.com/watch?v=3ME7gayYeUQ （2）Linux命令之basename 命令 http://blog.sina.com.cn/s/blog_5f70c7060100ukyh.html （3）高通量测序数据的质控工具—fastx_toolkit软件使用说明 http://blog.sciencenet.cn/blog-1509670-848270.html]]></content>
      <categories>
        <category>NGS</category>
      </categories>
      <tags>
        <tag>NGS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux命令之nohup]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F25%2Flinux%E5%91%BD%E4%BB%A4%E4%B9%8Bnohup%2F</url>
    <content type="text"><![CDATA[命令后台运行及相关操作 nohup命令参考用途：该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。nohup就是不挂起的意思( no hang up) 语法：nohup Command [ Arg … ] [ &amp; ] 描述：nohup 命令运行由 Command 参数和任何相关的 Arg 参数指定的命令，忽略所有挂断（SIGHUP）信号。在注销后使用 nohup 命令运行后台中的程序。要运行后台中的 nohup 命令，添加 &amp; （ 表示”and”的符号）到命令的尾部 将任务移至后台在应用Unix/Linux时，我们一般想让某个程序在后台运行，于是我们将常会用 &amp; 在程序结尾来让程序自动运行。比如我们要在后台运行一个aaa.sh的脚本： ./test2.sh &amp; 可是当我们注销或者屏保后，这个脚本就会自动停止，这时我们就需要nohup命令， 怎样使用nohup命令呢？请看: [wangdong@frontend ~]$ nohup ./test2.sh &amp; [1] 18569 [wangdong@frontend ~]$ nohup: ignoring input and appending output to `nohup.out&apos; 注意： 只有当虚拟终端是 $ 或者 # 时候（如下图所示），才可关闭此终端，否则可能导致已经启动的进程被关闭（如果程序持续输出信息而没有出现 $ 或 #，按一下enter即可） 后台执行脚本（有多个脚本，每个窗口最好只执行一个），如果在同一个窗口执行，下个执行命令需要等待前面一个执行完成才能执行，这种效率不是我们要的。 无论是否将 nohup 命令的输出重定向到终端，输出都将附加到当前目录的 nohup.out 文件中，起到了log的作用。 如果当前目录的 nohup.out 文件不可写，输出重定向到 $HOME/nohup.out 文件中。除非另外指定了输出文件：nohup command &gt; myout.file 2&gt;&amp;1 &amp; /dev/null 是黑洞路径， 可用于用于丢弃输出，例如nohup COMMAND &gt; /dev/null &amp; 其他操作用 jobs 命令可以查看当前Screen中的后台任务。jobs [1]+ Done nohup ./test2.sh （1）jobs命令执行的结果，＋表示是一个当前的作业，减号表是是一个当前作业之后的一个作业。 （2）jobs -l 选项可显示所有任务的PID, （3）jobs的状态可以是running, stopped, Terminated 但当关闭该窗口在打开查看，这时候jobs已经不能使用，只能通过以下方式查看后台执行的脚本是否正在执行 ps -ef | grep conda #grep conda用于过滤命令 用 fg JOBID 命令可以将后台的任务调入前台继续运行（Foreground）。&emsp;&emsp;如果执行完后台脚本命令，当前窗口没有关闭，可以执行jobs查看当前窗口运行的后台进程的进程号，再通过fg n，将此后台进程调到前台执行。 命令在前台执行一半了，想转入后台执行 首先用 ctrl + z 快捷键可以将一个正在前台执行的命令放到后台，并且暂停 接着用 bg JOBID 命令可以将一个在后台暂停的命令，变成继续执行（Background）。 进程的终止前台进程的终止：ctrl+c 后台进程的终止： 方法一：通过jobs命令查看job号（假设为num），然后执行kill num 方法二：通过ps命令查看job的进程号（PID，假设为pid），然后执行kill pid 补充概念：当前任务&emsp;&emsp;如果后台的任务号有2个，[1],[2]；如果当第一个后台任务顺利执行完毕，第二个后台任务还在执行中时，当前任务便会自动变成后台任务号码“[2]”的后台任务。所以可以得出一点，即当前任务是会变动的。当用户输入“fg”、“bg”和“stop”等命令时，如果未指定jobsid，则所变动的均是当前任务。 ps -ef 是用标准的格式显示进程的 其中各列的内容意思如下 UID //用户ID、但输出的是用户名 PID //进程的ID PPID //父进程ID C //进程占用CPU的百分比 STIME //进程启动到现在的时间 TTY //该进程在那个终端上运行，若与终端无关，则显示? 若为pts/0等，则表示由网络连接主机进程。 CMD //命令的名称和参数 参考（1）http://rajaruan.blog.51cto.com/2771737/822199 （2）https://wenku.baidu.com/view/28863c79581b6bd97f19ead8.html （3）http://blog.sina.com.cn/s/blog_673ee2b50100iywr.html]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[创建生信分析pipline（dna-seq/rna-seq）-基础版]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F22%2F%E5%88%9B%E5%BB%BA%E7%94%9F%E4%BF%A1%E5%88%86%E6%9E%90pipline(DNA-seq%2CRNA-seq)-%E5%9F%BA%E7%A1%80%E7%89%88%2F</url>
    <content type="text"><![CDATA[创建生信分析pipline（dna-seq/rna-seq）-基础版 dna-seq建立项目文件系统(step1-prep_mkdir_cp.sh)1234567891011121314#建立合理的文件结构cdmkdir bin proj1 toolscd proj1mkdir reads fastqc ref snp-calling#拷贝相关的分析工具cp -r /home/training/tools/bwa ./tools/cp -r /home/training/tools/fastqc ./tools/cp -r /home/training/tools/samtools ./tools/#执行程序路径export PATH=$PATH:~/bin/ 安装软件（step2-install_software.sh）1234567891011121314151617181920212223242526272829303132#安装FastQCcd ~/tools/fastqc/unzip fastqc_v0.10.1.zipcd FastQCchmod +x fastqccd ~/bin/ln -s ~/tools/fastqc/FastQC/fastqc ./cdfastqc -h#安装BWAcd ~/tools/bwa/tar -jxvf bwa-0.7.3a.tar.bz2cd bwa-0.7.3a/makecp bwa ~/bin/cdbwa#安装samtoolscd ~/tools/samtools/tar -jxvf samtools-0.1.19.tar.bz2cd samtools-0.1.19/makecp samtools ~/bin/cp bcftools/vcfutils.pl ~/bin/cp bcftools/bcftools ~/bin/cdsamtoolsvcfutils.pl step3-snp_calling.sh123456789101112131415161718192021222324252627282930#测序数据质量控制cp /home/training/data/DNA-Seq/example1.* ~/proj1/reads/cd ~/proj1/fastqc/fastqc -f fastq -o ./ ../reads/example1.*#建立参考基因组索引cp /home/training/data/DNA-Seq/ref1.fa ~/proj1/ref/cd ~/proj1/ref/bwa index -a is ref1.fa#拼接组装##生成sai文件cd ~/proj1/bwa aln ref/ref1.fa reads/example1.L.fq &gt; aln_example1.L.saibwa aln ref/ref1.fa reads/example1.R.fq &gt; aln_example1.R.sai##生成sam文件bwa sampe ref/ref1.fa aln_example1.L.sai aln_example1.R.sai reads/example1.L.fq reads/example1.R.fq &gt; aln_example1.sam生成bam文件samtools view -bS aln_example1.sam | samtools sort - aln_example1_sorted#生成bcf文件cd ~/proj1/samtools mpileup -ugf ref/ref1.fa aln_example1_sorted.bam | bcftools view -bvcg - &gt; snp-calling/var_example1_sorted.raw.bcf#生成vcf文件bcftools view snp-calling/var_example1_sorted.raw.bcf | vcfutils.pl varFilter -D100 &gt; snp-calling/var_example1_sorted.flt.vcf rna-seq建立项目文件系统及软件安装（step1-prep_install.sh）123456789101112131415161718192021222324252627282930313233343536# generate foldercd ~/mkdir proj2cd proj2mkdir reads fastqc ref tophat# copy toolscd ~/cp -fr /home/training/tools/bowtie ./tools/cp -fr /home/training/tools/tophat ./tools/cp -fr /home/training/tools/cufflinks ./tools/# change pathexport PATH=$PATH:~/bin/# install bowtiecd ~/tools/bowtie/unzip bowtie2-2.0.5-linux-x86_64.zipcd bowtie2-2.0.5/cp bowtie2* ~/bin/# install tophatcd ~/tools/tophat/tar -zxvf tophat-2.0.8.Linux_x86_64.tar.gzcd tophat-2.0.8.Linux_x86_64/cp * ~/bin/# install cufflinkscd ~/tools/cufflinks/tar -zxvf cufflinks-2.0.2.Linux_x86_64.tar.gzcd cufflinks-2.0.2.Linux_x86_64/cp * ~/bin/# back to workpathecho "preparation and install complete..."cd ~/ 测序数据质量评估(step2-quality_control.sh)1234567# quality controlcp /home/training/data/RNA-Seq/example2-* ~/proj2/reads/cd ~/proj2/fastqc/fastqc -f fastq -o ./ ../reads/example2-*# back to workpathcd ~/ (step3-tophat_cuffdiff.sh)123456789101112131415161718192021222324252627282930313233#拷贝参考基因组数据cp /home/training/data/RNA-Seq/ref2.fa ~/proj2/ref/cp /home/training/data/RNA-Seq/ann2.gtf ~/proj2/ref/#建立基因组索引cd ~/proj2/ref/bowtie2-build ref2.fa ref2#生成bam文件cd ~/proj2/tophat/tophat2 -o E2-1-thout ../ref/ref2 ../reads/example2-1.L.fq ../reads/example2-1.R.fqtophat2 -o E2-2-thout ../ref/ref2 ../reads/example2-2.L.fq ../reads/example2-2.R.fq# 生成gtf文件cd ~/proj2/tophat/cufflinks -o E2-1-clout E2-1-thout/accepted_hits.bamcufflinks -o E2-2-clout E2-2-thout/accepted_hits.bam# 生成assemblies.txt文件touch assemblies.txtecho "./E2-1-clout/transcripts.gtf" &gt;&gt; assemblies.txtecho "./E2-2-clout/transcripts.gtf" &gt;&gt; assemblies.txt#生成注释文件cd ~/proj2/tophat/cuffmerge -s ../ref/ref2.fa assemblies.txt# 基于注释文件比较E2-1和E2-2表达差异cuffdiff -o diff_out1 -b ../ref/ref2.fa -L E2-1,E2-2 -u merged_asm/merged.gtf ./E2-1-thout/accepted_hits.bam ./E2-2-thout/accepted_hits.bam# 基于参考注释比较E2-1和E2-2表达差异cuffdiff -o diff_out2 -b ../ref/ref2.fa -L E2-1,E2-2 -u ../ref/ann2.gtf ./E2-1-thout/accepted_hits.bam ./E2-2-thout/accepted_hits.bam 注意： 建立合理的文件结构 bin目录的创建利于管理不同项目，使用不同软件版本 使用sh脚本 source step1-prep_mkdir_cp.sh （用source而不用sh是因为step1-prep_mkdir_cp.sh有添加环境变量的步骤，sh 对添加环境变量没用） 安装相关的分析工具 使用sh脚本安装软件 sh step2-install_software.sh 验证是否安装成功 ls bin 基因组比对用BWA,转录组比对用tophat 数据分析 上述命令是软件最初版本用法，软件版本更新用法会变化]]></content>
      <categories>
        <category>NGS</category>
      </categories>
      <tags>
        <tag>NGS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何优雅的生成及遍历python嵌套字典]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F19%2F%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E7%9A%84%E7%94%9F%E6%88%90python%E5%B5%8C%E5%A5%97%E5%AD%97%E5%85%B8%2F</url>
    <content type="text"><![CDATA[如何优雅的生成及遍历python嵌套字典 嵌套字典生成方法一:定义类12345class Vividict(dict): def __missing__(self, key): value = self[key] = type(self)() return value 解释： 第一行：class后面紧接着是类名，即Vividict，类名通常是大写开头的单词，紧接着是(dict)，表示该类是dict类继承下来的。 我们可以使用dir(dict）查看dict的方法12In[22]: print(dir(dict))['__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values'] 同理，可以查看Vividict的方法12In[23]: print(dir(Vividict))['__class__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__iter__', '__le__', '__len__', '__lt__', '__missing__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values'] 比较两者可以发现，Vividict的方法比dict的方法多了一个missing方法，也就是我们添加的方法。所以这就是继承，继承最大的好处是子类获得了父类的全部功能，而不必重新造轮子。 第二行：python魔法方法中的自定义序列，类似于定义一个函数。missing 在字典的子类中使用，它定义了当试图访问一个字典中不存在的键时的行为（目前为止是指字典的实例，例如我有一个字典 d ， “george” 不是字典中的一个键，当试图访问 d[‘george’] 时就会调用 d.missing(“george”)，结果为{} ）。 第三行，第四行：访问字典中不存在的键(key)时，返回空字典作为其返回值（value） 例如：1234In[17]: a = dict()In[18]: type(a)()Out[18]: &#123;&#125; 注意： 特殊方法“missing”前后有两个下划线！！！ 和普通的函数相比，在类中定义的函数只有一点不同，就是第一个参数永远是实例变量self，并且，调用时，不用传递该参数。除此之外，类的方法和普通函数没有什么区别，所以，你仍然可以用默认参数、可变参数、关键字参数和命名关键字参数。 使用： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# coding=utf-8#导入模块import os, openpyxlimport pprintfrom pandas import DataFrame#pprint模块可以输出漂亮的字典结构，但是不利于后期利用R作图#DataFrame可以将字典结构转为数据框输出，方便后期利用R作图#切换工作路径os.chdir(r'F:\pycharm_project\mutation_count')#读取excel表格wb = openpyxl.load_workbook('东方肝胆数据综合.xlsx')sheet = wb.active#定义类class Vividict(dict): def __missing__(self, key): value = self[key] = type(self)() return value#实例化d = Vividict()#字典初始化，赋初值0for i in range(2,sheet.max_row+1): d[sheet.cell(row=i, column=1).value][sheet.cell(row=i, column=15).value] = 0#累加统计各个样本各种突变类型的数目for i in range(2,sheet.max_row+1): d[sheet.cell(row=i, column=1).value][sheet.cell(row=i, column=15).value] +=1pprint.pprint(d)#输出字典结构pprint.pprint(d)&#123;'PDC1279A_vs_PDC1279': &#123;'UTR3': 9, 'UTR5': 4, 'downstream': 5, 'exonic': 149, 'intergenic': 170, 'intronic': 163, 'ncRNA_exonic': 17, 'ncRNA_intronic': 23, 'splicing': 2, 'upstream;downstream': 2&#125;, 'PDC1279C_vs_PDC1279': &#123;'UTR3': 11, 'UTR5': 13, 'downstream': 1, 'exonic': 174, 'intergenic': 189, 'intronic': 172, 'ncRNA_exonic': 24, 'ncRNA_intronic': 36, 'splicing': 4, 'upstream': 2, 'upstream;downstream': 2&#125;&#125;#输出数据框结构，缺损的元素用 NaN补齐frame = DataFrame(d)print(frame) PDC1279A_vs_PDC1279 PDC1279C_vs_PDC1279 \UTR3 9.0 11.0 UTR5 4.0 13.0 downstream 5.0 1.0 exonic 149.0 174.0 exonic;splicing NaN NaN intergenic 170.0 189.0 intronic 163.0 172.0 ncRNA_exonic 17.0 24.0 ncRNA_intronic 23.0 36.0 ncRNA_splicing NaN NaN splicing 2.0 4.0 upstream NaN 2.0 upstream;downstream 2.0 2.0 方法二：使用defaultdict()两个维度字典：1234from collections import defaultdictd = defaultdict(defaultdict)d[1][2] = 3 等价于： 12345678from collections import defaultdictdef nested_dict_factory(): return defaultdict(int)def nested_dict_factory2(): return defaultdict(nested_dict_factory)db = defaultdict(nested_dict_factory2) 当然，第一种方法简洁的多！ 要获得更多维度，你可以（三维）： 1234from collections import defaultdictd = defaultdict(lambda :defaultdict(defaultdict))d[1][2][3] = 4 使用defaultdict任何未定义的key都会默认返回一个根据method_factory参数不同的默认值, 而相同情况下dict()会返回KeyError. python中lambda存在意义就是对简单函数的简洁表示 实际上 defaultdict也是通过missing方法实现的。defaultdict在dict的基础上添加了一个missing(key)方法, 在调用一个不存的key的时候, defaultdict会调用missing, 返回一个根据default_factory参数的默认值, 所以不会返回Keyerror. 12In[35]: print(dir(defaultdict))['__class__', '__contains__', '__copy__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__iter__', '__le__', '__len__', '__lt__', '__missing__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'clear', 'copy', 'default_factory', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values'] 嵌套字典的遍历方法一：一层一层的嵌套迭代,从而实现遍历1234for key,value in d.items(): for key2, val2 in value.items(): print (key2, val2) 在类中定义walk方法实现嵌套字典的遍历123456789101112class Vividict(dict): def __missing__(self, key): value = self[key] = type(self)() return value def walk(self): for key, value in self.items(): if isinstance(value, Vividict): for tup in value.walk(): yield (key,) + tup else: yield key, value 解释:第1-4行：上面已经解释过了第5-11行：定义一个walk函数，并对字典items对象的key和value进行遍历，isinstance用于判断对象类型，如果value是一个字典，那么对value调用walk（）方法继续进行遍历，一层一层将key,value存储在元祖中（）。当最里面一层，即else情况，输出key,value。整个过程即将字典数据结构扁平化为元祖 此时，我们可以这样来遍历字典（输出元祖） 12345678910111213141516#打印整个元祖for tup in d.walk(): print(tup)('PDC1279_vs_PDC1279C6', 'downstream', 3)('PDC1279_vs_PDC1279C6', 'UTR3', 11)('PDC1279_vs_PDC1279C6', 'intronic', 164)('PDC1279_vs_PDC1279C6', 'splicing', 4)**这就是扁平化的字典**#打印元祖的第3列for tup in d.walk(): print(tup[2]) 参考（1）https://ask.helplib.com/229754 （2）Python魔法方法指南（推荐阅读） http://pyzh.readthedocs.io/en/latest/python-magic-methods-guide.html]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用链接（持续更新）]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F19%2F%E5%B8%B8%E7%94%A8%E9%93%BE%E6%8E%A5%EF%BC%88%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%EF%BC%89%2F</url>
    <content type="text"><![CDATA[常用链接（持续更新） 论坛 Name Description 医学论坛网 医学资讯及文献导读 seqanswer 关于测序的一个论坛 PLoB Public Library of Bioinformatics (中文博客) rna-seqblog RNA-seq英文博客 biotrainee 生信技能树 oschina 开源中国 Capital of Statistics, COS 统计之都 数据人网 大数据 Qgenomics 人基因组学与生物信息学(推荐) 雪晴数据网资讯前沿 R与python及数据分析（推荐） 学术导航 Name Description 学术网 文献，科技部，工程院，基金，论坛 龙猫学术导航 国内外文献图书，科研工具，专利标准导航，信息交流等 搞科研 学术圈新闻，科研工具，生活小工具，在线小工具等 学术网站大全 cnki 官方，绝对够全 linux及数据分析 Name Description Linux命令大全 linux命令搜索，查看帮助 awk &amp; sed awk &amp;sed ppt教程 Linux command line exercises for NGS data processing NGS数据处理实践过程中的linux命令行知识点 Introduction to Linux for bioinformatics VIB Bioinformatics Core Wiki ITArticles/Linux/ linux技术文章 分析流程 Name Description Illumina Amplicons Processing Workflow 16S pipelines bio简书 bwa+samtools+picardtools+GATK call SNP 流程 生信数据库 Name Description SNPedia link SNP genotypes to specific traits （SNP与人类疾病，人群频率，文献报道） GWAS Catalog GWAS研究的SNP,triat,Study等 OMIM(Online Mendelian Inheritance in Man) &emsp; PheGenI &emsp; ClinVar &emsp; Pheno-scanner 基因型和表明相互关联数据库 miRBase miRNA “官方”列表。 TimeTree 进化树的时间尺度。 IPDGC International Parkinson Disease Genomics Consortium GIANT Genetic Investigation of Anthropometric Traits) 2017NAR发布的收录数据库和web应用网站 十分全面（推荐） ensembl: ftp://ftp.ensembl.org/pub) &emsp;NCBI: ftp://ftp.ncbi.nih.gov/genomes/ &emsp; UCSC: ftp://hgdownload.soe.ucsc.edu/goldenPath &emsp; HapMap:ftp://ftp.ncbi.nlm.nih.gov/hapmap/ 常用数据库FTP地址 开发社区 Name Description Biostars 专注于生物信息学，计算基因组学和生物数据分析的问答社区 Segmentfault 一个专注于解决编程问题，提高开发技能的社区 Stackoverflow 编程相关的IT技术问答网站 GitHub 面向开源及私有软件项目的git托管平台 Gitbook 电子书搜索下载与diy python Name Description PYMOTW Python2标准库的使用 Bioinformatics code libraries and scripts 常遇到的问题都有脚本（py/pl） Python4cn(news, jobs) Python商业网站,技术博客,开源项目,图书 ,教程,讨论区等 张志明的个人博客 python，shell,运维（推荐） 静觅 崔庆才的个人博客 python 爬虫 The Invent with Python Blog python英文博客 MySQL数据库与生信 Name Description Relational Databases For Biologists:Efficiently Managing And Manipulating Your Data (2006) MySQL数据库操作与设计（推荐） 在线小工具 Name Description 在线工具 开源中国开发的，主要面向码农 smallpdf pdf转word等，在线工具 正则表达式在线测试 检查脚本中使用的正则表达式是否正确，提前发现问题 RGB颜色对照表 绘图配色必备 在线代码着色 看看就好 在线 Markdown 编译器 用着不错 ASCII码对照表 速查 草料二维码（静态二维码） &emsp;&emsp; Q码（动态二维码） 比较好玩儿 Tagxedo 词云（不是所有浏览器都可以） 批改网 英语作文在线免费批改（还阔以） jvenn 在线绘制维恩图 搜索 Name Description PDFDRIVE 搜索下载免费的pdf电子书 slideshare 免费且高质量英文ppt下载与分享 Hi,PPTer PPT资源导航 Library Genesis 外文书籍的超级好用网址,免费下载 FreeBookCentre.net 英文版电子书分类免费下载 Explore · GitBook GitBook开源电子书搜索 PDF电子书基地 免费下载，比较全 youtube 可以搜索很多视频教程（可以设置加字幕和自动翻译） 全国图书馆参考咨询联盟 查询和试看 鸠摩搜书 pdf和epub电子书，好用 B–OK The world’s largest ebook library. 软件 Name Description 冰点文库下载器 自由下载百度、豆丁、丁香、MBALib、道客巴巴、Book118等文库文档 visio 专业绘制流程图 谷歌访问助手 “google/gmail/google scholar”一网打尽 Google和Google学术可用链接汇总 能用 硕鼠网络视频下载器 爬取网络视频 TeamViewer 12 远程控制 iMindMap 8 思维导图 OK &emsp; 美化大师 PPT特效，模板插件 prize &emsp; prize中国 prezi是一款在线的演讲文稿生成软件,“动态”PPT 多看 自带取词及字典功能，并且笔记可以直接导入印象笔记（推荐） Pan Download 百度网盘不限速下载软件 谷歌插件 Name Description Sci-Hub Links 转换 DOI 和 Pubmed 位址为 sci-hub 连结 医学文献助手(有限版) 在 PubMed 搜索结果页面，添加 PDF 链接、影响因子、F1000评论、作者发表记录 等 远方 New Tab 每天打开新页面都是一段不期而遇的旅行 Search by Image (by Google) 用图片搜索 新媒体管家 微信公众号排版及扩展功能 油猴（Tampermonkey） &emsp; 推荐“油猴脚本” 爱奇艺，腾讯VIP视频破解 &emsp; 无需插件、软件 在线解析VIP视频 通过安装各类脚本对网站进行定制 Tabs Outliner 书签快速保存 文献 Name Description Researchgate &emsp; 文献鸟 文献推送，文献跟踪与速读小工具 Web of Science 大型综合性、多学科、核心期刊引文索引数据库 PubMed 免费海量英文文献 中国知网 中文期刊 论文下载网 中文论文下载，知网，万方 SCI-HUB 收费文献免费下载 梅斯医学 影响因子查询（APP更好用） 科研动力 专注EndNote, 关注科研论文写作 GCBI 与PubMed同步并课显示影响因子 pubmedplus 文献摘要自动翻译（翻译得比较好） China PubMed 英文文献摘要翻译（更新不够快） Article-based PubMed Search Engine 快速搜寻到与你的PubMed索引论文所相关的文章 CNKI硕博士学位论文搜索 根据学位授予单位分类搜索（推荐） SJTU 上海交通大学学位论文数据库 论文写作与投稿 Name Description English Communication for Scientists 一本小书（推荐） International Science Editing 文章润色服务，其中writing tips部分很实用 ozdic 英语搭配词典（推荐） Purdue Online Writing Lab 普渡大学学术英语写作资源 Plagiarism Certification Tests for Master’s and Doctoral Students 印第安纳大学鉴别论文剽窃小测试 生信软件 Name Description hope的博客总结 常用生物信息在线工具 生信客部落总结 生物信息工具整合(包含在线工具与离线工具) ESPript 美得令人心动的序列显示工具 Pathway Builder Tool 零基础也能画出超酷信号通路图 cytoscape 相互作用网络绘图 ITOL 进化树后期编辑神器 EVOLVIEW 进化树编辑，编程加多层注释圈儿 Bioconductor annotation packages Bioconductor注释文件合集 DNAMAN DNAMAN 8 全功能英文版，免序列号（By Raindy） 实验 Name Description 实验小白 基础实验介绍 生物无忧 中文实验视频教程 Journal of Visualized Experiments ( JoVE) JoVE是一份展示可视化实验的期刊，是世界上第一个同行评议的科技视频期刊 PROTOCAL ONLINE 生物医学研究领域的实验protocols以及最新实验方面的文章 Nature protocols Nature的实验Protocol,述详细的实验步骤及注意事项 Nature methods 主要发表生命科学领域中新的研究方法或对经典方法有显著改进的相关论文 Nucleic Acids Research Methods 可按不同研究领域来查找具体的实验技术 Springer出版社的Protocol系列 在“Browse by discipline”一栏选择“Life science”,然后在“Content Type”一栏点击protocol,在搜索框输入关键词查询实验方法 Cold Spring Harbor Protocols 冷泉港实验室Protocols bio-protocol 质量非常高，而且全部免费，非常好用 Nature protocols或者Springer Protocol不仅可以搜实验的protocol，还可以搜分析的protocol 推荐博客 Name Description 施一公个人博客 心灵鸡汤 老D博客 网络为主，各种黑科技 车小胖的博客 网络 raindu’s home R与Excel数据可视化 Sam’s Note 专注转化医学，专注生物信息 Bob’s Blog 药学工作者中的极客达人，精通SQL，R，擅长数据分析，数据挖掘。 BioChen生物 Lnc RNA , ChiP, Genome（推荐） Y叔的博客 R ，生信 （推荐） hope 基因组和转录组方面 文字的博客 GWAS (推荐) free_mao的博客 python, linux ,gwas 生信客部落 python , R ,工具 生物信息博客 生物数据库、软件、程序介绍 刘辉的博客 生信基础 生信笔记 生信入门级 花之恋的博客 生信工具等 CALYX&amp;YAMOL 博客 WeGene基因一员 JackTalk 生物信息学，计算生物学，python fanyucai的博客 生信 EmanLee, Eman Lee’s Space (blog, website) 生信 飘的博客 生物信息学资料分享，写的比较简单 Zuguang Gu 英文博客，博主在德国 陈连福NGS 陈连福NGS,开培训班 博耘生物 已经不再更新了 Spring‘s info 生物信息，已经不再更新 rabbit gao’s blog 计算机、数学、生物混合男，已经不再更新了 Bioinformatics 已经不再更新了 铁汉1990的博客 已经不再更新了 生物日志-鸣一到 生物信息分析教程，数据可视化，已经不再更新了 云之南博客 已经不再更新了 Bioinformatics Zen 英文博客，已经不再更新了 沈梦圆博客 似乎已经不再更新了 R语言 Name Description awesome-R R包中文介绍【全】 Bioconductor - Mirrors Bioconductor镜像汇总 RStudio Blog &emsp;R-bloggers &emsp;R Programming Resource Center&emsp; R China RStudio 博客, R博客,R 编程资源中心（链接），R语言中文网 Quick-R &emsp; R Tutorial &emsp; R &emsp; R基础资料 R语言忍者秘笈 &emsp; R语言在线电子书 KENT STATE SPSS 教程 PUL Getting Started in Data Analysis: Stata, R, SPSS, Excel: SPSS MATLAB Bioinformatics Toolbox MATLAB生信分析示例 Kings college london R语言课件—伦敦国王大学 OnePageR A Survival Guide to Data Science with R R for Bioinformatics &emsp;R for Biomedical Statistics &emsp; R for Time Series R语言英文小书 Statistics with R &emsp; Using R for statistical analyses R统计 StatsBlogs 统计相关博客R,SAS等 Bioconductor for Genomic Data Science 关于Bioconductor的网络课程，有视频，有材料 张丹博客 原创了大量R语言技术实战文章，包括R的极客理想系列文章，RHadoop实战系列文章，R离间NoSQL系列文章，并写作”R的极客理想”系列图书 谢益辉的个人博客 博客主要包括各种有趣的技术和吐槽文章。谢益辉是统计之都的创始人，现任RStudio公司程序员 刘思喆的个人博客 博客中主要包括R语言企业级应用的文章。刘思喆现任京东推荐算法经历 李舰的个人博客 博客中主要包括R语言建模的文章。李舰现任Mango Solutions中国区数据总监 阿稳的个人博客——不周山 博客中主要包括R语言并行技术的文章。 WHAT YOU’RE DOING IS RATHER DESPERATE-（推荐） 作者为澳大利亚悉尼大学的生物信息学家 The Yhat Blog machine learning, data science, engineering(推荐) 数据可视化 Name Description plotly 支持R和python，图例丰富（含代码） ECharts ECharts GL 是 WebGL 的扩展包，提供了丰富的三维可视化组件以及常规图表的性能增强 WebGL 中文网 教程 网络生信课程 Name Description coursera 在网上学习全世界最好的课程 Bioinformatics: Introduction and Methods 北大MOOK生物信息学导论 陈魏学基因 NGS相关视频 Applied Bioinformatics Course 北大罗静初老师abc生信课程 Biomedical Data Science 统计，算法，NGS书籍 NOVOCRAFT NOVOCRAFT 公司NGS资料（干货） Workshops 康奈尔大学生信课件（2010-2017） Bioinformatics and Genome Analyses Courses 巴斯德研究所2005-2017生信课程全集 CGU-GIBMS 台湾长庚大学生信课件 Bioinformatics Team (BioITeam) at the University of Texas 德州大学奥斯丁分校生信课件 CANB7640 COURSE WEBSITE 生信课件-按研究内容分 UCLA 加利福尼亚大学洛杉矶分校NGS数据处理workshop最全合集 Purdue university Discovery Park 普渡大学生物信息课程 Bioinformatics Crash Course - July 2014 &emsp; RNA-seq Analysis &emsp; Linux and HPCC 马里兰大学的生信中心课程-NGS分析，RNA-seq分析，linux和高性能计算 BTI Bioinformatics Course 2017 美国康奈尔大学博伊斯汤普森植物研究所课件 Swiss Institute of Bioinformatics 瑞士生物信息学研究所8年培训资料 BioFrontiers Education 科罗拉多大学生物信息课程 Statistics 246 Statistical Genetics Spring 2006 伯克利大学的遗传统计学课程 NGSchool 2016 Workshops NGSchool非常全面的ppt BIOINFORMATICS PLATFORM 德国柏林医学院中心生物信息学课程资料 UT HEALTH SCIENCE CENTER 圣安东尼奥-生命健康中心-jin实验室生物信息学课程ppt MSc lecture Genomics 柏林自由大学 Genomics12 Statistical Genetics &emsp;NGS &emsp;Statistical Genetics and Genomics 伯明翰阿拉巴马大学-生物信息视频教程 Babraham Institute the Bioinformatics 伯拉罕研究所生信课程 Applied Bioinformatics 2014 biostar handbook-生信分析 Computational and Systems Biology” Video Lectures MIT计算与系统生物学课程 UCR(Institute for In tegrative Genome Biology) Manuals 包括NGS with R/Bioconductor，NGS Analysis with Galaxy and IGV， EMBOSS，Linux 等十分全面。 VIB Bioinformatics Core 从基础到实践，非常全面 Learn about Bioinformatics and Computational Tools for Biology 一直在更新，推荐 Canadian Bioinformatics Workshops Course Materials 2004-2016超级多 Bioinformatics Links Directory 生信资源，工具和数据库 WikiOmics Open Bioinformatics - a collection of questions and answers about bioinformatics.]]></content>
      <categories>
        <category>常用链接</category>
      </categories>
      <tags>
        <tag>常用链接</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bedtools使用教程详解]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F19%2Fbedtools%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;bedtools开发的目的是为了快速，灵活的比较大量的基因组特征（genomic features）。而genomic features通常使用Browser Extensible Data (BED) 或者 General Feature Format (GFF)文件表示，用UCSC Genome Browser进行可视化比较。 &emsp;&emsp;例如,bedtools可以进行取intersect（交集）, merge（并集）, count（计数）, complement（补集），以及用来对广泛使用的基因组文件格式，例如BAM, BED, GFF/GTF, VCF等进行基因组区间的转换。单个的工具设计的目的是应对简单的任务，复杂的分析能通过组合多个bedtools工具操作实现。同时，该工具允许控制输出结果的呈现形式。最初的bedtools版本支持单独的6列BED文件。但是，如今增加了对序列比对BAM文件的支持。以及GFF文件的特征，BED文件。以及VCF文件。这些工具是相当快速的，并且即使是大的数据集也可以在数秒内完成任务。 我们用bedtools都可以做些啥?&emsp;&emsp;bedtools总共有二三十个工具/命令来处理基因组数据。比较典型而且常用的功能举例如下：格式转换，bam转bed（bamToBed），bed转其他格式（bedToBam，bedToIgv）；对基因组坐标的逻辑运算，包括：交集（intersectBed，windowBed），”邻集“（closestBed），补集（complementBed），并集（mergeBed），差集（subtractBed）;计算覆盖度（coverage）（coverageBed，genomeCoverageBed）；此外，还有一些强大而实用的工具（shuffleBed，groupBy，annotateBed，……） Utility Description annotate Annotate coverage of features from multiple files. bamtobed Convert BAM alignments to BED (&amp; other) formats. bamtofastq Convert BAM records to FASTQ records. bed12tobed6 Breaks BED12 intervals into discrete BED6 intervals. bedpetobam Convert BEDPE intervals to BAM records. bedtobam Convert intervals to BAM records.** closest Find the closest, potentially non-overlapping interval. cluster Cluster (but don’t merge) overlapping/nearby intervals. complement Extract intervals not represented by an interval file. coverage Compute the coverage over defined intervals. expand Replicate lines based on lists of values in columns. flank Create new intervals from the flanks of existing intervals. genomecov Compute the coverage over an entire genome. getfasta Use intervals to extract sequences from a FASTA file. groupby Group by common cols. &amp; summarize oth. cols. (~ SQL “groupBy”) igv Create an IGV snapshot batch script. intersect Find overlapping intervals in various ways. jaccard Calculate the Jaccard statistic b/w two sets of intervals. links Create a HTML page of links to UCSC locations. makewindows Make interval “windows” across a genome. map Apply a function to a column for each overlapping interval. maskfasta Use intervals to mask sequences from a FASTA file. merge Combine overlapping/nearby intervals into a single interval. multicov Counts coverage from multiple BAMs at specific intervals. multiinter Identifies common intervals among multiple interval files. nuc Profile the nucleotide content of intervals in a FASTA file. overlap Computes the amount of overlap from two intervals. pairtobed Find pairs that overlap intervals in various ways. pairtopair Find pairs that overlap other pairs in various ways. random Generate random intervals in a genome. reldist Calculate the distribution of relative distances b/w two files. shift Adjust the position of intervals. shuffle Randomly redistribute intervals in a genome. slop Adjust the size of intervals. sort Order the intervals in a file. subtract Remove intervals based on overlaps b/w two files. tag Tag BAM alignments based on overlaps with interval files. unionbedg Combines coverage intervals from multiple BEDGRAPH files. window Find overlapping intervals within a window around an interval. BEDTools suite使用详细bedtools官网： http://bedtools.readthedocs.io/en/latest/ bedtools使用说明： http://quinlanlab.org/tutorials/bedtools/bedtools.html#bedtools-merge BEDTools主要使用BED格式的前三列,即： chrom: 染色体信息 start: genome feature的起始位点，从0开始 end: genome feature的终止位点，至少为1 一般常用物种的genome file在BEDTools安装目录的/genome里面 BEDPE格式是其自定义的一种新的格式，为了简洁的描述不连续的genome features，例如结构变异和双端测序比对 注意： start1和start2起始坐标第一个碱基都为0，所以start=9, end=20表示碱基跨度是从第10位到第20位 chrom1或者chrom2用.表示unknown;start1，end1,start2,end2用-1表示unknown (1)intersect&emsp;&emsp;可以计算两个或者多个BED/BAM/VCF/GFF文件中基因组坐标位置的交集(overlap)，根据参数不同，可以得到不同的结果。 两个BED文件比较图示 一对多比较图示 语法：bedtools intersect -a &lt;bed/gff/vcf/bam&gt; -b &lt;bed/gff/vcf/bam&gt; [OPTIONS] -wa参数可以报告出原始的在A文件中的feature -wb参数可以报告出原始的在B文件中的feature -c参数可以报告出两个文件中的overlap的feature的数量 -wo 返回overlap碱基数 -v 返回非overlap区间 -s 相同链上的feature 当用bedtools intersect 处理大文件时比较耗内存，有效的方法是对A和B文件按照染色体名字(chromosome)和位置(position)排序(sort -k1,1 -k2,2n),然后用-sorted参数重新intersect 案例注意，自己生成测试bed文件，都必须用tab键分割，否则会报错！！ 案例一：包含着染色体位置的两个文件，分别记为A文件和B文件。分别来自于不同文件的染色体位置的交集是什么？ $ cat A.bed chr1 10 20 chr1 30 40 $ cat B.bed chr1 15 25 $ bedtools intersect -a A.bed -b B.bed chr1 15 20 案例二：包含着染色体位置的两个文件，分别记为A文件和B文件。求A文件中哪些染色体位置是与文件B中的染色体位置有overlap. $ cat A.bed chr1 10 20 chr1 30 40 $ cat B.bed chr1 15 25 $ bedtools intersect -a A.bed -b B.bed -wa chr1 10 20 案例三：包含着染色体位置的两个文件，分别记为A文件和B文件。求A文件中染色体位置与文件B中染色体位置的交集，以及对应的文件B中的染色体位置. $ cat A.bed chr1 10 20 chr1 30 40 $ cat B.bed chr1 15 25 $ bedtools intersect -a A.bed -b B.bed -wb chr1 15 20 chr1 15 25 案例四（经用）： 包含着染色体位置的两个文件，分别记为A文件和B文件。求对于A文件的染色体位置是否与文件B中的染色体位置有交集。如果有交集，分别输入A文件的染色体位置和B文件的染色体位置；如果没有交集，输入A文件的染色体位置并以’. -1 -1’补齐文件。 $ cat A.bed chr1 10 20 chr1 30 40 $ cat B.bed chr1 15 25 $ bedtools intersect -a A.bed -b B.bed -loj chr1 10 20 chr1 15 25 chr1 30 40 . -1 -1 案例五： 包含着染色体位置的两个文件，分别记为A文件和B文件。对于A文件中染色体位置，如果和B文件中染色体位置有overlap,则输出在A文件中染色体位置和在B文件中染色体位置，以及overlap的长度. $ cat A.bed chr1 10 20 chr1 30 40 $ cat B.bed chr1 15 20 chr1 18 25 $ bedtools intersect -a A.bed -b B.bed -wo chr1 10 20 chr1 15 20 5 chr1 10 20 chr1 18 25 2 案例六： 包含着染色体位置的两个文件，分别记为A文件和B文件。对于A文件中染色体位置，如果和B文件中染色体位置有overlap,则输出在A文件中染色体位置和在B文件中染色体位置，以及overlap的长度；如果和B文件中染色体位置都没有overlap,则用’. -1-1’补齐文件 $ cat A.bed chr1 10 20 chr1 30 40 $ cat B.bed chr1 15 20 chr1 18 25 $ bedtools intersect -a A.bed -b B.bed -wao chr1 10 20 chr1 15 20 5 chr1 10 20 chr1 18 25 2 chr1 30 40 . -1 -1 案例七： 包含着染色体位置的两个文件，分别记为A文件和B文件。对于A文件中染色体位置，输出在A文件中染色体位置和有多少B文件染色体位置与之有overlap. $ cat A.bed chr1 10 20 chr1 30 40 $ cat B.bed chr1 15 20 chr1 18 25 $ bedtools intersect -a A.bed -b B.bed -c chr1 10 20 2 chr1 30 40 0 案例八(常用)： 包含着染色体位置的两个文件，分别记为A文件和B文件。对于A文件中染色体位置，输出在A文件中染色体位置和与B文件染色体位置至少有X%的overlap的记录。 $ cat A.bed chr1 100 200 $ cat B.bed chr1 130 201 chr1 180 220 $ bedtools intersect -a A.bed -b B.bed -f 0.50 -wa -wb chr1 100 200 chr1 130 201 (2)merge&emsp;&emsp;用于合并位于同一个bed/gff/vcf 文件中有overlap或者距离在一定范围内的相邻区间，距离可由参数(-d)定义。需要注意的是，做合并之前需要先对bed文件做根据染色体排序，可以用bedtoolssort命令实现。 图示 语法bedtools merge [OPTIONS] -i &lt;bed/gff/vcf&gt; 案例$ cat A.bed chr2 10 20 chr1 30 40 chr1 15 20 chr1 18 25 排序： sort -k1,1 -k2,2n A.bed &gt; A.sort.bed $ cat A.sort.bed chr1 15 20 chr1 18 25 chr1 30 40 chr2 10 20 案例一：取并集 bedtools merge -i A.sort.bed chr1 15 25 chr1 30 40 chr2 10 20 案例二：计算重叠区间的个数,-i 指定统计的列，-o指定操作5 bedtools merge -i exons.bed -c 1 -o count chr1 15 25 2 chr1 30 40 1 chr2 10 20 1 案例三：-d 两个独立区域间距小于（等于）该值时将被合并为一个区域；-o collapse显示合并了哪些标签 $ bedtools merge -i A.sort.bed -d 5 -c 1 -o count，collapse chr1 15 40 3 chr1,chr1,chr1 chr2 10 20 1 chr2 (3)complement：返回基因组非覆盖区（用途，比如多轮设计panel)图示 语法bedtools complement -i &lt;BED/GFF/VCF&gt; -g &lt;genome files&gt; (4)genomecov：染色体和全基因组覆盖度计算要求：单个输入bed文件（-i指定）和genome files；如果输入为bam(-ibam指定)文件，则不需要genome files 图示 语法bedtools genomecov [OPTIONS] -i &lt;bed/gff/vcf&gt; -g &lt;genome&gt; 案例$ cat ranges-cov-sorted.bed chr1 4 9 chr1 1 6 chr1 8 19 chr1 25 30 chr2 0 20 $ cat cov.txt （染色体及每条染色体总碱基数） chr1 30 chr2 20 bedtools genomecov -i ranges-cov-sorted.bed -g cov.txt chr1 0 7 30 0.233333 1 chr1 1 20 30 0.666667 chr1 2 3 30 0.1 chr2 1 20 20 1 2 genome 0 7 50 0.14 3 genome 1 40 50 0.8 genome 2 3 50 0.06 #name 覆盖次数 覆盖碱基数 总碱基数 覆盖度 #同时计算单染色体和全基因组覆盖度 ranges-cov.bed文件需提前排序sort -k1,1 ranges-cov.bed &gt; ranges-cov-sorted.bed -bg参数可得到每个碱基的覆盖度。 (5)coverage 计算染色体给定区间覆盖度，输入可以是 BAM 文件$ cat A.bed chr1 0 100 chr1 100 200 chr2 0 100 $ cat B.bed chr1 10 20 chr1 20 30 chr1 30 40 chr1 100 200 $ bedtools coverage -a A.bed -b B.bed chr1 0 100 3 30 100 0.3000000 chr1 100 200 1 100 100 1.0000000 chr2 0 100 0 0 100 0.0000000 #name 覆盖次数 覆盖碱基数 总碱基数 覆盖度 (6)getfasta：提取序列提取指定位置的 DNA 序列，也是很好用的一个功能，反向互补链也可以提，不用自己写脚本提了 语法bedtools getfasta [OPTIONS] -fi &lt;fasta&gt; -bed &lt;bed/gff/vcf&gt; -fo &lt;fasta&gt; 案例要求：基因组fasta文件（-fi指定）,提取区间BED/GTF/GFF/VCF文件(-bed指定),输出文件FASTA（-fo 指定） bedtools getfasta -fi Mus_musculus.GRCm38.75.dna_rm.toplevel_chr1.fa -bed mm_GRCm38_3kb_promoters.gtf -fo mm_GRCm38_3kb_promoters.fasta 扩展： 提取序列之samtools（速度较快） #首先建立fai索引文件（第一列为染色体名字，第二列为序列碱基数） samtools faidx Mus_musculus.GRCm38.75.dna.chromosome.8.fa #序列提取，多提取区间空格隔开 samtools faidx Mus_musculus.GRCm38.75.dna.chromosome.8.fa \ 8:123407082-123410744 8:123518835-123536649 &gt;8:123407082-123410744 GAGAAAAGCTCCCTTCTTCTCCAGAGTCCCGTCTACCCTGGCTTGGCGAGGGAAAGGAAC CAGACATATATCAGAGGCAAGTAACCAAGAAGTCTGGAGGTGTTGAGTTTAGGCATGTCT [...] &gt;8:123518835-123536649 TCTCGCGAGGATTTGAGAACCAGCACGGGATCTAGTCGGAGTTGCCAGGAGACCGCGCAG CCTCCTCTGACCAGCGCCCATCCCGGATTAGTGGAAGTGCTGGACTGCTGGCACCATGGT [...] (7)nuc: 计算GC含量即各碱基数语法bedtools nuc [OPTIONS] -fi &lt;fasta&gt; -bed &lt;bed/gff/vcf&gt; Options: -fi 输入FASTA文件 -bed 提取区间BED/GTF/GFF/VCF文件(-bed指定) 案例bedtools nuc -fi hg19.fa -bed CDS.bed 输出结果解释：在原bed文件每行结尾增加以下几列 Output format:The following information will be reported after each BED entry: 1) %AT content 2) %GC content 3) Number of As observed 4) Number of Cs observed 5) Number of Gs observed 6) Number of Ts observed 7) Number of Ns observed 8) Number of other bases observed 9) The length of the explored sequence/interval. 10) The seq. extracted from the FASTA file. (opt., if -seq is used) 11) The number of times a user&apos;s pattern was observed. (opt., if -pattern is used.) 高级用法Coverage analysis for targeted DNA capture RNA-seq coverage analysis 参考：（1）王球爸的博客： http://blog.sina.com.cn/s/blog_5d5f892a0102v665.html （2）生信人 https://www.wxzhi.com/archives/871/gk4yd3ujan57e0ft/ （3）hope http://tiramisutes.github.io/2016/03/18/bedtools.html]]></content>
      <categories>
        <category>NGS软件</category>
      </categories>
      <tags>
        <tag>NGS软件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[全基因组关联分析(GWAS)简介]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F18%2FGWAS-%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[GWAS介绍&emsp;&emsp;全基因组关联分析是对多个个体在全基因组范围的遗传变异多态性进行检测，获得基因型，进而将基因型与可观测的性状，即表型，进行群体水平的统计学分析，根据统计量或P值筛选出最有可能影响该性状的遗传变异。 分析流程材料选择，性状调查，基因分型，模型选择，关联结果，数据深度挖掘 GWAS分析选材原则 保证选取的样本具有足够的代表性； 样本中不能有明显的亚群分化（例如生殖隔离等），因为明显分化的群体会使得遗传背景的噪音较大； 建议选择几个比较重要且遗传力较高的表型性状作为研究的重点； 质量性状尽量为0、1二值性状，并且两类性状的样本数应当尽量相近； 数量性状尽量精确量化记录（如抗病性可以量化为发病率、死亡率、存活率、病斑数、病斑面积等，而不是简单的多级衡量），并使表型总体呈近似正态分布； 栽培植物可以进行多年多点多重复记录，多年多点的观测结果可以分别进行关联分析，多重复可以取平均值进行关联分析； 表型变异丰富、性状有明显的主效位点控制时样本量可以适当减小，推荐200个以上；表型差异较小，多基因控制时样本量应当增大，推荐500个以上。 自然群体GWAS的研究对象非严格遗传群体： 种质资源 半同胞家系，混合家系 MAGIC/NAM家系 多个F2/RIL/全同胞家系 高杂合类物种：F1群体 师兄你好！打扰百忙之中的你，学弟我甚感抱歉。我是南农的在读研究生，正在试图运用关联分析方法进行大豆各性状的QTL分析研究，很荣幸拜读了你发表在《TAG》和《作物学报》上的几篇文章，受益匪浅。 由于底子浅薄，我对关联分析方面的一些问题不能很透彻地理解。比如在群体材料方面，关联分析要求材料应为相互间无直接亲缘关系。请问这一要求是为了能获得更多更广泛的等位基因数目、获得更好的多态吗？ 这一要求是不是也与群体LD的强弱有关？然而即使材料间没有直接亲缘关系，但是当取材时如果这些材料出现产地位置的结构特征，而会使分析结果产生假阳性。为避免产生假阳性得用STRUCTURE对群体结构进行校正，请问这里所用的软件（STRUCTURE）和方法，与SA方法中对群体分层进行校正所用的相同吗？ 期待你的回信与帮助，谢谢！ 你好，感谢你对我研究的关注。 材料选择所要求的无直接亲缘关系，主要是为了避免等位变异在家族的传递不平衡，以及由于亲缘关系造成的群体的亚分等群体结构问题，因此获得更多更广泛的等位基因数目及多态不是问题的核心。 材料不管怎么选都会有假阴、假阳这样那样的问题，因此你所说的“产地位置的结构特征，而会使分析结果产生假阳性”不无可能，这 才使得人们考虑用统计方法做矫正；structure软件分析原理我的论文里有所涉及，如要深究可参考论文后给出的参考文献 Pritchard J K, Stephens M, Donnelly P. Inference of population structure using multilocus genotype data. Genetics, 2000, 155:945-959， 可以肯定的说它与SA方法中对群体分层进行校正所用的不相同。 模型选择 &emsp;&emsp;目前的 GWAS 多采用两个阶段的设计：首先采用覆盖整个基因组的高通量 SNP 分型芯片（如affy6.0）对一批样本进行扫描，然后筛选出最显著的 SNP（如 P&lt;10-7）供第二阶段进行扩大样本验证,第一步选的snp大多是tag snp。GWAS 两阶段研究设计减少了基因分型的工作量和花费，同时通过重复实验降低了研究的假阳性率。GWAS 的整体过程比较复杂，其大致流程如下： 经过处理的 DNA 样品与高通量的 SNP 分型芯片进行杂交； 通过特定的扫描仪对芯片进行扫描，将每个样品所有的 SNP 分型信息以数字形式储存于计算机中 对原始数据进行质控，检测分型样本和位点的得率（call rate） 、病例对照的匹配程度、人群结构的分层情况等； 对经过各种严格质控的数据进行关联分析； 根据关联分析结果，综合考虑基因功能多方面因素后，筛选出最有意义的一批 SNP 位点； 根据需要验证 SNP 的数量选择合适通量的基因分型技术在独立样本中进行验证； 合并分析 GWAS 两阶段数据。 关联分析的优点（1）不需要专门构建作图群体，自然群体或种质资源都可作为研究材料；（2）广泛的遗传材料可同时考察多个性状大多数QTL关联位点及其等位变异，不受传统的FBL的“两亲本范围”的限制；（3）自然群体经历了许多轮重组后，LD衰减，存在于很短的距离内，保证了定位的更高精确性 生物信息学中的连锁分析与关联分析有哪些区别和联系？ 连锁分析的定位依赖于家族中标记基因型与表现性的共分离，从而探知影响疾病风险的遗传位点 关联分析的定位方法则是通过探测在种群水平上，哪个基因型或等位基因位点与表现性有关，并且这种相关能够在家系样本或者随机个体间实现（GWAS） 关联分析与 QTL定位的区别 全基因组关联研究(GWAS)后的新热点？&emsp;&emsp;GWAS的目的是从常见基因突变(common variant，&gt;5％的人口所具有的突变)中来找到至病的变异。现在越来越多的人开始还疑这个昂贵方法的普适性。 因为到目前为止，没有一个真正成功地用这个方法找到至病的变异，只能找到一些似是而非的、有统计意义但没有什么用的突变。 现在，有些人认为应该找罕见基因突变（rare variant，0.1-1％的人口所具有的突变）中来找到至病的变异。对于这样的变异，GWAS的方法就无效了。这个提议的假定是常见基因突变致病可能性不大，只有罕见基因突变才能致病。Hall的文中提到普林斯顿大学教授David Botstein认为1.4亿美金的HapMap(人类基因组单体型图计划)是巨大的失败(magnificent failure)。 所以在可见的将来，随着下一代测序技术(Next generation sequencing)的普及，寻找常见基因突变的GWAS的死亡将催生寻找罕见基因突变的方法及其应用。一个新的热点即将涌现。 &emsp;&emsp;我不认为GWAS是一个完全的失败，对于有些疾病还是发现了一些治病基因的。Nature Genetics上有很多的例子。另外，HapMap我也不认为是一个彻底的失败。毕竟，很多信息具有重大的参考价值，比如Allele frequency，不做怎么知道呢？第三，rare variants究竟有多大的作用还是一个比较不明朗的命题。如果真的是只有rare variants才能导致疾病，那每个人的治病基因就都不一样了。我同意你的结论，next generation sequencing是技术发展的根本，但是我觉得除了rare variants之外，通过NGS找到novel variation，进而找到causal variation也会是一个重要的研究方向。 在这里面，bioinformatics（而不是statistician）的作用就会更为明显。 参考：（1）http://blog.sciencenet.cn/blog-472757-376473.html （2）http://blog.sina.com.cn/s/blog_4b55f4130100us1m.html （3）http://blog.sina.com.cn/s/blog_4b55f4130100ibtg.html]]></content>
      <categories>
        <category>GWAS</category>
      </categories>
      <tags>
        <tag>GWAS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux解压缩]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F16%2Flinux%E8%A7%A3%E5%8E%8B%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[总结下linux下各种压缩文件的解压方法及批量解压 首先要弄清两个概念：打包 和 压缩： 打包 是指将一大堆文件或目录变成一个总的文件； 压缩 则是将一个大的文件通过一些压缩算法变成一个小文件。 &emsp;&emsp;Linux中很多压缩程序只能针对一个文件进行压缩，这样当你想要压缩一大堆文件时，你得先将这一大堆文件先打成一个包（tar命令），然后再用压缩程序进行压缩（gzip bzip2命令）。 单个解压 *.tar 用 tar -xvf 解压 *.gz 用 gzip -d或者gunzip 解压 .tar.gz和.tgz 用 tar -xzf 解压 *.bz2 用 bzip2 -d或者用bunzip2 解压 *.tar.bz2用tar -xjf 解压 *.Z 用 uncompress 解压 *.tar.Z 用tar -xZf 解压 *.rar 用 unrar e解压 *.zip 用 unzip 解压 *.tar.xz 用 $xz -d *.tar.xz $tar -xvf *.tar *.tgz 用tar zxvf 解压 单个压缩文件 gzip File 压缩为.gz文件 bzip2 File 压缩为.bz2文件 zip [打包后的文件名] [打包的目录路径] 压缩为.zip文件 压缩一个或多个文件/文件夹格式：tar -options compressed_file files_to_be_compressed tar –zcvf 运行 + gzip 产生 a .tar.gz 文件 tar –jcvf 运行 tar + bzip2 产生 a .tbz2 文件 tar –cvf p运行 tar 产生 a .tar 文件 批量解压 方法一： for i in $(ls *.tar.gz);do tar xvf $i;done 方法二: for tarfile in *.tar.gz; do tar -xvf $tarfile; done 方法三: ls *.tar.gz|xargs -n1 tar xzvf &emsp;&emsp;xargs 是一条 Unix 和类 Unix 操作系统的常用命令；它的作用是将参数列表转换成小块分段传递给其他命令，以避免参数列表过长的问题。 例如： echo “1 2 3 4”|xargs -n11234echo “1 2 3 4”|xargs -n21 23 4 所以，加n1参数，则*.tar.gz会拆成每个tar.gz文件后，一个一个传给tar xzvf命令 方法四： find -maxdepth 1 -name “*.tar.gz”|xargs -i tar xvzf {} &emsp;&emsp;这条命令可解压当前目录下的所有gz文件，maxdepth表示搜索深度，1代表只搜索当前目录 补充使用tar -xvf *.tar.gz会出错，提示“Not found in archive”？ 通配符是shell解决的问题,如 tar -xvf *.tar.gz 实际上执行tar时，tar接收到的是 tar -xvf a.tar.gz b.tar.gz c.tar.gz … 如果当前目录跟本没有tar的东西，那么tar就收到’*.tar.gz’这个参数 与win不同，linux所有字符都可以作文件名，也即目录中不存在着 *.tar.gz这个文件 扩展同样可以利用for循环遍历操作某一类文件，比如批量建索引。 for i in `ls` do samtools faidx $i done]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NGS数据格式BED/GFF/GTF之介绍，比较，转换]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F15%2FNGS%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8FBED-GFF-GTF%E4%B9%8B%E4%BB%8B%E7%BB%8D-%E6%AF%94%E8%BE%83-%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;NGS数据格式BED/GFF/GTF之介绍，比较，转换 Ensemble与UCSC各种格式说明Ensemble各种格式说明： http://www.ensembl.org/info/website/upload/bed.html UCSC各种格式说明： http://genome.ucsc.edu/FAQ/FAQformat.html#format1 基因组坐标系统&emsp;&emsp;计算机和现实世界有一个差别，即计算机的计数是从0开始的（即0-based），而现实生活中，我们的计数是从1开始的（即1-based）。在生物学领域的不同场合，这两种计数交杂出现 两套不同理念的坐标系统——base coordinate system(BCS)和interbase coordinate system（ICS） 我们先来分析一下这样的两套系统在表述序列位置时的差别 类型 1-based 0-based 单个核苷酸 4-4: T 3-4: T 一组核苷酸 4-6: TAA 3-6: TAA deletion 4-4: T/- 3-4: T/- insertion（表示AGGTCGAAGT） 4-5: -/CG 4-4: -/CG &emsp;&emsp;如果说我们不管0-based和1-based这两种计数方式对应的坐标系统，都将其视为对序列第一个核苷酸的描述（即第一个核苷酸是算0还是算1），那么0-based的系统，其区间其实是一个左闭右开的区间，而1-based的系统则是一个闭区间。 1-based，正如前述，就是为了符合人的计数习惯。那UCSC为什么要采用0-based的interbase coordinate system系统？ICS主要的好处体现在下面的几个方面： 对于splicing sites等涉及两个核苷酸之间位置的情况，ICS提供了一个更好的呈现方案； 对于site的计算更加方便，如length=end-start; 对于坐标的转换，尤其是正负链坐标间的转换更加方便。（revStart = chromSize – oneEnd，revEnd = chromeSize – zeroStart）。 NCBI等组织采用的是和人日常习惯一致的1-based计数系统，即用数字指代核苷酸位置。但是UCSC发现使用0-based计数系统（用数字指代核苷酸间的位置） &emsp;&emsp;应用范畴：对于直接给所有用户提供直观访问服务的data portal（比如说NCBI、Ensembl、UCSC Genome Browser、mirBase）、常用的生物信息学软件（如BLAST）以及某些格式的文件（如GFF、SAM、VCF），采用的是1-based的计数方法。与之相反，UCSC的Table Browser以及它在数据库中存储的数据、BED/BAM等格式的文件，以及Chado、DAS2、dbSNP这样的data portal则采用的是0-based的计数方法。 单个格式介绍BED格式 注意： chromStart起始坐标第一个碱基为0 BED行名会展示在基因组浏览器中的bed行的左侧 UCSC中展示效果： 其实，你可以通过UCSC Genome Browser展示你的注释 tracks数据,并以可视化的方式与其他注释数据进行比较！！！ UCSC官方说明—-Displaying Your Own Annotations in the Genome Browser： http://genome.ucsc.edu/goldenPath/help/customTrack.html#EXAMPLE1 具体介绍&emsp;&emsp;个人的注释数据再上传48小时后将失效，除非你登陆并创建会话。 如何登陆Genome Browser并创建会话： http://genome.ucsc.edu/goldenPath/help/hgSessionHelp.html#CTs 当然，你也可以查看别人的定制注释tracks数据： http://genome.ucsc.edu/goldenPath/customTracks/custTracks.html Genome Browser annotation tracks是基于面向行格式的文件的，文件中每行展示一个track特征，注释文件包含三种类型的行：browser lines, track lines, and data lines。空行和其他以#开头的行将被忽略。 如何构建注释文件呢？ （1）数据格式 目前支持标准的GFF格式，以及为Human Genome Project 和 UCSC Genome Browser定制的数据格式，常见的如BED,BAM,VCF,MAF等。注意GFF和GTF文件必须以tab键分割，而不是以空格分隔。参考染色体必须以chrN的形式（区分大小写）。 （2）定义 Genome Browser展示特征的区间 例如：browser position chr22:20100000-20100900 没有的话，展示的tracks在基因组的位置可能不对 （3）定义注释track的一些属性 例如track的name, description, colors, initial display mode, use score等等 如何展示呢？ 例如注释数据如下： browser position chr22:1000-10000browser hide alltrack name=”BED track” description=”BED format custom track example” visibility=2 color=0,128,0 useScore=1chr22 1000 5000 itemA 960 + 1100 4700 0 2 1567,1488, 0,2512chr22 2000 7000 itemB 200 - 2200 6950 0 4 433,100,550,1500 0,500,2000,3500 网址： http://genome.ucsc.edu/cgi-bin/hgCustom &emsp;&emsp;当然，可视化展示并不是真正的目的，主要目的是与其他注释数据进行比较，从而发现某些科学问题。最上面是你的track(Custom Tracks),下面就是按照不同数据类型划分的注释数据，默认都是隐藏的（hide），你可以勾选相应你感兴趣的数据，选择（hide,dense,squish,pack,full）不同展现形式显示其他注释数据，然后refresh，进行可视化比较。 GFF格式参考： http://www.bbioo.com/lifesciences/40-112835-1.html GFF格式（General Feature Format）是一种简单的以tab键分割的，用于描述基因组特征的格式文件。有GFF1, GFF2，GFF3和GTF2四种版本。 GFF1 注意： start第一个碱基是从1开始计数的 frame：在feature是coding exon时，frame可以取值0-2，表示读码框的第一个碱基；如果不是coding exon,则用.表示 GFF1与其他GFF格式的最大区别在于所有相同的feature的行是聚集在一起的 扩展名为.gff1而不是gff GFF2http://gmod.org/wiki/GFF2 注意： start第一个碱基是从1开始计数的 frame：在feature是coding exon时，frame可以取值0-2，表示读码框的第一个碱基；如果不是coding exon,则用.表示 attribute：用;分隔键值对（tag-value），为每个feature提供额外的信息。同时attribute包含identifiers，可以用于联系各个特征。 扩展名为.gff2而不是gff GFF2格式的不足 &emsp;&emsp;GFF2的一个问题是它只能表现一个层次的嵌套功能。 当处理具有多个交替剪接的转录物的基因时存在问题。 GFF2无法处理基因→转录本→外显子的三级体系。 大多数人给出一系列的转录本，并给出类似的名称来表明它们来自相同的基因。 第二个限制是，虽然GFF2允许您创建两级层次结构，如转录本→外显子，但它没有层次结构方向的任何概念。 所以它不知道外显子是否是转录本的subfeature，反之亦然。 这意味着你必须使用“aggregators”来整理关系。 因此，GFF2格式已被弃用，转而支持GFF3格式。 但是，目前Genome Browser是不支持GFF3的，所有的GFF track都是基于Sanger’s GFF2 specification GFF3http://gmod.org/wiki/GFF3 注意： start第一个碱基是从1开始计数的 phase：在feature是coding exon时，phase可以取值0-2，表示读码框的第一个碱基；如果不是coding exon,则用.表示 attribute：用 “=” 分隔键值对（tag-value），为每个feature提供额外的信息。同时attribute包含identifiers，可以用于联系各个特征。 扩展名为.gff3而不是gff 与其他GFF版本格式最大区别在于： 第一行表明版本：##gff-version 3 第9列attributes关联了转录本，之前的GFF版本限制于低层次的feature(exon) key=value具有特殊的含义 ID - unique idenfifier for this feature. Parent - idenfier of parent feature. Name - used as the feature label in the feature map. GTFGTF是另外一种与GFF2十分相似的数据格式，有时称之为GFF2.5。GTF的前8列与GFF2完全一样，group列（attribute列）被扩展了，每个属性包含了一个 type/value pair，属性以;结束，初次之外，属性之间还有一个空格。 属性列（第9列）： gene_id value - A globally unique identifier for the genomic source of the sequence. transcript_id value - A globally unique identifier for the predicted transcript. 属性列例子(UCSC)： gene_id &quot;Em:U62317.C22.6.mRNA&quot;; transcript_id &quot;Em:U62317.C22.6.mRNA&quot;; exon_number 1 属性列例子(Ensemble)： gene_id &quot;ENSG00000223972&quot;; transcript_id &quot;ENST00000456328&quot;; gene_name &quot;DDX11L1&quot;; gene_sourc e &quot;havana&quot;; gene_biotype &quot;transcribed_unprocessed_pseudogene&quot;; transcript_name &quot;DDX11L1-002&quot;; transcript_source &quot;havana&quot;; GTF是不被 GMOD支持的，如果使用，可以将其转换为GFF3格式。 不同格式比较BED和GFF/GTF BED文件中起始坐标为0，结束坐标至少是1,； GFF中起始坐标是1而结束坐标至少是1 两者都是基因注释文件，gtf的一行表示一个exon，多行表示一个基因。而bed的一行表示一个基因。 GTF与GFF区别 简单来说：GFF 2 -&gt; GTF -&gt; GFF 3 G的定义不同 GFF全称为general feature format，这种格式主要是用来注释基因组。 GTF全称为gene transfer format，主要是用来对基因进行注释。 GTF是在GFF（GFF2）的基础上的一个改良，GTF的前8列信息与GFF是一样的，主要区别在第九列，GTF扩充很多GFF不具备的其他的信息 数据格式转换 file conversion script GTF to GFF3 gft2gff3 GTF to BED gtf2Bed.pl BED to GTF bedToGtf.sh 汇总，将各种格式转换为GFF格式的脚本。这些脚本分散在不同的软件包中，可以根据需要下载使用。 bioPerl script description search2gff This script will turn a protein Search report (BLASTP, FASTP, SSEARCH, AXT, WABA) into a GFF File. genbank2gff3.pl Genbank to gbrowse-friendly GFF3 gff2ps This script provides GFF to postscript handling. 如果你在win下安装了perl及bioperl直接在文件夹下搜索便可以找到上述脚本，可以直接使用 gbrowse script description ucsc_genes2gff Convert UCSC Genome Browser-format gene files into GFF files suitable for loading into gbrowse blast92gff3.pl BLAST tabular output (-m 9 or 8) conversion to GFF version 3 format, DAWGPAWS script description cnv_blast2gff.pl This program will translate a blast report for a single query sequence into the GFF format. Tandy software script description gff2aplot a program to visualize the alignment of two genomic sequences together with their annotations. From GFF-format input files it produces PostScript figures for that alignment. blat2gff Converts BLAT output files to GFF formatted files BioWiki中还有一篇，总结更多GFF工具的文章，请参看下面链接： http://biowiki.org/GffTools 参考（1）http://blog.sciencenet.cn/blog-981687-726831.html （2）http://blog.sciencenet.cn/blog-1509670-847310.html （3）https://www.yaolibio.com/2016/08/15/gene-coordinate-system/]]></content>
      <categories>
        <category>NGS</category>
      </categories>
      <tags>
        <tag>NGS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux神器之grep,awk操作gtf文件]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F14%2FLinux%E7%A5%9E%E5%99%A8%E4%B9%8Bgrep%2Cawk%E6%93%8D%E4%BD%9Cgtf%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Linux神器之grep,awk操作gtf文件 Linux神器之grep,awk操作gtf文件过滤#开头注释行grep -v ^# Homo_sapiens.GRCh38.89.chr.gtf |head -5 补充： -v:反向选择 过滤空行：grep -v ‘^$’ filename 过滤空行和以#开头的行: grep -vE ‘^#|^$’ filename -E表示“或”的关系。 提取并计数有多少类featuregrep -v ^# Homo_sapiens.GRCh38.89.chr.gtf |awk &apos;{print $3}&apos;| sort | uniq -c 结果： 710166 CDS 1193694 exon 143497 five_prime_utr 58174 gene 119 Selenocysteine 83231 start_codon 74952 stop_codon 136436 three_prime_utr 199167 transcript （第一列为数目，第二列为features） 筛选出特定行第一列为染色体1-22加X,Y的行 awk &apos;$1 ~ /[0-9]/ || $1 ~ /[X|Y]/&apos; Homo_sapiens.GRCh38.89.chr.gtf | tail -5 第三列为”gene”的行 awk &apos;$3==&quot;gene&quot;&apos; Homo_sapiens.GRCh38.89.chr.gtf |head -5 查看每条染色体多少个基因 awk &apos;$1 ~ /[0-9]/ || $1 ~ /[X|Y]/&apos; Homo_sapiens.GRCh38.89.chr.gtf | awk &apos;$3==&quot;gene&quot;&apos;| awk &apos;{print $1}&apos;| sort | uniq -c 结果： 5224 1 2208 10 3248 11 2952 12 1312 13 2214 14 2155 15 2509 16 3018 17 1174 18 2951 19 3971 2 1391 20 837 21 1339 22 3019 3 2504 4 2869 5 2860 6 2884 7 2367 8 2246 9 2366 X 519 Y （第一列为数目，第二列为染色体号） 先过滤出基因，然后按照chr顺序排序然后根据基因起始位置排序awk &apos;$1 ~ /[0-9]/ || $1 ~ /[X|Y]/ &amp;&amp; $3==&quot;gene&quot;&apos;Homo_sapiens.GRCh38.89.chr.gtf|sort -t $&apos;\t&apos; -k1,1n -k4,4n &gt;result_gene_sort.txt (因为根据asc码排序，所以X,Y会排在1-22之前) awk &apos;$1 ~ /[0-9]/ &amp;&amp; $3==&quot;gene&quot;&apos; Homo_sapiens.GRCh38.89.chr.gtf | sort -t $&apos;\t&apos; -k1,1n -k4,4n &gt;result_gene_sort.txt awk &apos;$1 ~ /[X|Y]/ &amp;&amp; $3==&quot;gene&quot;&apos; Homo_sapiens.GRCh38.89.chr.gtf |sort -t $&apos;\t&apos; -k1,1 -k4,4n &gt;&gt;result_gene_sort.txt sort 参数： -k 选择以哪个区间进行排序 -n 依照数值的大小排序 sort多字段的排序: -t $’\t’ 指定使用tab键分列 -k1,1n 指定以第一列按照数字（asc码）从大到小排序 -k4,4n 指定以第一列按照数字（asc码）从大到小排序 注意： （1）对于tab分隔符，还是得稍微注意一下。比如下面的命令： $sort -t &apos;\t&apos; -k3,3n a.txt &gt;a.sort sort: multi-character tab `\t’ (出错） 这个不能按照tab分隔符进行字段排序，换成如下方式就可以了 $ sort -t $&apos;\t&apos; -k3,3n a.txt&gt;a.sort （2）linux sort 命令能不能识别 科学计数法？例如 9.98E-9。使用-n得不到想要的结果，但可以使用-g 替代，它会将数字还原成一版格式再进行比较。 计算所有CDS的累积长度，其他类似cat Homo_sapiens.GRCh38.89.chr.gtf | awk &apos;$3 ==&quot;CDS&quot; { len=$5-$4 + 1; size += len; print &quot;Size:&quot;, size } &apos; （结果输出刷屏了！！加上个 |tail -1） 计算1号染色体cds的平均长度awk &apos;BEGIN {s = 0;line = 0 } ;$3 ==&quot;CDS&quot; &amp;&amp; $1 ==&quot;1&quot; { s += ($5 - $4);line += 1}; END {print &quot;mean=&quot; s/line}&apos; Homo_sapiens.GRCh38.89.chr.gtf 补充： awk的BEGIN和END 通常使用BEGIN来显示变量和预置（初始化）变量，使用END来输出最终结果。 总结一下awk基本结构为 : BEGIN{BEGIN操作,在输入文件之前执行} ; {文件行处理块,对来自输入文件datafile的每一行都要执行一遍} ; END{END操作,输入文件关闭后awk退出之前执行} 从gtf文件中分离提取基因名字$3 == &quot;gene&quot; { # 通过 ; 分离提取第9列 split($9, x, &quot;;&quot;) # 基因名字是第一个元素。 # 去除基因名字旁边的双引号 name = x[1] # 由于 &quot; 是一个特殊字符，我们必须写成 \&quot;;*反斜杠\表示转义符。 gsub(&quot;\&quot;&quot;, &quot;&quot;, name) # 打印特征类型、基因名字以及大小。 print name, $5 - $4 + 1} 最后，我们可以写成下边这条命令: cat result_gene_sort.txt |awk &apos;$3 == &quot;gene&quot;{split($10,x,&quot;;&quot;);name = x[1];gsub(&quot;\&quot;&quot;, &quot;&quot;, name);print name,$5-$4+1}&apos;|head -5 根据基因名列表提取gtf文件（1）shell脚本 ./sub_gtf.sh gene.txt Homo_sapiens.GRCh38.89.chr.gtf &gt;logfile 2&gt;&amp;1 #!/bin/bash set -u set -e set -o pipefail if [[ $# != 2 ]];then echo &quot;Parameter incorrect.&quot; exit 1 fi gene_file=$1 gtf_file=$2 gene=($(cat ${gene_file})) for gene in ${gene[@]}; do grep &quot;\&quot;${gene}\&quot;&quot; ${gtf_file} &gt;&gt;result.txt done （2）单行命令 gene=($(cat gene.txt));for gene in ${gene[@]}; do grep &quot;\&quot;${gene}\&quot;&quot; Homo_sapiens.GRCh38.89.chr.gtf &gt;&gt;result.txt;done 补充基本语法：&emsp;&emsp;awk擅长处理表格形式的数据。它逐行从文本中读取数据，将整行数据（record)定义为$0,然后根据指定的分隔符，将各列数据（record)分别定义为$1,$2,$3，然后使用如下结构处理数据： pattern1 {action1};pattern2 {action2};.... 注意： 如果没有定义pattern,则直接执行action； 如果没有提供action,则直接输出满足pattern的内容 参考：（1）http://www.jianshu.com/p/7af624409dcd （2）https://mp.weixin.qq.com/s/NZCt2SR3WmCnqpb2FGcbsQ]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何写shell脚本]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F13%2F%E5%A6%82%E4%BD%95%E5%86%99shell%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[shell代码风格规范及技巧 shell代码风格规范及技巧命名有标准文件名规范，以.sh结尾，方便识别 编码要统一&emsp;&emsp;在写脚本的时候尽量使用UTF-8编码，能够支持中文等一些奇奇怪怪的字符。不过虽然能写中文，但是在写注释以及打log的时候还是尽量英文，毕竟很多机器还是没有直接支持中文的，打出来可能会有乱码。 &emsp;&emsp;这里还尤其需要注意一点，就是当我们是在windows下用utf-8编码来写shell脚本的时候，一定要注意这个utf-8是否是有BOM的。默认情况下windows判断utf-8格式是通过在文件开头加上三个EF BB BF字节来判断的，但是在Linux中默认是无BOM的。因此如果我们是在windows下写脚本的时候，一定要注意将编码改成Utf-8无BOM，一般用notepad++之类的编辑器都能改。否则，在Linux下运行的时候就会识别到开头的三个字符，从而报一些无法识别命令的错。 开头有shebang&emsp;&emsp;所谓shebang其实就是在很多脚本的第一行出现的以”#!”开头的注释，他指明了当我们没有指定解释器的时候默认的解释器，一般可能是下面这样： #!/bin/bash 写出健壮Bash Shell脚本技巧set -x set -e set -u set -o pipeline （1）set -x会在执行每一行 shell 脚本时，把执行的内容输出来。它可以让你看到当前执行的情况，里面涉及的变量也会被替换成实际的值。 （2）在”set -e”之后出现的代码，一旦出现了返回值非零，整个脚本就会立即退出。 &emsp;&emsp;set -e结束程序的条件比较复杂，在man bash里面，足足用了一段话描述各种情景。大多数执行都会在出错时退出，除非 shell 命令位于以下情况： 一个 pipeline 的非结尾部分，比如 error | ok 一个组合语句的非结尾部分，比如 ok &amp;&amp; error || other 一连串语句的非结尾部分，比如 error; ok 位于判断语句内，包括test、if、while等等。 （3）set -u，当你使用未初始化的变量时，让bash自动退出 （4）set -o pipefail 设置了这个选项以后，包含管道命令的语句的返回值，会变成最后一个返回非零的管道命令的返回值。听起来比较绕，其实也很简单： 例如test.sh set -o pipefail ls ./a.txt |echo &quot;hi&quot; &gt;/dev/null echo $? &amp;&gt; 等如 2&gt;&amp;1 , &gt; 等如 1&gt; ,那是缩写 1 是 STDOUT, 2 是 STDERR, 2&gt;&amp;1 就是 STDOUT 和 STDERR，同导向到同一文件里 运行test.sh，因为当前目录并不存在a.txt文件，输出：ls: ./a.txt: No such file or directory1 # 设置了set -o pipefail，返回从右往左第一个非零返回值，即ls的返回值1 注释掉set -o pipefail 这一行，再次运行，输出：ls: ./a.txt: No such file or directory0 # 没有set -o pipefail，默认返回最后一个管道命令的返回值 工作路径我们会先获取当前脚本的路径，然后一这个路径为基准，去找其他的路径。 work_dir=$1 reference=${work_dir}/data/reference/TAIR10_chr_all.fas 环境变量PATH一般情况下我们会将一些重要的环境变量定义在开头，确保脚本中使用的命令能被bash搜索到。 PATH=/bin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/bin 运行脚本chmod +x ./test.sh #给脚本权限 ./test.sh #执行脚本 Shell中的变量 “=”前后不能有空格 定义时不用$,使用时需要$,且推荐给所有变量加上花括号{} 脚本的参数 先定义具体含义，后使用 gene_file=$1 ${gene_file} 代码有注释 简述某一代码段的功能 各个函数前的说明注释 太长要分行在调用某些程序的时候，参数可能会很长，这时候为了保证较好的阅读体验，我们可以用反斜杠来分行： ./configure \ –prefix=/usr \ –sbin-path=/usr/sbin/nginx \ –conf-path=/etc/nginx/nginx.conf \ 日志例如： ./sub_gtf.shell gene.txt Homo_sapiens.GRCh38.89.chr.gtf &gt;logfile 2&gt;&amp;1 1 ：表示stdout标准输出，系统默认值是1，所以”&gt;logfile”等同于”1&gt;logfile”2 ：表示stderr标准错误&amp; ：表示等同于的意思，2&gt;&amp;1，表示2的输出重定向等同于1 回显例如 if [[ $# != 2 ]];then echo &quot;Parameter incorrect.&quot; exit 1 fi 当执行： ./sub_gtf.shell gene.txt 因为参数数目不对，输出Parameter incorrect.至屏幕 当执行： ./sub_gtf.shell gene.txt &gt;logfile 2&gt;&amp;1 同样参数数目不对，但输出Parameter incorrect.至日志 函数相关巧用main函数,使得代码可读性更强#!/bin/bash func1(){ #do sth } func2(){ #do sth } main(){ func1 func2 } main &quot;$@&quot; 考虑作用域shell中默认的变量作用域都是全局的，比如下面的脚本： #!/usr/bin/env bash var=1 func(){ var=2 } func echo $var 他的输出结果就是2而不是1，这样显然不符合我们的编码习惯，很容易造成一些问题。 &emsp;&emsp;因此，相比直接使用全局变量，我们最好使用local, readonly这类的命令，其次我们可以使用declare来声明变量。这些方式都比使用全局方式定义要好。 local一般用于局部变量声明，多在在函数内部使用。（1）shell脚本中定义的变量是global的，其作用域从被定义的地方开始，到shell结束或被显示删除的地方为止。 （2）shell函数定义的变量默认是global的，其作用域从“函数被调用时执行变量定义的地方”开始，到shell结束或被显示删除处为止。函数定义的变量可以被显示定义成local的，其作用域局限于函数内。但请注意，函数的参数是local的。 （3）如果同名，Shell函数定义的local变量会屏蔽脚本定义的global变量。所以在函数内声明的变量，请务必记得加上 local 限定词 使用举例： #!/bin/bash function Hello() { local text=&quot;Hello World!!!&quot; #局部变量 echo $text } Hello 只读变量使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变。 例如： readonly myUrl myUrl=&quot;http://www.runoob.com&quot; declare -r 只读 (declare -r var1与readonly var1作用相同) declare -r var1 -i 整数 declare -i number -a 数组 declare -a indices -f 函数 declare -f 函数返回值&emsp;&emsp;在使用函数的时候一定要注意，shell中函数的返回值只能是整数，估计是因为一般情况下一个函数的返回值通常表示这个函数的运行状态，所以一般都是0或者是１就够了，因此就设计成了这样。不过，如果非得想传递字符串，也可以通过下面变通的方法: func(){ echo &quot;2333&quot; } res=$(func) echo &quot;This is from $res.&quot; 这样，通过echo或者print之类的就可以做到传一些额外参数的目的。 使用新写法&emsp;&emsp;这里的新写法不是指有多厉害，而是指我们可能更希望使用较新引入的一些语法，更多是偏向代码风格的，比如 尽量使用func(){}来定义函数，而不是func{} 尽量使用[[]]来代替[] 尽量使用$()将命令的结果赋给变量，而不是反引号 在复杂的场 下尽量使用printf代替echo进行回显 事实上，这些新写法很多功能都比旧的写法要强大，用的时候就知道了。 其他小tip 读取文件时不要使用for loop而要使用while read 简单的if尽量使用&amp;&amp; ||，写成单行。比如[[ x &gt; 2]] &amp;&amp; echo x 利用/dev/null过滤不友好或者无用的输出信息 例如 if grep ‘pattern1’ some.file &gt; /dev/null &amp;&amp; grep ‘pattern2’ some.file &gt; dev/null then echo “found ‘pattern1’ and ‘pattern2’ in some.file” fi /dev/null ：代表空设备文件 安装shellcheckShellCheck, a static analysis tool for shell scripts shellcheck 除了可以提醒语法问题以外，还能检查出 shell 脚本编写常见的 bad code。 使用方式（１）网页版： http://www.shellcheck.net github仓库： https://github.com/koalaman/shellcheck 下载安装： wget -q https://storage.googleapis.com/shellcheck/shellcheck-latest.linux.x86_64.tar.xz xz -d shellcheck-latest.linux.x86_64.tar.xz tar -xvf shellcheck-latest.linux.x86_64.tar echo ‘export PATH=/home/wangdong/softwares/shellcheck:$PATH’&gt;&gt;~/.bashrc source ~/.bashrc 使用方式（2）终端： shellcheck yourscipts Shell不能做什么 需要精密的运算的时候 需要语言效率很高的时候 需要一些网络操作的时候 总之Shell就是可以快速开发一个脚本简化开发流程，并不可以用来替代高级语言 &emsp;&emsp;解决特定的问题要用合适的工具。知道什么时候用 shell，什么时候切换到另外一门更通用的脚本语言（比如ruby/python/perl），这也是编写可靠 shell 脚本的诀窍。如果你的任务可以组合常见的命令来完成，而且只涉及简单的数据，那么 shell 脚本就是适合的锤子。如果你的任务包含较为复杂的逻辑，而且数据结构复杂，那么你需要用ruby/python之类的语言编写脚本。 参考：（1）https://blog.mythsman.com/2017/07/23/1/ （2）https://github.com/koalaman/shellcheck （3）https://segmentfault.com/a/1190000006900083]]></content>
      <categories>
        <category>shell脚本</category>
      </categories>
      <tags>
        <tag>shell脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计sanger测序引物，验证突变位点]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F12%2F%E8%AE%BE%E8%AE%A1sanger%E6%B5%8B%E5%BA%8F%E5%BC%95%E7%89%A9%EF%BC%8C%E9%AA%8C%E8%AF%81%E7%AA%81%E5%8F%98%E4%BD%8D%E7%82%B9%2F</url>
    <content type="text"><![CDATA[在进行Variation calling 分析之后的故事 &emsp;&emsp;在进行Variation calling 分析之后。首先需要进行过滤，一般基于frequency和function两方面进行突变的过滤。 frequency过滤：根据千人基因组测序项目的等位基因频率预测的0.5%作为阈值，小于这一值的认为是突变，因为千人基因组对于我们检测到的变异属于健康对照，若该变异在健康对照的频率过高，则是致病突变的可能性就比较低了。 function过滤：一般看exonic区的non-synonymous，frameshift deletion，frameshift insertion，nonframeshift deletion，nonframeshift insertion，nonsynonymous SNV，stopgain，synonymous SNV等。 &emsp;&emsp;接着需要使用IGV查看变异位点的可靠性，并没有具体的标准。 一般先看看变异位点是否在reads的两端，若是，可能是接头没去干净； 其次看看具有该变异位点的reads占总reads的占比，一般保留&gt;50%的； 最后看看变异位点两端是否”干净”; 另外，如果有家系的数据，可以对比着看看家庭其他成员在该位点是否有相同的变异，并推测可能的遗传方式。 &emsp;&emsp;当然，以上方法得到的变异只能是初步的结果，还必须通过一代测序进行突变真实性的验证，今天的正题就是如何设计sanger测序引物？ 获取指定区域DNA序列网址： http://hgsv.washington.edu/cgi-bin/hgc?hgsid=2655438_sG5Zu9tXr3MZHMAtoJMdHACABHm4&amp;o=8420409&amp;g=getDna&amp;i=mixed&amp;c=chr21&amp;l=8420409&amp;r=8420416&amp;db=hg19&amp;hgsid=2655438_sG5Zu9tXr3MZHMAtoJMdHACABHm4 贴入位点，如chr7:74009352-74009352 先在UCSC中输入位点前500，后500个碱基， 点击get DNA ,复制结果 Primer-BLAST 设计引物网址： https://www.ncbi.nlm.nih.gov/tools/primer-blast/ PCR product size 取300-500之间主要因为节省测序成本，同时，如果设计的引物没有中点在500附近的，主要可以通过调整PCR product size的上限，这样获得的产物长点就长点吧！ Get primers NCBI的Primer Blast 中这里选择的Genome(reference assembly from selected organisms)实际上是GRCh38版本。细心的可能注意到了，第一步获取序列用的是hg19版本的，但没关系，还有第三步UCSC In-Silico PCR验证呢！这一步选择的是hg19版本，这一步会进行blast!所以primer blast 和另两步的版本不同对结果不会有影响。 点击submit，之后比较慢，等等吧！（同时完成引物的设计和blast，主要是blast比较耗时！） 一般第一条结果比较好，但是如果给出的若干个引物中有一条的中心更趋近与500处（待检测位点位于中间有利于测序，因为一般测序结果两端50bp会出现杂峰），则最好选择这一条，翻查下面对应的引物的正反向序列。 UCSC In-Silico PCR验证引物网址： https://genome.ucsc.edu/cgi-bin/hgPcr?hgsid=603243225_C0DlViEzt0mvZEqK0DLtJx4pfsRN &emsp;&emsp;UCSC的PCR选项为电子PCR，输入引物（&gt;15bp），即可得到两引物间序列。UCSC是基于基因组而非转录组，如果两引物间隔很大，则先调节Max Product Size。UCSC自动预测的TM值是基于primer3的，跟我们用的DNAman算出来的值也比较接近，PCR时可直接使用其退火温度。 用法：打开UCSC中的in silicon PCR，将上下游引物分别输入，可以选择物种，基因组，产物长度等，submit即可 Submit后，如果结果只对应处一条染色体上的一个位点，且primer melting温度控制在2度左右差范围即可，如下 整理结果：]]></content>
      <categories>
        <category>NGS</category>
      </categories>
      <tags>
        <tag>NGS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基因组数据下载]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F12%2F%E5%8F%82%E8%80%83%E5%9F%BA%E5%9B%A0%E7%BB%84%E6%95%B0%E6%8D%AE%E4%B8%8B%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;常用到的基因组数据格式括:fasta,fastq,gff,GenBank format, EMBL format；常用的基因组数据库包括：（1）Ensembl基因组注释数据库；（2）UCUS基因组浏览器 &emsp;&emsp;常用到的基因组数据格式括:fasta,fastq,gff,GenBank format, EMBL format；常用的基因组数据库包括：（1）Ensembl基因组注释数据库；（2）UCUS基因组浏览器 （1）通过Ensembl基因组注释数据库下载网址：http://www.ensembl.org/info/data/ftp/index.html 下载数据前一定要仔细查看相应目录下的README文件 上述网页链接都是最新版的，并且会随着基因组版本的更新而更新，历史版本可以在以下网址查询并下载：http://www.gencodegenes.org/releases/ 可以找到Ensembl来源的各个版本的基因注释GTF或者GFF3文件 其FTP site地址为：ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human Freeze date GENCODE release Ensembl release Release date Genome assembly version 01.2017 27 90 08.2017 GRCh38.p10 03.2016 25 85, 86, 87 07.2016 GRCh38.p7 基因组序列下载:&emsp;&emsp;Ensembl提供的参考基因组有2种组装形式和3种重复序列处理方式, 分别是primary, toplevel和unmasked (dna)、soft-masked (dna_sm)和masked (dna_rm)。一般选择dna.primary或dna_sm.primary。 为什么选择Primary Primary assembly contains all toplevel sequence regions excluding haplotypes and patches. This file is best used for performing sequence similarity searcheswhere patch and haplotype sequences would confuse analysis. 网页版的也为Primary Assembly版本的，例如Homo sapiens chromosome 7, GRCh38.p7 Primary Assembly 为什么不选择masked &emsp;&emsp;Masked基因组是指所有重复区和低复杂区被N代替的基因组序列，比对时就不会有reads比对到这些区域。一般不推荐用masked的基因组，因为它造成了信息的丢失，由此带来的一个问题是uniquely比对到masked基因组上的reads实际上可能不是unique的。而且masked基因组还会带来比对错误，使得在允许错配的情况下，本来来自重复区的reads比对到基因组的其它位置。 另外检测重复区和低复杂区的软件不可能是完美的，这就造成遮盖住的重复序列和低复杂区并不一定是100%准确和敏感的。 &emsp;&emsp;soft-masked基因组是指把所有重复区和低复杂区的序列用小写字母标出的基因组，由于主要的比对软件，比如BWA、bowtie2等都忽略这些soft-mask，直接把小写字母当做大写字母比对，所以使用soft-masked基因组的比对效果和使用unmasked基因组的比对效果是相同的。 (1)文件命名规则： \.\.\.\.\.fa.gz species:物种的名称 assembly:基因组的版本 sequence type（主要有三类）: ‘dna’ - unmasked genomic DNA sequences. ‘dna_rm’ - masked genomic DNA.通过RepeatMasker软件 检测弥散的重复序列和低复杂度的区域，并将重复序列使用N替代。 ‘dna_sm’ - soft-masked genomic DNA. 指Soft-masked的DNA序列，其中的重复序列和低复杂度的区域会用其相应碱基的小写字母来表示 举个栗子（1）连续的N &gt;Homo_sapiens.GRCh38.dna_rm.chromosome.15.fa NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNTTAGAGACCTTGAGA GGAATTAAACATCTCTGTGAGTATATGCTGTAGGGCTTTGCTGCACTGTCCTTGGAGGCT （2）小写字母表示碱基 &gt;Homo_sapiens.GRCh38.dna_sm.chromosome.X.fa nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnctaaccctaaccctaaccct aaccctaaccctaaccctctgaaagtggacctatcagcaggatgtgggtgggagcagatt gacaacCCCTAGAAGAGCACCTGGTTGAtaacccagttcccatctgggatttaggggacc aggggccctgggacagccctgtacatgagctcctggtctgtaacacagttcccctgtggg gatttagggACTTGGGCCTTCTGTCTTTGGGATCTACTCTCTATGGGCCACACAGATATG &emsp;&emsp;所以，使用RepeatMasker和Tandem Repeats Finder (with period of 12 or less)识别的重复在dna_rm中用大写的N表示，而在dna_sm中用小写字母表示，而非重复序列用大写字母表示。至于序列的开头和结尾，无论是dna_rm还是dna_sm，N/n表示（A,C,G,T）任意一种，也叫做Gaps，它意味着真实具体是哪一种碱基，测序平台不能确定，N/n的数目可能是不确定碱基数,都被一并maked掉了，在序列分析的时候并不会分析。 为什么要这样做呢？&emsp;&emsp;RepeatMasker是一款基于Library-based，通过相似性比对来识别重复序列，可以屏蔽序列中转座子重复序列和低复杂度序列（默认将其替换成N），几乎用于所有物种，是做基因组、非编码RNA的必备软件。在人类基因组分析当中，大约 56% 的序列会被mask； RepeatMasker在进行序列比对时可以选用常见的几种算法，包括nhmmer、cross_match、ABBlast/WUBlast、RMBlast 、Decypher（可以安装多个比对引擎，但每次只能使用其中一个）。 &emsp;&emsp;RepeatMasker应该是做已知repeat和TE的。基本原理是用已知repeats去blast。所以潜在的未知重复序列应该是无法用repeatmasker找到的，而在mRNA中的repeat序列也不应该用RepeatMasker找。repeatmasker的实际意义，就象这个软件的名字一样，是为了mask掉repeat。从而在查找基因，鉴定有功能的ncRNA，或者设计引物等提供一个精简的序列。毕竟用带有大量TE和repeat的序列做引物设计或者基因和蛋白功能分析，最后验证的时候只是得到了一批repeat 重复序列的种类： Tandem repeats 串连重复 Satellite DNA 卫星DNA Variable number tandem repeat /Minisatellite 小卫星 Short tandem repeat（STR）/Microsatellite (Trinucleotide repeat disorders)微卫星 Interspersed repeats 散落重复 Transposon (Transposable elements (TEs) )转座子 Retrotransposon 反转录转座子 SINEs – Alu sequence, MIR 短散落元件 LINEs – LINE1, LINE2 长散落元件 LTRs – HERV, MER4, retroposon 长末端重复 DNA transposon DNA转座子 MER1, MER2, Mariners TIR（Terminal Inverted Repeat） 末端方向重复 Genomic island Genomic island &emsp;&emsp;这种对RepeatMasker 和 Tandem Repeats Finder用小写表示的方式，可以用于UCSC的Genome Browser来展示重复序列。 id type: 可选值包括chromosome,nonchromosomal,seqlevel。 （1）chromosome就不用说了； （2）nonchromosal中包含了暂时未能确定染色体的序列； （3）seqlevel包括sequence scaffolds, chunks 或者clones三个层次。 ‘scaffold’：通过短的测序reads拼接组装成更大的序列—-contigs（通常来自全基因组鸟枪测序，WGS），但是还不能组成到染色体的程度。通常还需要更多的测序，来消除gaps并确定顺序（tiling path） ‘chunk’:当contigs序列能被组装成大的实体时，有时候必须认为的将其打断为更小的实体，称之为’chunks’。这是由于注释管道的限制并且受存储序列和注释信息的MySQL数据库的记录限制。 ‘clone’ ：通常这是最小的序列实体。它经常与一个BAC clone或者部分区域的序列是一样的。 id:实际序列的标示符，和\对应 fa:表示文件格式为FASTA gz:文件压缩方式为GUN Zip toplevel文件中包含了所有的在ensembl数据库schema中被定义为toplevel的序列，包括染色体，未能组装到染色体上的区域以及含有N的haplotype/patch区域。 primary_assembly文件相比于toplevel文件，减去了含有N的haplotype/patch区域。这类文件比较适合用来比对。 除基因组注释文件（GTF或者GFF）下载&emsp;&emsp;例如在RNA-seq分析流程中，参考基因组序列用于reads的比对，而GTF或者GFF用于确定比对上的reads是否落在基因内，由此来相对定量基因的表达量，鉴定差异表达基因。 (1)文件命名规则：\.\.\.gtf.gz 注释基因生物证据的比对（例如蛋白，cdna，RNA-seq）来组装基因组 对于预测的基因集：\.\.\.abinitio.gtf.gz 这里的预测是指通过GenScan和其他的从头预测工具分析产生的基因。 确定了需要下载什么后，那如何下载呢？可以通过浏览器下载，但需要再上传到服务器；也可以通过之前介绍的多线程下载工具axel下载，速度很快。当然还可以使用ensembl提供的rsync工具下载。 （2）通过UCSC下载基因组注释数据 Genome Browser’s “Table Browser”： http://genome.ucsc.edu/cgi-bin/hgTables?command=start Bulk Downloads page： http://hgdownload.cse.ucsc.edu/downloads.html &emsp;&emsp;对于NGS的序列分析来说，如果没有reference genome和genome annotation下游的分析可能根本无法进行。UCSC提供的数据分为sequence和annotation两大类，对于人、小鼠在内的71种脊椎动物，可以从UCSC同时下载sequence和annotation，其他的一般只有sequence。 基因组的版本： &emsp;&emsp;不同的生物信息学数据库对于基因组的命名方式各不相同。以人为例，NCBI/ENSEMBL用GRCh系列命名，而UCSC则使用hg系列命名。这两套命名系统背后的版本对应关系如下： UCSC NCBI hg18 GRCh36 hg19 GRCh37 hg38 GRCh38 hg系列和GRCh系列主要的差别有两处： （1）hg系列的染色体命名是”chr”+染色体号，而GRCh系列的染色体没有前缀的”chr”； （2）hg系列序列是0-based（第一个核苷酸记0），GRCh系列是1-based（第一个核苷酸记1，两种计数方法的区别参见《基因组的坐标系统》）。 （1）从FTP站点获取 获取sequence UCSC的reference genome是分染色体保存的。 对于hg38来说，单个的染色体序列可以在以下网址下载： http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/ 所有染色体打包好的文件在以下网址下载： http://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.chromFa.tar.gz 如果需要下载其他版本（19/18），可以把上述链接中的hg38换成hg19/hg18。如果需要将所有的染色体序列合并到一个文件中，可以在下载完成后依次执行下列命令（POSIX compatible）： tar –xzvf hg38.chromFa.tar.gz cd hg38 cat *.fa &amp;gt; hg38.fa 获取注释 UCSC提供了SNP、RepeatMask、refSeq、GENCODE等注释文件。 但是在UCSC的FTP站点： http://hgdownload.soe.ucsc.edu/goldenPath/hg38/database 这些数据被分成了两个文件——一个是.sql结尾的SQL语句文件，描述了数据表的结构和创建数据表的方法；另一个是.txt.gz结尾的数据文件。我们可以通过.sql文件来查看表的结构，再把.txt.gz格式的文件解压后转换成所需要的格式。下面是将.txt.gz的数据文件转换成gtf格式的三个例子： RepeatMask gunzip rmsk.txt.gz gawk &apos;OFS=&quot;\t&quot;{print $6,&quot;rmsk &quot;,$12,$7+1,$8,&quot;.&quot;,$10,&quot;.&quot;,&quot; repName \&quot;&quot;$11&quot;\&quot;; repFamily \&quot;&quot;$13&quot;\&quot;;&quot;}&apos; rmsk.txt &amp;gt; rmsk.gtf simpleRepeat gunzip simpleRepeat.txt.gz gawk &apos;OFS=&quot;\t&quot;{print $2,&quot; simpleRepeat&quot;,&quot;trf&quot;,$3+1,$4,&quot;.&quot;,&quot;+&quot;,&quot;.&quot;,&quot;name \&quot;&quot;$5&quot;\&quot;; sequence \&quot;&quot;$17&quot;\&quot;;&quot;}&apos; simpleRepeat.txt &amp;gt; simpleRepeat.gtf RefSeq &emsp;&emsp;UCSC为RefSeq和GENCODE等以genePred形式保存的注释文件提供了专门的格式转换程序——genePredToBed、genePredToGenePred、genePredToFakePsl、genePredToGtf、genePredToMafFrames 各自的使用方法参见： http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/FOOTER 若需将RefSeq转存为gtf格式，可参考下列命令： gunzip refGene.txt.gz cut -f 2- refGene.txt | genePredToGtf -utr -source=hg38 file stdin refGene.gtf （2）从Table Browser获取针对annotation，UCSC还通过Table Browser页面 http://genome.ucsc.edu/cgi-bin/hgTables 提供了一个更加友好的获取方法。Table Browser的使用基本使用方法可以参考 http://genome.ucsc.edu/goldenPath/help/hgTablesHelp.html#GettingStarted 为了避免和官方帮助文档的重叠，我仅在这里分享在做REP项目过程中发现的几种比较tricky的用法。 （1）获取GENCODE转录本ID和Gene Symbol的映射 clade设置为”Mammal”, genome设置为”Human”，assembly设置为”Dec. 2013 (GRCh38/hg38)”； 将group设置为”Genes and Gene Predictions”，track设置为”All GENCODE V24”； Table设置为”Basic (wgEncodeGencodeBasicV24)”； Output format设置为”selected fields from primary and related tables”； 点击”get output”； 在新页面中勾选”name”和”name2”前的复选框； 点击”get output”即可； （2）获取5’UTR/3’ UTR/Coding Exons/Intron的BED文件 clade设置为”Mammal”, genome设置为”Human”，assembly设置为”Dec. 2013 (GRCh38/hg38)”； 将group设置为”Genes and Gene Predictions”，track设置为”All GENCODE V24”； Table设置为”Basic (wgEncodeGencodeBasicV24)”； Output format设置为”BED – browser extensible data”； 点击”get output”； 在新页面中勾选自己需要的elements； 点击”get BED”即可。 （3）获取指定范围的序列 UCSC可以通过使用符合自己需要的注释数据，然后再获取进一步的数据。操作方法如下： 点击Table Browser的”add custom tracks”按钮； 在”Paste URLs or data”中添加数据的地址或者使用旁边的文件上传框上传文件，点击”Submit”； 在新页面中选择view in“Table Brower”，点击”go”，这时会跳回Table Browser； 这个时候将region选为“genome”，将Output format设置成”sequence”，结果可以选择“plain text”呈现或者“gzip compressed”下载，点击”get output”即可获取指定范围内的序列。 （4）Table Browser使用过程中可能会遇到的问题 由于抽取数据脚本执行超时（&gt;10min）或者下载地的网络不佳，下载下来的文件可能会不完整； 获取3’ UTR等序列时，若直接将track指定为系统自带的annotation，会有概率出现序列的start与annotation中不符的情况（0-based和1-based的杂合），建议先下载BED文件，然后通过前述的“获取指定范围的序列”来下载序列。 参考：（1）为什么序列分析要repeatmasker： http://www.dxy.cn/bbs/topic/9424163 （2）博耘生物： http://boyun.sh.cn/bio/?p=1845 （3）linux进进阶屋： http://sookk8.blog.51cto.com/455855/328076/ （4）BioDog的博客： https://www.yaolibio.com/2016/09/01/retrieve-genome-data-from-ucsc/ （5）6有才 http://www.jianshu.com/p/542c78a8ee0a （6）生信宝典 https://mp.weixin.qq.com/s/2OoXy4f1t0hE8OUqsAt1kw]]></content>
      <categories>
        <category>NGS</category>
      </categories>
      <tags>
        <tag>NGS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux多线程下载工具axel编译安装]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F06%2FLinux%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8B%E8%BD%BD%E5%B7%A5%E5%85%B7axel%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;Axel插件是基于yum下的一个多线程下载插件。axel插件也可以当独立下载工具来使用。当成独立下载工具使用时，适用于所有Linux发行版。通过打开多个HTTP/FTP连接来将一个文件进行分段下载，从而达到加速下载的目的。对于下载大文件，该工具特别有用。同时支持断点续传，速度通常情况下是Wget的几倍。可用于CentOS、RHEL、Fedora等使用yum的Linux发行版。由于没有管理员权限，只能编译安装源码。使用Axel可以在低速网络环境里提高数倍的下载速度。 Linux多线程下载工具axel编译安装&emsp;&emsp;Axel插件是基于yum下的一个多线程下载插件。axel插件也可以当独立下载工具来使用。当成独立下载工具使用时，适用于所有Linux发行版。通过打开多个HTTP/FTP连接来将一个文件进行分段下载，从而达到加速下载的目的。对于下载大文件，该工具特别有用。同时支持断点续传，速度通常情况下是Wget的几倍。可用于CentOS、RHEL、Fedora等使用yum的Linux发行版。由于没有管理员权限，只能编译安装源码。使用Axel可以在低速网络环境里提高数倍的下载速度。 官方主页:http://axel.alioth.debian.org/ 下载安装下载： wget https://sourceforge.net/projects/axel2/files/axel-2.4/axel-2.4.tar.gz tar -xzf axel-2.4.tar.gz cd axel-2.4 编译安装： ./configure --prefix=/home/u641750/axel-2.4 make &amp;&amp; make install 添加环境变量： echo &apos;export PATH=/home/u641750/axel-2.4/bin:$PATH&apos;&gt;&gt;~/.bashrc source ~/.bashrc 使用参数如下： –max-speed=x , -s x 最高速度x，指定每秒的最大比特数 –num-connections=x , -n x 指定线程数 –output=f , -o f 指定另存为目录f –search[=x] , -S [x] 搜索镜像 –header=x , -H x 添加头文件字符串x（指定 HTTP header） –user-agent=x , -U x 设置用户代理（指定 HTTP user agent –no-proxy ， -N 不使用代理服务器 –quiet ， -q 静默模式 –verbose ，-v 更多状态信息 –alternate ， -a Alternate progress indicator –help ，-h 帮助 –version ，-V 版本信息 测试例如下载Python安装包：time axel -n 10 http://mirrors.sohu.com/python/3.4.1/Python-3.4.1.tar.xz time wget http://mirrors.sohu.com/python/3.4.1/Python-3.4.1.tar.xz 如果下载过程中下载中断可以再执行下载命令即可恢复上次的下载进度 当然，linux也还有其他的多线程下载工具，比如myget。有人测试，axel、myget支持多线程，且速度较快都在2M。断点续传对比，三个工具都支持，但wget需要增加-c参数，axel、myget再次执行命令即可。 例如： wget -c http://url/iso/Centos/x86_64/CentOS-6.4-x86_64-bin-DVD1.iso 参考：（1）http://www.ha97.com/621.html （2）http://man.linuxde.net/axel （3）Dreamway的运维点滴（推荐阅读） http://dreamway.blog.51cto.com/1281816/1151886/]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SRA数据加速下载打包解决]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F06%2FSRA%E6%A0%BC%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%8A%A0%E9%80%9F%E4%B8%8B%E8%BD%BD%E6%89%93%E5%8C%85%E8%A7%A3%E5%86%B3%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;NCBI维护Short Read Archive (SRA)作为大规模平行测序（NGS）项目产生的数据仓库。这些方法在单个循环中能产生数百万碱基至千兆级碱基的数据，是标准Sanger测序仪输出的数百万倍。运用这些技术，包括新基因组的测序，捕获基因组区域测序，多个个体的完整基因组测序来寻找变异，转录组测序来研究样本可变剪切变异和表达水平，环境样本和其他宏基因组，染色质DNA结合蛋白分析等。SRA数据库可以用于搜索和展示SRA项目数据，包括SRA主页和 Entrez system。 &emsp;&emsp;NCBI维护Short Read Archive (SRA)作为大规模平行测序（NGS）项目产生的数据仓库。这些方法在单个循环中能产生数百万碱基至千兆级碱基的数据，是标准Sanger测序仪输出的数百万倍。运用这些技术，包括新基因组的测序，捕获基因组区域测序，多个个体的完整基因组测序来寻找变异，转录组测序来研究样本可变剪切变异和表达水平，环境样本和其他宏基因组，染色质DNA结合蛋白分析等。SRA数据库可以用于搜索和展示SRA项目数据，包括SRA主页和 Entrez system。 SRA下载方式：（1）Aspera（2）sratoolkit（3）FTP 比较：通过sratoolkit，可以直接下载成fastq格式，速度比ftp快，比aspera慢。 Aspera关于速铂Aspera &emsp;&emsp;速铂Aspera是一套商业的高速文件传输解决方案，随着高通量数据的大量产生，从而对于大文件快速传输的需求，开始应用到生物领域，目前NCBI、EBI的SRA库都提供这样的服务。 &emsp;&emsp;传统的FTP、HTTP等数据传输协议都是基于TCP的，TCP在远距离数据传输中存在一些先天的不足，文件越大、距离越远，其丢包、延时等问题对于传输速度的影响就越大。 Aspera使用的两种方式（1）客户端的下载与安装 &emsp;&emsp;即便Aspera是商业软件，但是作为客户应用方（相对于NCBI），我们使用其客户对进行数据的上传与下载是不用支付费用的。 &emsp;&emsp;网页下载：速度很快，不过需要把数据再上传到服务器上，多费一道工序下载网页版AsperaConnectML-3.5.2.97180.msi安装，网页上下载SRA数据时点aspera下载链接就可以。 客户端下载链接： http://downloads.asperasoft.com/connect2/ 设置下载目录及速度限制等： 至此，客户端工具准备妥当了~ （2）使用ascp下载SRA数据：ascp是Aspera Connect的命令行程序。 下载与安装（不需要root或者sudo权限）： curl -O http://download.asperasoft.com/download/sw/connect/3.6.1/aspera-connect-3.6.1.110647-linux-64.tar.gz tar zxf asper-commect-3.6.1.110647-linux.tar.gz sh aspera-connect-2.4.7.37118-linux-64.sh 添加环境变量： export PATH=&quot;/home/u641750/.aspera/connect/bin:$PATH&quot; 可以将密钥备份到/home/的家目录下方便使用（后文将用到）: $ cp ~/.aspera/connect/etc/asperaweb_id_dsa.openssh ~/ 至此，命令行工具准备妥当了~ 那么如何找到我们要下载的SRA数据呢？首先我们需要了解下NSBI的SRA数据结构的层次关系： NCBI官网说明：https://www.ncbi.nlm.nih.gov/books/NBK7522/ &emsp;&emsp;SRA数据库中的数据分为Studies, Experiments, Samples和相应的Runs四个层次。Studies有一个总体目标并可能包含数个Experiments。一个Experiments描述具体测了什么和使用的方法。它包括DNA来源信息，样本，测序平台，数据处理。每个Experiments由一个或者多个Runs组成。一个Run包含来自每个spot的reads结果。在未来，一些数据将具有相关分析。这些分析可能包括short reads组装为基因组或者转录组的contigs，现有基因组的比对，SRA数据的比对。每个水平的记录具有唯一的accession identifiers ，并且具有三个大写字母前缀： &emsp;&emsp;NCBI中SRA数据结构的层次关系： Studies: SRA Study accessions (prefixes SRP, DRP, ERP) Examples: SRP000002, DRP000617, ERP002000 BioProject accessions (prefixes PRJNA, PRJDB, PRJEB) Examples: PRJNA111397, PRJDB90, PRJEB1976 dbGaP study accessions (prefix phs) Example: phs000159 GEO Study (prefix GSE) Example: GSE12578 Samples: SRA Sample accessions (prefixes SRS, DRS, ERS) Examples: SRS000013, DRS000020, ERS000016 BioSample accessions (prefixes SAMN, SAME) Examples: SAMN00000013, SAMEA774460 GEO Sample (prefix GSM) Example: GSM769008 SRA Experiment(s) SRA Experiment accessions (prefixes SRX, DRX, ERX) Example: SRX000002,SRX000003,SRX000004 Figure 2 shows Study (SRP000095, top panel), Experiment (SRX000113, middle panel, and SRX000114), and Run (SRR000416, bottom panel) records for the 454 sequencing of James Watson’s genome by Cold Spring Harbor Laboratory. Study and Run records are displayed in the SRA browser. The corresponding Experiment records are displayed in the NCBI Entrez system as described in the next section. 在SRA浏览页面和Entrez可以搜索和查看SRA数据 Studies, Runs和它们相关的Samples可以通过SRA主页浏览和查看： www.ncbi.nlm.nih.gov/Traces/sra Experiment记录可以通过搜索 Entrez SRA数据库获得： www.ncbi.nlm.nih.gov/sites/entrez?db=sra 接下来具体介绍：搜索地址：https://www.ncbi.nlm.nih.gov/Traces/study/ （1）通过以上网址，查询得到SRA数据的SRA Experiment accessions (prefixes SRX, DRX, ERX) （2）在NCBI搜索SRA Experiment accessions，例如：SRX000004 点击Download data： 此时鼠标选中SRX实验或者任意一个SRR记录，通过Aspera client客户端下载。但这样需要等下载完再使用FTP上传到自己的服务器，前面提过，FTP速度很慢！！！ 那么如何在服务器使用ascp命令行工具下载呢？命令格式： ascp -i &lt;path-to-asperaweb_id_dsa.openssh&gt; -k1 -QTr –l200m anonftp@ftp-private.ncbi.nlm.nih.gov:/&lt;files to transfer&gt; &lt;local destination&gt; 相关的参数 –Q (for adaptive flow control) – needed for disk throttling! –T to disable encryption –k1 enable resume of failed transfers –l (maximum bandwidth of request, try 200M and go up from there) –r recursive copy –i &lt;密钥文件&gt; 表明下载存放路径，一定要有，缺少会报错！！！关键是如何获取，也就是你要下载的SRR数据的地址，并且一定要保证其存在，否则会报错！！！ 将鼠标选中上图任意一个SRR文件，例如SRR00006.sra,右键，复制链接地址：fasp://anonftp@ftp.ncbi.nlm.nih.gov:22/sra/sra-instant/reads/ByExp/sra/SRX/SRX000/SRX000004/SRR000006/SRR000006.sra?auth=no&amp;port=33001&amp;bwcap=300000&amp;targetrate=100p&amp;policy=fair&amp;enc=none&amp;lockpolicy=no&amp;locktargetrate=no&amp;lockminrate=no&amp;v=2 摘取/sra/sra-instant/reads/ByExp/sra/SRX/SRX000/SRX000004/SRR000006/SRR000006.sra部分即为 网上有其他教程说遵循如/sra/sra-instant/reads/ByRun/sra/SRR/SRR689/SRR689250/SRR689250.sra固定的格式，但实际并非如此，比如上面的例子，所以并不能图省事硬套上述格式，还是要再win下如上述方法找到文件具体的地址，摘取部分，以确保文件存在，否则会报错：“ascp: no remote host specified, exiting” 举个栗子： （1）单个文件下载： ascp -i ~/asperaweb_id_dsa.openssh -k1 -Tr -l100m anonftp@ftp-private.ncbi.nlm.nih.gov:/sra/sra-instant/reads/ByExp/sra/SRX/SRX000/SRX000004/SRR000006/SRR000006.sra ~ （2）批量下载： 观察发现，一个SRX Experiment accessions下的是有规律的，如win下的目录结构所示,只是后两个字段不同：/sra/sra-instant/reads/ByExp/sra/SRX/SRX000/SRX000004/SRR000006/SRR000006 因此可以整理为下面的格式黏贴在文本SRR_Download_List_file_list.txt 中： /sra/sra-instant/reads/ByExp/sra/SRX/SRX000/SRX000004/SRR000006/SRR000006.sra/sra/sra-instant/reads/ByExp/sra/SRX/SRX000/SRX000004/SRR000009/SRR000009.sra/sra/sra-instant/reads/ByExp/sra/SRX/SRX000/SRX000004/SRR000010/SRR000010.sra …….等 ascp -i ~/asperaweb_id_dsa.openssh --mode recv --host ftp-private.ncbi.nlm.nih.gov --user anonftp --file-list SRR_Download_List_file_list.txt ~ 如此可以实现批量下载！ 使用后会发现，从NCBI上下载SRA速度，一般的宽带的话，也可以达到100M/s，大大节约了下载的时间，非常给力 注意事项： （1）如果报错：Error: Server aborted session: Client requests stronger encryption than server allows，那么可以参考：https://support.asperasoft.com/hc/en-us/articles/216126788-Error-Client-requests-stronger-encryption-than-server-allows 对客户端和命令行两种方式都给出了解决方案。在linux命令行下也就是加个-T参数，即： ascp -T -i ~/asperaweb_id_dsa.openssh --mode recv --host ftp-private.ncbi.nlm.nih.gov --user anonftp --file-list SRR_Download_List_file_list.txt ./ （2）放入后台下载，这样不用担心关闭客户端，下载也停止了~ nohup ascp -T -i ~/asperaweb_id_dsa.openssh --mode recv --host ftp-private.ncbi.nlm.nih.gov --user anonftp --file-list SRR_Download_List_file_list.txt ./ &amp; （3）aspera默认不支持断点续传，要支持这个功能添加参数： ascp -k1 -T -i ~/asperaweb_id_dsa.openssh --mode recv --host ftp-private.ncbi.nlm.nih.gov --user anonftp --file-list SRR_Download_List_file_list.txt ./ 所以 ascp -i ~/asperaweb_id_dsa.openssh -k1 -Tr -l100m anonftp@ftp-private.ncbi.nlm.nih.gov:命令可以通用 （4）从EBI上下载也类似，给个例子： ascp -i ~/asperaweb_id_dsa.putty era-fasp@fasp.sra.ebi.ac.uk:/vol1/ERA012/ERA012008/sff/library08_GJ6U61T06.sff 提供Aspera的数据库： （1）NCBI的Sequence Read Archive (SRA), dbGaP. （2）1000genomes – EBI Aspera site, the NCBI Aspera site 1000genomes – EBI Aspera site: http://www.internationalgenome.org/aspera 1000genomes – the NCBI Aspera site https://www.ncbi.nlm.nih.gov/projects/faspftp/1000genomes/ sra数据转为fastaq&emsp;&emsp;sra是NCBI 推出的存储高通量数据的格式，而平常我们工作用得多是fastq格式。如果需要把sra 转成fastq，则下载NCBI SRA Toolkit。 下载地址：https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?cmd=show&amp;f=software&amp;m=software&amp;s=software (1)下载安装（CentOS Linux 64 bit architecture）： curl -O https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.8.2-1/sratoolkit.2.8.2-1-centos_linux64.tar.gz tar xzvf sratoolkit.2.8.2-1-centos_linux64.tar.gz cd sratoolkit.2.8.2-1-centos_linux64 程序都在bin目录下，来看看有什么： cd bin SRA Toolkit Documentation, Frequently Used Tools: fastq-dump: Convert SRA data into fastq format prefetch: Allows command-line downloading of SRA, dbGaP, and ADSP data sam-dump: Convert SRA data to sam format sra-pileup: Generate pileup statistics on aligned SRA data vdb-config: Display and modify VDB configuration information vdb-decrypt: Decrypt non-SRA dbGaP data (“phenotype data”) (2)添加环境变量 echo &apos;export PATH=/home/wangdong/softwares/sratoolkit.2.8.2-centos_linux64/bin&apos;&gt;&gt;~/.bashrc source ~/.bashrc (3)使用： 使用prefetch下载SRA数据下载文件: （1）单个下载 prefetch SRR1553610 （2）批量下载 for i in $(seq 58 79);do prefetch -v SRR8287$i ;done 这些文件区哪儿了？这些文件去哪里了？存在了你home目录下的一个默认文件夹里。 ls ~/ncbi 从NCBI下下来的数据，双端测序数据是放在一个文件里的，所以需要把它们重新拆解为两个文件。我们用程序fastq-dump来把文件拆包 fastq-dump --split-files SRR1553610 那么我怎么知道哪些数据是双端测序的呢？上文的网址关于NSBI的SRA数据结构的Study层次对实验方法有具体介绍：网址再贴一遍：https://www.ncbi.nlm.nih.gov/Traces/study/ 小细节之拆包后文件的命名： File name Description SRR030257_1.fastq Paired-end Illumina, First of pair, FASTQ format SRR030257_2.fastq Paired-end Illumina, Second of pair, FASTQ format 因为在后续分析，mapping到RefSeq上时，单端测序和双端测序命令有些不同！需要注意下。 更多的说明,请参见官方的SRA下载手册: NCBI: https://www.ncbi.nlm.nih.gov/books/NBK242625/ EBI: http://www.ebi.ac.uk/ena/about/sra_data_download FTP下载方式：使用Xftp5 匿名登录FTP站点即可下载资源，慢点就慢点吧~有时需要下载的文件也就1-2M 三大数据库的FTP地址： ensembl : ftp://ftp.ensembl.org/pub NCBI : ftp://ftp.ncbi.nih.gov/genomes/ UCSC：ftp://hgdownload.soe.ucsc.edu/goldenPath 补充：使用SRA Run Selector下载网址：https://www.ncbi.nlm.nih.gov/Traces/study/帮助（步骤很详细了）：https://www.ncbi.nlm.nih.gov/Traces/study/?go=help 参考：（1）博耘生物： http://boyun.sh.cn/bio/?p=1933 （2）Keep Learning的博客 http://blog.csdn.net/xubo245/article/details/50513201 （3）郑俊娟的博客： http://blog.sciencenet.cn/blog-1271266-775638.html （4）鳉鲈的博客： http://blog.sina.com.cn/s/blog_71df25810102w2vf.html （5）生信笔记的博客： http://www.bioinfo-scrounger.com/ （6）Rethink的博客 http://blog.leanote.com/post/hwoihann/how-to-download-series-of-sra-data-in-one-command]]></content>
      <categories>
        <category>NGS软件</category>
      </categories>
      <tags>
        <tag>NGS软件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何优雅的输出python字典]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F05%2F%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E7%9A%84%E8%BE%93%E5%87%BApython%E5%AD%97%E5%85%B8%2F</url>
    <content type="text"><![CDATA[如何优雅的输出python字典&emsp;&emsp;python中的字典是一种清晰的数据结构，生信学习过程中有很多统计的事儿。比如有10个样本，Variant calling 分析后需要根据每个样本，统计每种变异类型并绘图。此时就可以用到python的嵌套字典或者嵌套列表。 &emsp;&emsp;python中的字典是一种清晰的数据结构，生信学习过程中有很多统计的事儿。比如有10个样本，Variant calling 分析后需要根据每个样本，统计每种变异类型并绘图。此时就可以用到python的嵌套字典或者嵌套列表。 （1）一层字典1gene_symbol_ENGS=&#123;'PKHD1': 'ENSG00000170927', 'ATP6V0A4': 'ENSG00000105929', 'TSC2': 'ENSG00000103197'&#125; 123456789from pandas import Seriesframe1 = Series(gene_symbol_ENGS)print(frame1) ATP6V0A4 ENSG00000105929 PKHD1 ENSG00000170927 TSC2 ENSG00000103197 dtype: object （2）嵌套字典1sample_mutation_count = &#123;'PDC668_vs_PDC668A2': &#123;'ncRNA_intronic': 32, 'UTR5': 6, 'ncRNA_splicing': 0, 'exonic;splicing': 0, 'UTR3': 6, 'upstream;downstream': 1, 'downstream': 6, 'exonic': 155, 'upstream': 2, 'splicing': 1, 'ncRNA_exonic': 19, 'intergenic': 162, 'intronic': 162&#125;, 'PDC3748_vs_PDC3748B4': &#123;'ncRNA_intronic': 21, 'UTR5': 6, 'ncRNA_splicing': 0, 'exonic;splicing': 0, 'UTR3': 2, 'upstream;downstream': 0, 'downstream': 2, 'exonic': 123, 'upstream': 8, 'splicing': 2, 'ncRNA_exonic': 15, 'intergenic': 122, 'intronic': 130&#125;&#125; 1234567891011121314151617181920212223242526from pandas import DataFrameframe2 = DataFrame(sample_mutation_count)print(frame2) PDC3748_vs_PDC3748B4 PDC668_vs_PDC668A2 UTR3 2 6 UTR5 6 6 downstream 2 6 exonic 123 155 exonic;splicing 0 0 intergenic 122 162 intronic 130 162 ncRNA_exonic 15 19 ncRNA_intronic 21 32 ncRNA_splicing 0 0 splicing 2 1 upstream 8 2 upstream;downstream 0 1``` 那么问题来了，当每个样本的行不完全一样时如何解决呢，```pythonlen_exon = &#123;'ENSG00000008710': &#123;'ENST00000570193': 591, 'ENST00000483558': 573&#125;,'ENSG00000089597': &#123;'ENST00000526210': 529, 'ENST00000526392': 255, 'ENST00000532402': 3695&#125;&#125; 12345678910111213141516171819202122232425from pandas import Seriesfor key,value in len_exon.items(): print(key) print(Series(value)) ENSG00000008710 ENST00000483558 573 ENST00000570193 591 dtype: int64 ENSG00000089597 ENST00000526210 529 ENST00000526392 255 ENST00000532402 3695 dtype: int64``` ## 当然，也可以使用pprint输出字典结构，看着还行，但不利于后续R绘图```pythonimport pprintresultFile = open('result.py', 'w')resultFile.write(pprint.pformat(len_exon))resultFile.close() 1234&#123;'ENSG00000008710': &#123;'ENST00000483558': 573, 'ENST00000570193': 591&#125;, 'ENSG00000089597': &#123;'ENST00000526210': 529, 'ENST00000526392': 255, 'ENST00000532402': 3695&#125;&#125;]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[picard]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F03%2Fpicard%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;picard 是用java开发的用于处理高通量测序数据和格式转换（SAM/BAM/CRAM和VCF）的命令行工具集 #Picard 高通量测序数据处理及格式转换工具大合集 picard 是用java开发的用于处理高通量测序数据和格式转换（SAM/BAM/CRAM和VCF）的命令行工具集 其官网为：http://broadinstitute.github.io/picard/ 先看看picard都有哪些工具集： Available Programs:Alpha Tools: 目前无支持，需要进一步测试完善的工具集 CollectIndependentReplicateMetrics 预测bam文件中reads的独立重复率 CollectWgsMetricsWithNonZeroCoverage 收集关于全基因组（WGS）测序的覆盖度和测序质量信息 UmiAwareMarkDuplicatesWithMateCigar 利用read的位置和UMIs信息鉴定重复的reads Fasta: 操作FASTAor相关数据工具集 CreateSequenceDictionary 创建参考序列的序列字典 ExtractSequences 从参考序列中创建子区间存为新的FASTA NonNFastaSize 对fasta文件中non-N碱基计数 NormalizeFasta 规范FASTA文件中的序列行为相同长度 Fingerprinting Tools: 操作印迹图谱工具集 CheckFingerprint 计算来自提供的(SAM/BAM or VCF) 文件的指纹图谱，并与提供的基因型比较 ClusterCrosscheckMetrics 通过LOD得分对CrosscheckFingerprints的结果进行聚类 CrosscheckFingerprints 检查是否所有的指纹图谱来自相同的个体 CrosscheckReadGroupFingerprints 弃用：使用CrosscheckFingerprints.检查是否所有的指纹图谱来自相同的个体 Illumina Tools: 操作具体Illumina测序数据工具集 CheckIlluminaDirectory 维护具体的Illumina basecalling数据的有效性 CollectIlluminaBasecallingMetrics 从一次测序运行中收集Illumina Basecalling矩阵 CollectIlluminaLaneMetrics 对于每个给定的BaseCalling分析路径收集Illumina lane矩阵 ExtractIlluminaBarcodes 确定Illumina lane中每个read的barcode IlluminaBasecallsToFastq 从Illumina basecall read数据中产生ASTQ 文件 IlluminaBasecallsToSam 转换原始的Illumina测序数据为unmapped 的SAM或者BAM 文件. MarkIlluminaAdapters 读SAM或者BAM 文件并用新的接头修饰标签重写 Interval Tools: 操作Picard区间列表工具集 BedToIntervalList 转换BED 文件为Picard区间列表 IntervalListToBed 转换Picard的IntervalList文件为BED文件. IntervalListTools 操作区间列表 LiftOverIntervalList 从一个参考转为另一个时去除区间列表 ScatterIntervalsByNs 通过Ns分割参考，写入区间列表 Metrics: 各种不同数据类型报表矩阵工具集 AccumulateVariantCallingMetrics 组合多个变异Calling矩阵为单个文件 CollectAlignmentSummaryMetrics &lt;b&gt;从SAM或BAM文件产生比对矩阵总结 &lt;/b&gt; CollectBaseDistributionByCycle 对SAM或BAM文件中每个循环核苷酸分布制表 CollectGcBiasMetrics 收集关于GC bias的矩阵 CollectHiSeqXPfFailMetrics 将HiSeqX Illumina Basecalling directory下的PF-Failing reads归类为不同类别 CollectHsMetrics 从SAM或BAM文件收集杂交选择（HS）矩阵 CollectInsertSizeMetrics 从配对末端文库中收集插入片段分布矩阵 CollectJumpingLibraryMetrics 收集跳跃文库矩阵 CollectMultipleMetrics 收集多类矩阵 CollectOxoGMetrics 收集矩阵评估氧化产物 CollectQualityYieldMetrics 收集关于通过质控阈值和 Illumina-specific过滤的reads矩阵 CollectRawWgsMetrics 收集全基因组测序相关矩阵 CollectRnaSeqMetrics 从SAM或BAM文件中产生RNA比对矩阵 CollectRrbsMetrics &lt;b&gt;从简化的亚硫酸氢盐测序(Rrbs)数据收集矩阵&lt;/b&gt; CollectSequencingArtifactMetrics 收集量化单末端测序产品的矩阵 CollectTargetedPcrMetrics 从目标测序数据中收集PCR相关矩阵 CollectVariantCallingMetrics 从提供的VCF文件文件中收集每个样本和包含所有样本集合的矩阵 CollectWgsMetrics 收集关于全基因组测序（WGS）实验的覆盖度和质量矩阵 CompareMetrics 比较两个矩阵文件 ConvertSequencingArtifactToOxoG 从广义的人工矩阵提取OxoG矩阵 EstimateLibraryComplexity 预测测序文库中特异性分子数量 MeanQualityByCycle 通过循环收集均值质量 QualityScoreDistribution 为质量得分的分布绘制表格 Miscellaneous Tools: 混杂工具集 BaitDesigner 为杂交选择反应设计寡核苷酸baits FifoBuffer FIFO buffer 用来缓冲具有可定制缓冲大小的输入和输出流 SAM/BAM: 操作SAM, BAMor者相关数据的工具集 AddCommentsToBam 为headerBAM 文件的header增加评论 AddOrReplaceReadGroups 替代BAM 文件的read groups BamIndexStats 从BAM 文件产生索引统计 BamToBfq 通过maq aligner从BAM文件产生BFQ文件s from a BuildBamIndex 生成BAM索引，&quot;.bai&quot; 文件 CalculateReadGroupChecksum 基于read groups(RG)产生哈希码 CheckTerminatorBlock 维护提供的gzip 文件(e.g., BAM)最后一个区块格式正确; 否则RC 100 CleanSam 清除提供的SAM/BAM，soft-clipping beyond-end-of-reference alignments并且对于未比对上的reads设置MAPQ为0 CompareSAMs 比较两个输入的&quot;.sam&quot; or &quot;.bam&quot; 文件 DownsampleSam 对SAM或BAM文件缩小取样 FastqToSam 转换FASTQ文件为unaligned的BAM或SAM文件 FilterSamReads 从SAM或BAM文件取read数据子集 FixMateInformation 如果需要，在mates和fix之间确认mate-pair信息 GatherBamFiles 尽可能高效的连接一个或多个BAM文件 MarkDuplicates 鉴定重复的reads MarkDuplicatesWithMateCigar 鉴定重复的reads,解释mate CIGAR. MergeBamAlignment 从SAM或者BAM文件中合并alignment数据到unmapped BAM文件 MergeSamFiles 合并多个SAM和/或BAM 文件为单个文件 PositionBasedDownsampleSam 缩小SAM或者BAM文件取样来维持reads子集，基于reads在每个flowcell的每个tile的位置 ReorderSam 对SAM或者BAM文件中的reads重排序，来匹配参考文件中的顺序 ReplaceSamHeader 替代SAM或BAM文件的SAMFileHeader RevertOriginalBaseQualitiesAndAddMateCigar 转换原始碱基质量并增加mate cigar到read-group BAMs RevertSam 转换SAM或BAM 文件回到之前状态 SamFormatConverter BAM文件与SAM 文件互相转换 SamToFastq 转换SAM或者BAM文件为FASTQ文件 SetNmAndUqTags 弃用：使用SetNmMdAndUqTags代替 SetNmMdAndUqTags 修改SAM文件中NM,MD和UQ标签 SortSam 对一个SAM或BAM文件排序 SplitSamByLibrary 通过文库分割SAM或BAM文件为独立文件 ValidateSamFile 确认SAM或BAM文件 ViewSam 打印SAM或BAM文件到屏幕 Unit Testing: 测试单元 SimpleMarkDuplicatesWithMateCigar 测试提供的SAM或BAM文件中比对上的记录来定位重复分子 VCF/BCF: 操作VCF, BCFor者相关数据的工具集 FilterVcf 严格过滤VCF FindMendelianViolations 在VCF文件中寻找所有违反孟德尔法则的类型 FixVcfHeader 代替或者修改VCF header GatherVcfs 文件从一个分散的多个VCF文件产生单个VCF文件 GenotypeConcordance 评估callsets之间的基因型一致性 LiftoverVcf 从一个引用构建另一个引用时留下一个VCF文件 MakeSitesOnlyVcf 从VCF或BCF文件创建没有基因型信息的VCF文件 MergeVcfs 合并多个VCF或者BCF文件为一个VCF文件或者BCF文件 RenameSampleInVcf 对VCF或BCF中样本重命名 SortVcf 对一个活多个VCF 文件排序 SplitVcfs 分割SNPs和INDELs为独立的文件 UpdateVcfSequenceDictionary 对于VCF文件和包含有序列字典的文件，利用新的序列字典更新VCF文件 VcfFormatConverter VCF与BCF互相转换 VcfToIntervalList 转换VCF or BCF 文件为Picard区间列表 （1）接下来看看如何安装： #查看Java版本 java -version Java 1.8及以后版本即OK #从github拷贝库 git clone https://github.com/broadinstitute/picard.git #进入picard文件目录 cd picard/ #编译 ./gradlew shadowJar 此时，在build/libs/文件夹下可见picard.jar程序 #运行 java -jar build/libs/picard.jar （2）如何设置环境变量： vim ~/.bashrc i PICARD=&apos;/home/u631758/biosoftwares/picard/build/libs/picard.jar&apos; alias picard=&quot;java -jar $PICARD&quot; 如此就可以用picard命名代替官网中的java -jar picard.jar命令了！！！ （3）查看有哪些可用的工具： picard （4）查看某个工具的具体用法： 例如VcfFormatConverter工具： picard VcfFormatConverter -h 结果为： java -jar picard.jar VcfFormatConverter \ I=input.vcf \ O=output.bcf \ 所以使用方式为： picard VcfFormatConverter I=input.vcf O=output.bcf]]></content>
      <categories>
        <category>NGS软件</category>
      </categories>
      <tags>
        <tag>NGS软件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux中python2和python3环境搭建及共存]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F03%2Fpython%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;本教程将展示如何在CentOS上通过源码编辑安装最新的Python 2 和 Python 3。以下例子使用Python 2.7.13 and Python 3.6.2，但是操作流程对于其他新版python都是相同的。 linux中python2和python3环境搭建及共存&emsp;&emsp;如果你使用的是CentOS 6，你可以使用本教程安装 Python 2.7.x and Python 3.6.x。对于CentOS 7 只有Python 3.6.x的安装方法是适用的。警告！不要在CentOS 7上使用本教程安装Python 2.7.13。这样你的系统将有两个不同的python2.7二进制文件，每个具有自己的安装包路径。这将可能造成不能区分的问题！ 查看linux系统版本信息：lsb_release -a 注: 这个命令适用于所有的linux，包括Redhat、SuSE、Debian等发行版 问题所在？&emsp;&emsp;CentOS携带Python作为基础系统的一个重要部分。因为其十分重要，所以未及时更新，或者为了避免安全漏洞。缺少更新，意味着CentOS 6用户无法摆脱2010年8月发布的Python 2.6.6，CentOS 7用户无法摆脱2013年发布的Python 2.7.5。 通常/usr/bin下面的都是系统预装的可执行程序，会随着系统升级而改变 cd /usr/bin ls |grep ^p 只安装了python2.6 YUM&emsp;&emsp;YUM（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及CentOS中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，避免了手动安装的麻烦(寻找资源、下载；放到指定目录安装；处理依赖关系并下载依赖关系的包进行安装)。所以用yum安装，实质上是用RPM安装，所以RPM查询信息的指令都可用。 如果使用RPM安装了一些包，一般来说，RPM默认安装路径如下： Directory Contents of Directory /usr/bin 一些可执行文件 /usr/lib 一些程序使用的动态函数库 /usr/share/doc 一些基本的软件使用手册与帮助文档 /usr/share/man 一些man page文件所以没有root权限，是没有办法通过yum进行软件安装的 需要考虑的事项：使用 “make altinstall” 来避免麻烦 &emsp;&emsp;当安装定制版的Python时使用make altinstall时十分重要的。如果使用常规的make install的结局是安装不同的python版本，但出现在文件系统中却出现同为python的程序,这将导致很难辨别的麻烦。 &emsp;&emsp;编译安装的准备 为了编译Python之前最好先安装一系列的开发工具和一些拓展库，但不是必须的，但这样Python才能依赖这些工具和拓展库展示它强悍的功能。 (1)下载并编译安装pythonPython 2.7.13: wget http://python.org/ftp/python/2.7.13/Python-2.7.13.tar.xz tar xf Python-2.7.13.tar.xz cd Python-2.7.13 ./configure --prefix=/home/wangdong/python/python27 make &amp;&amp; make altinstall Python 3.6.2: wget http://python.org/ftp/python/3.6.2/Python-3.6.2.tar.xz tar xf Python-3.6.2.tar.xz cd Python-3.6.2 ./configure --prefix=/home/wangdong/python/python36 make &amp;&amp; make altinstall (2)设置环境变量echo &apos;export PATH=/home/wangdong/python/python36/bin:$PATH&apos;&gt;&gt;~/.bashrc echo &apos;export PATH=/home/wangdong/python/python27/bin:$PATH&apos;&gt;&gt;~/.bashrc &emsp;&emsp;注意使用python3.6, 使用python仍旧为系统预装的python版本，以后使用Python解释器时,同样需要注意使用python和python3.6调用的Python解释器是不同的！！ linux添加环境环境变量注意事项： （1）=号左右两边没有空格 （2）路径之间用：分隔 （3）$PATH 表示原先设定的路径仍然有效，注意不要漏掉 （4）需要引号,因为用echo命令输出加引号的字符串时，将字符串原样输出 (3) 安装/升级pip,setuotools和wheel 安装pip,setuotools和wheel系统中的每个Python解释器都需要自己的pip,setuotools和wheel,安装和升级这些包最简单的方式是使用get-pip.py脚本。 First get the script: wget https://bootstrap.pypa.io/get-pip.py Then execute it using Python 2.7 and/or Python 3.6: python2.7 get-pip.py python3.6 get-pip.py PYTHONPATH是Python搜索路径，默认我们import的模块都会从PYTHONPATH里面寻找 echo &apos;export PYTHONPATH=/home/wangdong/python/python36/lib/python3.6/site-packages&apos;&gt;&gt;~/.bashrc echo &apos;export PYTHONPATH=/home/wangdong/python/python27/lib/python2.7/site-packages&apos;&gt;&gt;~/.bashrc source ~/.bashrc With pip installed you can now do things like this: pip2.7 install [packagename] pip2.7 install –upgrade [packagename] pip2.7 uninstall [packagename] pip3.6 install [packagename] pip3.6 install –upgrade [packagename] pip3.6 uninstall [packagename] &emsp;&emsp;注意使用pip2.7和pip3.6安装软件不同点在于，安装文件的路径不同。pip2.7的安装路径是/home/wangdong/python/python27/lib/python2.7/site-packages，而pip3.6的安装路径是/home/wangdong/python/python36/lib/python3.6/site-packages ###（4）虚拟环境的使用 &emsp;&emsp;如果你使用Python2.7，则强烈推荐使用安装virtualenv并且学习使用它。virtualenv可以创建独立的Python环境。如果你使用Python3.3+，那么你没有必要安装virtualenv，因为其功能已经内建了。 每个独立的Python环境（又叫sandbox）能具有自己的Python版本和包。这对于多项目或者相同项目需要不同的版本的场合是十分重要的。 先看看virtualenv中文教程： http://virtualenv-chinese-docs.readthedocs.io/en/latest/#id29 Install virtualenv for Python 2.7 and create a sandbox called my27project: pip2.7 install virtualenv virtualenv my27project Use the built-in functionality in Python 3.6 to create a sandbox called my36project: python3.6 -m venv my36project (1)Check the system Python interpreter version: python --version This will show Python 2.6.6 Activate the my27project sandbox: source my27project/bin/activate (2)Check the Python version in the sandbox (it should be Python 2.7.13): python --version This will show Python 2.7.13 Deactivate the sandbox: deactivate Activate the my36project sandbox: source my36project/bin/activate (3)Check the Python version in the sandbox (it should be Python 3.6.2): python --version This will show Python 3.6.2 Deactivate the sandbox: deactivate 小结：(1) 从（1）和（2）或者（1）和（3）的对比看出：创建虚拟环境并激活后，虚拟环境的环境变量和系统的环境变量是隔离的，互不影响。 (2) 创建的虚拟环境的Python解释器版本是如何指定的呢？先看看virtualenv用法: $ virtualenv [OPTIONS] DEST_DIR其中一个选项-p PYTHON_EXE, –python=PYTHON_EXE &emsp;&emsp;指定所用的python解析器的版本，比如 –python=python2.5 就使用2.5版本的解析器创建新的隔离环境。 默认使用的是当前目录下安装(/home/wangdong/python/python36/bin/python3.6或者/home/wangdong/python/python27/bin/python2.7)的python解析器 所以可以在python27下使用-p指定python3.6解释器创建虚拟环境： 反过来，对于Python3.3+ 通过venv模块创建指定python2.7虚拟环境则不行了！ 首先看看venv模块官方文档： https://docs.python.org/3/library/venv.html 需要注意的是，在Python3.3中使用”venv”命令创建的环境不包含”pip”，你需要进行手动安装。在Python3.4中改进了这一个缺陷。 并没有相关参数！！ 所以类似的可以使用virtualenv解决： 在python36目录下： virtualenv my27project_test source my27project_test/bin/activate python This will show Python 2.7.13 在python36目录下： virtualenv -p /home/wangdong/python/python36/bin/python3.6 my36proje_test source my36project_test/bin/activate python This will show Python 3.6.2 （3）在对应虚拟环境下使用对应pip安装软件：例如： source my36project/bin/activate pip3.6 install numpy &emsp;&emsp;安装路径为： ./my36project/lib/python3.6/site-packages **所以安装包也和系统是完全隔离的，二者互不影响。因此虚拟环境不再使用时，直接删除该虚拟环境即可。 rm -rf my36project 在my27project下则使用pip2.7,其他类似。 ####（4）接下来讲讲pip使用 使用清华的pip源安装包更快： pip3.6 install -i https://pypi.tuna.tsinghua.edu.cn/simple bcbio-gff biopython cython nose numpy pandas shove sqlalchemy python-memcached pyvcf (不同安装包之间使用空格即可) 指定安装包的版本,例如： pip3.6 install pysam==0.7.5 卸载指定版本安装包，例如： pip3.6 uninstall biopython==1.70 把常用的包离线下载，然后使用pip离线安装包，例如： pip3.6 install pysam-0.7.5.tar.gz 查看当前环境pip已安装包列表： pip3.6 list 参考： https://danieleriksson.net/2017/02/08/how-to-install-latest-python-on-centos/]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[添加PATH环境变量及设置别名]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F02%2F%E6%B7%BB%E5%8A%A0PATH%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E5%8F%8A%E8%AE%BE%E7%BD%AE%E5%88%AB%E5%90%8D%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;Linux下想让某个命令可以全局使用，可以有两种方法来实现，具体请看下文！ 想让某个命令可以全局使用，可以有两种方法来实现：方法一.在命令行将路径添加到 .bashrc文件echo &apos;export PATH=~/home/biosoftwares/FastQC/bin/fastqc:$PATH&apos; &gt;&gt;~/.bashrc source ~/.bashrc 注意： （1）注意添加的路径是可执行文件的绝对路径，一般在bin下，可执行文件为绿色 （2）用echo命令输出加引号的字符串时，将字符串原样输出 （3）export可新增，修改或删除环境变量，供后续执行的程序使用。export的效力仅及于该次登陆操作 （4）&gt;&gt;以追加方式添加 （5） ~/.bashrc表示家目录下的.bashrc文件，为隐藏文件，使用ls -a可以查看 （6）使用source ~/.bashrc之前需要先回到家目录 （7）Source命令也称为“点命令”，也就是一个点符号（.）。source命令通常用于重新执行刚修改的 初始化文件，使之立即生效，而不必注销并重新登录。用法：source filename 或 . filename 也可以在用vim打开~/.bashrc文件直接添加 vim ~/.bashrc i PATH=~/home/biosoftwares/FastQC/bin/fastqc Esc :x 注意： （1）i 进入vim编辑模式 （2）Esc退出vim编辑模式 （3）:x保存修改并退出（需要在英文输入法状态下） 方法二. 将目录添加到~/.bashrc 文件，比如~/bin，然后创建软连接（相当于win下的快捷方式）#创建~/bin目录 mkdir -p ~/bin #将~/bin目录添加到PATH echo &apos;export PATH=~/bin:$PATH&apos;&gt;&gt;~/.bashrc #使修改生效 source ~/.bashrc #在/bin下生成fastqc软连接 ln -s ~/src/fastQC/fastqc ~/bin/fastqc 设置命令别名alias ll=’ls -l’ #在当前的Shell生效 echo “alias ll=’ls -l’” &gt;&gt;~/.bashrc #永久生效source ~/.bashrc 其他常用别名举例： （1）alias ll=’ls -l –color=tty’ #按系统预定义的颜色区分不同的文件类型 （2）alias la=’ls -a’ #显示隐藏文件 （3）alias rmall=’rm -rf’ #显示隐藏文件 （4）alias less=’less -S’ #根据屏幕大小显示,且列对齐 （5）解压命令 *.tar 用 tar -xvf 解压，echo “alias untar=’tar -xvf’” &gt;&gt;~/.bashrc *.gz 用 gzip -d或者gunzip 解压, echo “alias ungz=’gunzip’” &gt;&gt;~/.bashrc .tar.gz和.tgz 用 tar -xzf 解压, echo “alias untargz=’tar -xzf’” &gt;&gt;~/.bashrc *.bz2 用 bzip2 -d或者用bunzip2 解压, echo “alias unbz2=’bunzip2’” &gt;&gt;~/.bashrc *.tar.bz2用tar -xjf 解压, echo “alias untarbz2z=’tar -xjf’” &gt;&gt;~/.bashrc *.Z 用 uncompress 解压, echo “alias unZ=’uncompress’” &gt;&gt;~/.bashrc *.tar.Z 用tar -xZf 解压, echo “alias untarZ=’tar -xZf’” &gt;&gt;~/.bashrc *.rar 用 unrar e解压, echo “alias unrar=’unrar e’” &gt;&gt;~/.bashrc *.zip 用 unzip 解压 *.tar.xz 用 $xz -d *.tar.xz $tar -xvf *.tar *.tgz 用tar zxvf 解压, echo “alias untgz=’tar zxvf’” &gt;&gt;~/.bashrc (5)列出目前所有的别名设置。 alias 或 alias -p 注意： （1）mkdir -p 表示创建多级目录 （2）ln是link的缩写，-s选项创建软连接 创建软连接格式： ln/link SOURCE [TARGET] 创建软链接： ln/link -s SOURCE [TARGET] 软连接不占用磁盘空间，硬链接相当于拷贝，占磁盘空间 如下图，-&gt;带有这个符号的则为软连接,initrd.img为软连接的文件名，而-&gt;后面跟着的boot/initrd.img-3.13.0-32-geberic则为这个软连接文件的真实路径 参考（1）我使用过的Linux命令之alias - 设置命令的别名，让 Linux 命令更简练]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[修改host文件-科学上网]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F02%2F%E4%BF%AE%E6%94%B9host%E6%96%87%E4%BB%B6-%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;Hosts文件是Windows系统中一个负责IP地址与域名快递解析的文件，以ASCLL格式保存。计算机在键入域名的时候，首先会去看看hosts文件汇总有没有关于此域名IP地址的记录。如果有，就直接登陆该网站；如果没有再查询DNS服务器。那么，为什么修改HOSTS文件可以实现翻墙呢？请看下文~ &emsp;&emsp;Hosts文件是Windows系统中一个负责IP地址与域名快递解析的文件，以ASCLL格式保存。计算机在键入域名的时候，首先会去看看hosts文件汇总有没有关于此域名IP地址的记录。如果有，就直接登陆该网站；如果没有再查询DNS服务器。那么，为什么修改HOSTS文件可以实现翻墙呢？ &emsp;&emsp;无论你上网打的什么网站地址, 实际上最终总归是要转换成一个IP地址才能访问的,平时这个转换工作是有网络上的DNS服务器来完成的。但是有些时候,有些网站, 处于某些原因, 网络上的DNS服务器无法给出正确的或可用IP地址(天朝特别多, 大家懂的)。 &emsp;&emsp;这个时候hosts文件就可以代劳了, 你可以直接用记事本打开这文件看看就知道了, 里面一行就是一条记录, 一个IP地址接一个空格或tab, 再后面就是一个网址。它起到的作用就是直接在你本机上就把这些网址翻译成IP地址. 从Windows 2000开始，Windows解析名称的顺序为：DNS cache –&gt; hosts 文件 –&gt; DNS Server –&gt; NetBIOS cache –&gt; WINS Server –&gt; 广播 –&gt; LMHOSTS 文件hosts 文件的优先级高于 DNS Server，因此修改hosts文件可以跳过被污染的dns服务器。 Hosts文件格式是咋样的？ &emsp;&emsp;用记事本打开hosts文件，它的作用是包含IP地址和Host name(主机名)的映射关系，是一个映射IP地址和Hostname(主机名)的规定，规定要求每段只能包括一个映射关系，IP地址要放在每段的最前面，空格后再写上映射的Host name(主机名)。对于这段的映射说明用“#”分割后用文字说明。 为啥还需要及时更新Hosts文件？ Hosts文件配置的映射是静态的，如果网络上的计算机更改了请及时更新IP地址，否则将不能访问。 该Git项目持续更新可用的Hosts文件： https://github.com/racaljk/hosts 如何修改Hosts文件实现翻墙呢？ （1）. 从上述Git项目中复制hosts文件内容至txt文件，命名为HOSTS, 并去掉扩展名！！！ Win7 系统HOSTS文件位于 C:\Windows\System32\drivers\etc\hosts，没有拓展名。 （2）. 使之生效 Windows 开始 -&gt; 运行 -&gt; 输入cmd -&gt; 在CMD窗口输入 ipconfig /flushdns （3）使用谷歌浏览器随意登陆Google、Gmail、维基百科、Twitter、Facebook等，但必须使用https加密方式打开 谷歌香港：https://www.google.com.hk 谷歌：https://www.google.com/ncr PS: www.google.com/ncr中的”/ncr”是什么意思?起什么作用的? ncr : no country redirect &emsp;&emsp;If google thinks you are from a foreign country or region,it likes to redirect you to your regional google page.For most people,this makes sense.However,if you prefer the generic,english,plain version,this would be very annoying. 改hosts和vpn的区别 &emsp;&emsp;修改host相当于绕过dns服务器来访问，vpn是通过另外一台主机来访问，然后所有数据通过主机来交换。就翻++墙而言没有区别，不过vpn可以获取你的信息。 &emsp;&emsp;修改host就是添加一个本地的ip对应表，vpn是通过别的机器来访问。简单的说就是，国内不允许使用一些dns服务来访问外站，所以有了天朝防火墙，而通过ip直连就可以绕过天朝防火墙来进行和外网交互。vpn是通过一个天朝防火墙之外的一个主机来进行访问，所有的交互数据通过主机来传递 参考： （1）老D博客（很多黑科技~）： https://laod.cn/hosts/2017-google-hosts.html （2）新浪博客：http://blog.sina.com.cn/s/blog_9caf88850102xnlb.html]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生命在于折腾之Github+HEXO搭建博客]]></title>
    <url>https%3A%2F%2F%2FAnJingwd.github.io%2F2017%2F08%2F02%2F%E7%94%9F%E5%91%BD%E5%9C%A8%E4%BA%8E%E6%8A%98%E8%85%BE%E4%B9%8BGithub-HEXO%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;生命在于折腾之Github+HEXO搭建博客 &emsp;&emsp;花了近四个小时，总算还是成功了~现在的心情是这样的： 开心之余，将我的经验和踩过的坑分享给大家~ 首先推荐下参考的博客，基本是对的，实在大赞。（1）Never_yu’s的博客： https://neveryu.github.io/2016/09/03/hexo-next-one/ （2）金石开的博客 http://www.cnblogs.com/zhcncn/p/4097881.html 建议主要根据上面两个博客的方法搭建，遇到问题看看这些填坑博客~（1）文青程序猿的博客： http://www.jianshu.com/p/31eb84182156 （2）WebEnh的博客 http://www.cnblogs.com/webenh/p/5792632.html 我的安装过程还算顺利，遇到的问题：(1)安装nvm后node -v报错，表明环境变量问题：解决： ivanyb的简书文章 http://www.jianshu.com/p/07c3456e875a 孙群的博客 http://blog.csdn.net/iispring/article/details/8023319/ （2）使用npm install -g hexo-cli命令安装Hexo，很卡，最后还报错，查了查，说因为npm被墙了。 所以首先更改了淘宝的源： nvm node_mirror https://npm.taobao.org/mirrors/node/ nvm npm_mirror https://npm.taobao.org/mirrors/npm/ 参考：https://github.com/coreybutler/nvm-windows 问题依旧！！！ 查了下报错：npm ERR! 参考：http://blog.csdn.net/weng423811758/article/details/51537594 因为我开了全局VPN,但依然没解决。 淘宝说用cnpm代替npm： 参考：https://npm.taobao.org/ 然并卵！！ 最后，查看node官网，后来看看其官网，推荐使用v6.11.2LTS，改为v6.11.2LTS之后就解决了（ps:开始是使用的node的最新的v8.2.1版本） （3）部署时报错error deployer not found:git 解决：http://www.jianshu.com/p/4d2c07a330da 我的deploy配置 deploy: type: git repository: https://github.com/AnJingwd/AnJingwd.github.io.git branch: master 然后就成功了！！！ 我的博客：https://anjingwd.github.io/ 未完待续~ 博客将继续完善]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
</search>
